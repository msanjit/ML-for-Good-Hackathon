{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3d1008c",
   "metadata": {},
   "source": [
    "# ML-for-Good-Hackathon\n",
    "# Team Name: Vcare\n",
    "# Participants: Sanjit Mehta, Naveena Chandwani, Rohith Rathod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c71baa",
   "metadata": {},
   "source": [
    "### Import Common packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dc08c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914208a0",
   "metadata": {},
   "source": [
    "### Import NLP related packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "259abc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install contractions\n",
    "import contractions\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from IPython.display import display\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "#pip install gensim\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "from time import time \n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2af62a5",
   "metadata": {},
   "source": [
    "### Import Data and Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "678d49a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    path = 'C:/Users/NLP/data/' # use your path\n",
    "    all_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for filename in all_files:\n",
    "        df = pd.read_csv(filename, index_col=None, header=0)\n",
    "        df_list.append(df)\n",
    "\n",
    "    df = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "\n",
    "    # Drop duplicates\n",
    "    cols_req = [\"focus_group_subtype\", \"focus_group_subtype_id\", \"doc_no_within_subtype\", \"question_id\", \n",
    "                \"question_text\", \"parent_num\", \"parent_answer\"]\n",
    "\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df = df[cols_req]\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a681d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>focus_group_subtype</th>\n",
       "      <th>focus_group_subtype_id</th>\n",
       "      <th>doc_no_within_subtype</th>\n",
       "      <th>question_id</th>\n",
       "      <th>question_text</th>\n",
       "      <th>parent_num</th>\n",
       "      <th>parent_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gaming_group</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>So, I was thinking we could start by just goin...</td>\n",
       "      <td>5</td>\n",
       "      <td>Sure. Hi. My name is Parent 5. I have three ki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gaming_group</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>So, I was thinking we could start by just goin...</td>\n",
       "      <td>1</td>\n",
       "      <td>Hi everyone. My name is Parent 1, I have two b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gaming_group</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>So, I was thinking we could start by just goin...</td>\n",
       "      <td>3</td>\n",
       "      <td>Hi everybody. My name is Parent 3 and I have a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gaming_group</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>So, I was thinking we could start by just goin...</td>\n",
       "      <td>4</td>\n",
       "      <td>Hi, I'm Parent 4, I have a 15-year-old daughte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gaming_group</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>So, I was thinking we could start by just goin...</td>\n",
       "      <td>2</td>\n",
       "      <td>Oh, I'm sorry. I lost connection, I couldn't h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  focus_group_subtype  focus_group_subtype_id  doc_no_within_subtype  \\\n",
       "0        gaming_group                       1                      1   \n",
       "1        gaming_group                       1                      1   \n",
       "2        gaming_group                       1                      1   \n",
       "3        gaming_group                       1                      1   \n",
       "4        gaming_group                       1                      1   \n",
       "\n",
       "   question_id                                      question_text  parent_num  \\\n",
       "0            1  So, I was thinking we could start by just goin...           5   \n",
       "1            1  So, I was thinking we could start by just goin...           1   \n",
       "2            1  So, I was thinking we could start by just goin...           3   \n",
       "3            1  So, I was thinking we could start by just goin...           4   \n",
       "4            1  So, I was thinking we could start by just goin...           2   \n",
       "\n",
       "                                       parent_answer  \n",
       "0  Sure. Hi. My name is Parent 5. I have three ki...  \n",
       "1  Hi everyone. My name is Parent 1, I have two b...  \n",
       "2  Hi everybody. My name is Parent 3 and I have a...  \n",
       "3  Hi, I'm Parent 4, I have a 15-year-old daughte...  \n",
       "4  Oh, I'm sorry. I lost connection, I couldn't h...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df = read_data()\n",
    "input_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914b1d9e",
   "metadata": {},
   "source": [
    "# Preprocessing of text Data\n",
    "1. Expand contraction\n",
    "2. Case handling\n",
    "3. Remove punctuations\n",
    "4. Remove words and digits containing digits\n",
    "5. Remove stop word\n",
    "6. Lemmatization\n",
    "7. Remove Extra Spaces "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f38ef70",
   "metadata": {},
   "source": [
    "#### 1. Expand contraction\n",
    "Contraction is the shortened form of a word like don’t stands for do not, aren’t stands for are not. Like this, we need to expand this contraction in the text data for better analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "215efe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contraction(df,columns=[]):\n",
    "    \n",
    "    for col in columns:\n",
    "        df[col] = df[col].apply(lambda text:contractions.fix(text))\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773f4738",
   "metadata": {},
   "source": [
    "#### 2. Case handling\n",
    "If the text is in the same case, it is easy for a machine to interpret the words because the lower case and upper case are treated differently by the machine. for example, words like Ball and ball are treated differently by machine. So, we need to make the text in the same case and the most preferred case is a lower case to avoid such problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e100e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_handling(df,columns=[]):\n",
    "    \n",
    "    for col in columns:\n",
    "        df[col] = df[col].str.lower() \n",
    "        \n",
    "    return df       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7527044f",
   "metadata": {},
   "source": [
    "#### 3. Remove punctuations\n",
    "One of the other text processing techniques is removing punctuations. there are total 32 main punctuations that need to be taken care of. we can directly use the string module with a regular expression to replace any punctuation in text with an empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f0171de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(df,columns=[]):\n",
    "    \n",
    "    for col in columns:\n",
    "        df[col] = df[col].apply(lambda text: re.sub('[%s]' % re.escape(string.punctuation), '' , text))\n",
    "        df[col] = df[col].apply(lambda text: text.replace(\"_\",\" \"))\n",
    "        \n",
    "    return df   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bc5a36",
   "metadata": {},
   "source": [
    "#### 4. Remove words and digits containing digits\n",
    "Sometimes it happens that words and digits combine are written in the text which creates a problem for machines to understand. hence, We need to remove the words and digits which are combined like game57 or game5ts7. This type of word is difficult to process so better to remove them or replace them with an empty string. we use regular expressions for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b22ebeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words_dgits(df,columns=[]):\n",
    "    \n",
    "    for col in columns:\n",
    "        df[col] = df[col].apply(lambda text: re.sub(\" \\d+\",'',text))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aa4247",
   "metadata": {},
   "source": [
    "#### 5. Remove stopword\n",
    "Stopwords are the most commonly occurring words in a text which do not provide any valuable information. stopwords like they, there, this, where, etc are some of the stopwords. NLTK library is a common library that is used to remove stopwords and include approximately 180 stopwords which it removes. If we want to add any new word to a set of words then it is easy using the add method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c2fb5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(df, columns=[]):\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def remove_sw(text):\n",
    "        txt_output = \" \".join([word for word in str(text).split() if word not in stop_words])\n",
    "        return txt_output\n",
    "    \n",
    "    for col in columns:\n",
    "        df[col] = df[col].apply(lambda text: remove_sw(text))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb660f89",
   "metadata": {},
   "source": [
    "#### 6. Lemmatization\n",
    "Lemmatization is similar to stemming, used to stem the words into root word but differs in working. Actually, Lemmatization is a systematic way to reduce the words into their lemma by matching them with a language dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b57de9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(df, columns=[]):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def lemmatize(text):\n",
    "        text_output = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "        return text_output\n",
    "    \n",
    "    for col in columns:\n",
    "        df[col] = df[col].apply(lambda text: lemmatize(text))\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010d922d",
   "metadata": {},
   "source": [
    "#### 7. Remove Extra Spaces \n",
    "Most of the time text data contain extra spaces or while performing the above preprocessing techniques more than one space is left between the text so we need to control this problem. regular expression library performs well to solve this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a86e6c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_spaces(df,columns=[]):\n",
    "    \n",
    "    for col in columns:\n",
    "        df[col] = df[col].apply(lambda text: re.sub(' +', ' ', text))\n",
    "        \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64ea0676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(df, columns=[]):\n",
    "    \n",
    "    df = expand_contraction(df,columns)\n",
    "    df = case_handling(df,columns) \n",
    "    df = remove_punctuations(df,columns)\n",
    "    df = remove_words_dgits(df,columns)  \n",
    "    df = remove_stopwords(df,columns) \n",
    "    df = lemmatize_words(df, columns)\n",
    "    df = remove_extra_spaces(df,columns) \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935f2c5b",
   "metadata": {},
   "source": [
    "## Data preprocessing function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38ebf681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "columns=['parent_answer']\n",
    "output_df =  data_preprocessing(input_df, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76c6bcf",
   "metadata": {},
   "source": [
    "## Unsupervised Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f37625a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df['parent_answer'] =  output_df['parent_answer'].apply(lambda text: text.split())\n",
    "sent = [row for row in output_df.parent_answer]\n",
    "phrases = Phrases(sent, min_count=1, progress_per=50000)\n",
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d439e1da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-0a943fa732d4>:12: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  w2v_model.init_sims(replace=True)\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec(min_count=3,\n",
    "                     window=4,\n",
    "                     #size=300,\n",
    "                     sample=1e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=multiprocessing.cpu_count()-1)\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=50000)\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "w2v_model.init_sims(replace=True)\n",
    "w2v_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b52abdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_export = output_df.copy()\n",
    "file_export['old_parent_answer'] = file_export.parent_answer\n",
    "file_export.old_parent_answer = file_export.old_parent_answer.str.join(' ')\n",
    "file_export.parent_answer = file_export.parent_answer.apply(lambda x: ' '.join(bigram[x]))\n",
    "file_export[['parent_answer']].to_csv('cleaned_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade7db53",
   "metadata": {},
   "source": [
    "# KMeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56ac9bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = Word2Vec.load(\"word2vec.model\").wv\n",
    "model = KMeans(n_clusters=2, max_iter=1000, random_state=True, n_init=50).fit(X=word_vectors.vectors.astype('double'))\n",
    "#word_vectors.similar_by_vector(model.cluster_centers_[1], topn=30, restrict_vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ac06b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_cluster_index = 1\n",
    "positive_cluster_center = model.cluster_centers_[positive_cluster_index]\n",
    "negative_cluster_center = model.cluster_centers_[1-positive_cluster_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e16e12a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame(word_vectors.index_to_key)\n",
    "words.columns = ['words']\n",
    "words['vectors'] = words.words.apply(lambda x: word_vectors[f'{x}'])\n",
    "words['cluster'] = words.vectors.apply(lambda x: model.predict([np.array(x)]))\n",
    "words.cluster = words.cluster.apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1728a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "words['cluster_value'] = [1 if i==positive_cluster_index else -1 for i in words.cluster]\n",
    "words['closeness_score'] = words.apply(lambda x: 1/(model.transform([x.vectors]).min()), axis=1)\n",
    "words['sentiment_coeff'] = words.closeness_score * words.cluster_value\n",
    "words[['words', 'sentiment_coeff']].to_csv('sentiment_dictionary.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd624a9f",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2612037b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the files\n",
    "final_file = pd.read_csv('cleaned_dataset.csv')\n",
    "sentiment_map = pd.read_csv('sentiment_dictionary.csv')\n",
    "sentiment_dict = dict(zip(sentiment_map.words.values, sentiment_map.sentiment_coeff.values))\n",
    "file_weighting = final_file.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2e92ae",
   "metadata": {},
   "source": [
    "### Getting tfidf scores of words in every sentence, and replacing them with their associated tfidf weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e0db424",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Apps\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:489: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=lambda y: y.split(), norm=None)\n",
    "tfidf.fit(file_weighting.parent_answer)\n",
    "features = pd.Series(tfidf.get_feature_names())\n",
    "transformed = tfidf.transform(file_weighting.parent_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cf5e09",
   "metadata": {},
   "source": [
    "### Replacing words in sentences with their tfidf scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4999ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_dictionary(x, transformed_file, features):\n",
    "    '''\n",
    "    create dictionary for each input sentence x, where each word has assigned its tfidf score\n",
    "    \n",
    "    inspired  by function from this wonderful article: \n",
    "    https://medium.com/analytics-vidhya/automated-keyword-extraction-from-articles-using-nlp-bfd864f41b34\n",
    "    \n",
    "    x - row of dataframe, containing sentences, and their indexes,\n",
    "    transformed_file - all sentences transformed with TfidfVectorizer\n",
    "    features - names of all words in corpus used in TfidfVectorizer\n",
    "\n",
    "    '''\n",
    "    vector_coo = transformed_file[x.name].tocoo()\n",
    "    vector_coo.col = features.iloc[vector_coo.col].values\n",
    "    dict_from_coo = dict(zip(vector_coo.col, vector_coo.data))\n",
    "    return dict_from_coo\n",
    "\n",
    "def replace_tfidf_words(x, transformed_file, features):\n",
    "    '''\n",
    "    replacing each word with it's calculated tfidf dictionary with scores of each word\n",
    "    x - row of dataframe, containing sentences, and their indexes,\n",
    "    transformed_file - all sentences transformed with TfidfVectorizer\n",
    "    features - names of all words in corpus used in TfidfVectorizer\n",
    "    '''\n",
    "    dictionary = create_tfidf_dictionary(x, transformed_file, features)   \n",
    "    return list(map(lambda y:dictionary[f'{y}'], x.parent_answer.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aeac97",
   "metadata": {},
   "source": [
    "### Replacing words in sentences with their sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a3c09b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_sentiment_words(word, sentiment_dict):\n",
    "    '''\n",
    "    replacing each word with its associated sentiment score from sentiment dict\n",
    "    '''\n",
    "    try:\n",
    "        out = sentiment_dict[word]\n",
    "    except KeyError:\n",
    "        out = 0\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a84b349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing words in sentences with their tfidf scores\n",
    "replaced_closeness_scores = file_weighting.parent_answer.apply(lambda x: list(map(lambda y: replace_sentiment_words(y, sentiment_dict), x.split())))\n",
    "replaced_tfidf_scores = file_weighting.apply(lambda x: replace_tfidf_words(x, transformed, features), axis=1)\n",
    "\n",
    "# Merging both previous steps and getting the predictions:\n",
    "replacement_df = pd.DataFrame(data=[replaced_closeness_scores, replaced_tfidf_scores, file_weighting.parent_answer]).T\n",
    "replacement_df.columns = ['sentiment_coeff', 'tfidf_scores', 'parent_answer']\n",
    "replacement_df['sentiment_rate'] = replacement_df.apply(lambda x: np.array(x.loc['sentiment_coeff']) @ np.array(x.loc['tfidf_scores']), axis=1)\n",
    "replacement_df['prediction'] = (replacement_df.sentiment_rate>0).astype('int8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d567ac6",
   "metadata": {},
   "source": [
    "## Classifying Parents response either Positive or Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba93678c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>focus_group_subtype</th>\n",
       "      <th>focus_group_subtype_id</th>\n",
       "      <th>doc_no_within_subtype</th>\n",
       "      <th>question_id</th>\n",
       "      <th>question_text</th>\n",
       "      <th>parent_num</th>\n",
       "      <th>parent_answer</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gaming_group</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>So, I was thinking we could start by just goin...</td>\n",
       "      <td>5</td>\n",
       "      <td>[sure, hi, name, parent, three, kid, nine, boy...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gaming_group</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>So, I was thinking we could start by just goin...</td>\n",
       "      <td>1</td>\n",
       "      <td>[hi, everyone, name, parent, two, boy, ayearol...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gaming_group</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>So, I was thinking we could start by just goin...</td>\n",
       "      <td>3</td>\n",
       "      <td>[hi, everybody, name, parent, anyearold, daugh...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gaming_group</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>So, I was thinking we could start by just goin...</td>\n",
       "      <td>4</td>\n",
       "      <td>[hi, parent, ayearold, daughter, ayearold, son...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gaming_group</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>So, I was thinking we could start by just goin...</td>\n",
       "      <td>2</td>\n",
       "      <td>[oh, sorry, lost, connection, could, hear, any...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  focus_group_subtype  focus_group_subtype_id  doc_no_within_subtype  \\\n",
       "0        gaming_group                       1                      1   \n",
       "1        gaming_group                       1                      1   \n",
       "2        gaming_group                       1                      1   \n",
       "3        gaming_group                       1                      1   \n",
       "4        gaming_group                       1                      1   \n",
       "\n",
       "   question_id                                      question_text  parent_num  \\\n",
       "0            1  So, I was thinking we could start by just goin...           5   \n",
       "1            1  So, I was thinking we could start by just goin...           1   \n",
       "2            1  So, I was thinking we could start by just goin...           3   \n",
       "3            1  So, I was thinking we could start by just goin...           4   \n",
       "4            1  So, I was thinking we could start by just goin...           2   \n",
       "\n",
       "                                       parent_answer sentiment  \n",
       "0  [sure, hi, name, parent, three, kid, nine, boy...  negative  \n",
       "1  [hi, everyone, name, parent, two, boy, ayearol...  positive  \n",
       "2  [hi, everybody, name, parent, anyearold, daugh...  negative  \n",
       "3  [hi, parent, ayearold, daughter, ayearold, son...  positive  \n",
       "4  [oh, sorry, lost, connection, could, hear, any...  positive  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replacement_df['sentiment'] = ['positive' if val == 1 else 'negative' for val in replacement_df['prediction']]\n",
    "final_df = pd.merge(input_df, replacement_df['sentiment'], left_index=True, right_index=True)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f90bdf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('focus_group_sentiment_output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
