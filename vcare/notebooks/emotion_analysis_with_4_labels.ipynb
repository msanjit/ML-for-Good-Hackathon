{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning pre-trained BERT for Sequence Classification model\n",
    "#### Emotion labels that were used initially-\n",
    "['serenity', 'joy', 'disgust', 'anxious', 'optimism', 'vigilance', 'sad', 'fear']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations from implementation in emotion_analysis.ipynb -\n",
    "#### Model scores were bad mostly due to less annotated data (emotions were added only for subsets of social and media groups' repsonses).\n",
    "#### Training loss was monitored at steps and it was fluctuating up and down.\n",
    "#### Validation loss was decreasing in each eval_step but it was sometimes falling below the training loss.\n",
    "#### Validation loss was not showing an upward trajectory even if the number of epochs was increased, potentially indicating that overfitting wasn't happening so far, which could have been the case as there were a lot of features with all bert layers unfrozen and small dataset size.\n",
    "\n",
    "### Things tried for improving the training loss fluctuation-\n",
    "##### 1] Learning rates - 2e-5, default LR as part of Training Class args (5e-5), 0.0001, 0.001, 0.1\n",
    "##### 2] Training+Validation dataset size tried - 50, 100, 200\n",
    "##### 3] Using BERT instead of DistilBERT\n",
    "##### 4] Changing the max_length for tokens - 512, 250, 200, 100, 80\n",
    "##### 5] Freezing all layers to improve memory requirements but due to training loss appearing to oscillate, unfreezing all layers\n",
    "##### and also freezing only the embeddings and initial trasnformer layers (upto 3) was tried\n",
    "##### 6] experimenting by adding/substracting the following pipeline components for this specific task - stop word removal, lemmatizer, punctuations\n",
    "##### 7] changing the number of epochs - 3, 5, 1, 10, 15, 30\n",
    "##### 8] with limited data, increasing the training set size and very small validation  set size was also tried\n",
    "\n",
    "##### Finally, to check if less training examples given 8 labels was causing an issue,  we reduced the number of labels- \n",
    "##### optimism was mapped to serenity\n",
    "##### responses tagged under vigilance, sadness and fear all involved some level of anxiety and so all 3 were mapped to anxiety\n",
    "\n",
    "#### Emotion labels that were used finally\n",
    "['serenity', 'joy', 'disgust', 'anxious']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.4SP5SUA7CBGXUEOC35YP2ASOICYYEQZZ.gfortran-win_amd64.dll\n",
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.wcdjnk7yvmpzq2me2zzhjjrj3jikndb7.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers import AdamW\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vhpld\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vhpld\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#pip install contractions\n",
    "#import contractions\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def case_handling(df,columns=[]):\n",
    "    \n",
    "    for col in columns:\n",
    "        df[col] = df[col].str.lower() \n",
    "        \n",
    "    return df       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(df,columns=[]):\n",
    "    \n",
    "    for col in columns:\n",
    "        df[col] = df[col].apply(lambda text: re.sub(r'[^\\w\\s]', '', text))\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(df, columns=[]):\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def remove_sw(text):\n",
    "        txt_output = \" \".join([word for word in str(text).split() if word not in stop_words])\n",
    "        return txt_output\n",
    "    \n",
    "    for col in columns:\n",
    "        df[col] = df[col].apply(lambda text: remove_sw(text))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(df, columns=[]):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def lemmatize(text):\n",
    "        text_output = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "        return text_output\n",
    "    \n",
    "    for col in columns:\n",
    "        df[col] = df[col].apply(lambda text: lemmatize(text))\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_spaces(df,columns=[]):\n",
    "    \n",
    "    for col in columns:\n",
    "        df[col] = df[col].apply(lambda text: re.sub(' +', ' ', text))\n",
    "        \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(df, columns=[]):\n",
    "    \n",
    "    #df = expand_contraction(df,columns)\n",
    "    df = case_handling(df,columns) \n",
    "    df = remove_punctuations(df,columns)\n",
    "    #df = remove_words_dgits(df,columns)  \n",
    "    df = remove_stopwords(df,columns) \n",
    "    df = lemmatize_words(df, columns)\n",
    "    df = remove_extra_spaces(df,columns) \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"focus_groups_convos_emotion_analysis.csv\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I have a similar experience to Parent 3. My high schooler had a lot of his homework that he had to type and word process... I don't know if that's the right word, into Docs, but it wasn't a huge amount. It wasn't doing research, it wasn't hours and hours. My elementary school guy, he didn't have any homework on the computer at all. And my middle schooler had very little, like type this one thing or something like that. It was very little on the computer. And for myself, in terms of my family, I lock the video games so that there was very little access to video games during the week.\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# response before pre-processing of text\n",
    "data.iloc[2]['parent_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['parent_answer']\n",
    "data =  data_preprocessing(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>focus_group_subtype</th>\n",
       "      <th>focus_group_subtype_id</th>\n",
       "      <th>doc_no_within_subtype</th>\n",
       "      <th>question_id</th>\n",
       "      <th>question_text</th>\n",
       "      <th>parent_num</th>\n",
       "      <th>parent_answer</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gaming_group</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>So just starting to think through some of the ...</td>\n",
       "      <td>2</td>\n",
       "      <td>oh okay well didnt use much mean teacher would...</td>\n",
       "      <td>serenity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gaming_group</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>So just starting to think through some of the ...</td>\n",
       "      <td>3</td>\n",
       "      <td>would say thing daughter fifth grade would cou...</td>\n",
       "      <td>serenity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gaming_group</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>So just starting to think through some of the ...</td>\n",
       "      <td>5</td>\n",
       "      <td>similar experience parent 3 high schooler lot ...</td>\n",
       "      <td>serenity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gaming_group</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>So just starting to think through some of the ...</td>\n",
       "      <td>5</td>\n",
       "      <td>weekend hour something like wasnt much youtube...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gaming_group</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>So just starting to think through some of the ...</td>\n",
       "      <td>4</td>\n",
       "      <td>go kid probably sound like online far rest dau...</td>\n",
       "      <td>serenity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  focus_group_subtype  focus_group_subtype_id  doc_no_within_subtype  \\\n",
       "0        gaming_group                       1                      1   \n",
       "1        gaming_group                       1                      1   \n",
       "2        gaming_group                       1                      1   \n",
       "3        gaming_group                       1                      1   \n",
       "4        gaming_group                       1                      1   \n",
       "\n",
       "   question_id                                      question_text  parent_num  \\\n",
       "0            1  So just starting to think through some of the ...           2   \n",
       "1            1  So just starting to think through some of the ...           3   \n",
       "2            1  So just starting to think through some of the ...           5   \n",
       "3            1  So just starting to think through some of the ...           5   \n",
       "4            1  So just starting to think through some of the ...           4   \n",
       "\n",
       "                                       parent_answer   emotion  \n",
       "0  oh okay well didnt use much mean teacher would...  serenity  \n",
       "1  would say thing daughter fifth grade would cou...  serenity  \n",
       "2  similar experience parent 3 high schooler lot ...  serenity  \n",
       "3  weekend hour something like wasnt much youtube...       joy  \n",
       "4  go kid probably sound like online far rest dau...  serenity  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'youtube bottomless hole cant even begin bad transition looking google classroom google meet here youtube easy always still job thing cant sit top accessibility technology type content like youtube thats beneficial kid thats problematic say least'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# response after pre-processing of text\n",
    "data['parent_answer'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "anxious     101\n",
       "serenity     71\n",
       "disgust      44\n",
       "joy          13\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['emotion'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'serenity': 0, 'joy': 1, 'disgust': 2, 'anxious': 3}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions = list(data['emotion'].unique())\n",
    "label_to_index = {emotion : index for index, emotion in enumerate(emotions)}\n",
    "label_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['emotion_label_index'] = data.apply(lambda x : label_to_index[x['emotion']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>focus_group_subtype</th>\n",
       "      <th>focus_group_subtype_id</th>\n",
       "      <th>doc_no_within_subtype</th>\n",
       "      <th>question_id</th>\n",
       "      <th>question_text</th>\n",
       "      <th>parent_num</th>\n",
       "      <th>parent_answer</th>\n",
       "      <th>emotion</th>\n",
       "      <th>emotion_label_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gaming_group</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>So just starting to think through some of the ...</td>\n",
       "      <td>2</td>\n",
       "      <td>oh okay well didnt use much mean teacher would...</td>\n",
       "      <td>serenity</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gaming_group</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>So just starting to think through some of the ...</td>\n",
       "      <td>3</td>\n",
       "      <td>would say thing daughter fifth grade would cou...</td>\n",
       "      <td>serenity</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gaming_group</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>So just starting to think through some of the ...</td>\n",
       "      <td>5</td>\n",
       "      <td>similar experience parent 3 high schooler lot ...</td>\n",
       "      <td>serenity</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gaming_group</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>So just starting to think through some of the ...</td>\n",
       "      <td>5</td>\n",
       "      <td>weekend hour something like wasnt much youtube...</td>\n",
       "      <td>joy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gaming_group</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>So just starting to think through some of the ...</td>\n",
       "      <td>4</td>\n",
       "      <td>go kid probably sound like online far rest dau...</td>\n",
       "      <td>serenity</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  focus_group_subtype  focus_group_subtype_id  doc_no_within_subtype  \\\n",
       "0        gaming_group                       1                      1   \n",
       "1        gaming_group                       1                      1   \n",
       "2        gaming_group                       1                      1   \n",
       "3        gaming_group                       1                      1   \n",
       "4        gaming_group                       1                      1   \n",
       "\n",
       "   question_id                                      question_text  parent_num  \\\n",
       "0            1  So just starting to think through some of the ...           2   \n",
       "1            1  So just starting to think through some of the ...           3   \n",
       "2            1  So just starting to think through some of the ...           5   \n",
       "3            1  So just starting to think through some of the ...           5   \n",
       "4            1  So just starting to think through some of the ...           4   \n",
       "\n",
       "                                       parent_answer   emotion  \\\n",
       "0  oh okay well didnt use much mean teacher would...  serenity   \n",
       "1  would say thing daughter fifth grade would cou...  serenity   \n",
       "2  similar experience parent 3 high schooler lot ...  serenity   \n",
       "3  weekend hour something like wasnt much youtube...       joy   \n",
       "4  go kid probably sound like online far rest dau...  serenity   \n",
       "\n",
       "   emotion_label_index  \n",
       "0                    0  \n",
       "1                    0  \n",
       "2                    0  \n",
       "3                    1  \n",
       "4                    0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'serenity', 1: 'joy', 2: 'disgust', 3: 'anxious'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_label = {index: label for label, index in label_to_index.items()}\n",
    "index_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" tried but not used finally\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=len(label_to_index))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\noptimizer = AdamW(filter(lambda p : p.requires_grad, model.parameters()), lr = learning_rate)\\noptim.SGD([\\n                {'params': model.base.parameters()},\\n                {'params': model.classifier.parameters(), 'lr': 1e-3}\\n            ], lr=1e-2, momentum=0.9)\\n\""
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### freezing all layers except the classification head when distilbert was tried\n",
    "for param in model.distilbert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = optim.Adam([{'params' : model.classifier.parameters()}], lr=1)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Following was not used -\n",
    "changing the optimizer\n",
    "differential learning rates\n",
    "\n",
    "optimizer = AdamW(filter(lambda p : p.requires_grad, model.parameters()), lr = learning_rate)\n",
    "optim.SGD([\n",
    "                {'params': model.base.parameters()},\n",
    "                {'params': model.classifier.parameters(), 'lr': 1e-3}\n",
    "            ], lr=1e-2, momentum=0.9)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### freezing only the initial pre-trained distilbert model's layers\n",
    "modules = [model.distilbert.embeddings, model.distilbert.transformer.layer[:2]]\n",
    "for module in modules:\n",
    "    for param in module.parameters():\n",
    "        param.require_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.distilbert??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=len(label_to_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(data[\"parent_answer\"])\n",
    "y = list(data[\"emotion_label_index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"oh okay. well, use much. mean, teacher would assign reading math assignment ready time time homework. yeah, it. would monitor much time roblox thing entertain themselves, easier control back go much. mean, that's was.\",\n",
       " 'would say thing. daughter fifth grade, would couple newsela projects, half hour homework. playground, neighborhood friends, school projects, school swimming. little bit tablet weekends, internet access youtube life, roblox, gaming. so, different world.',\n",
       " \"similar experience parent 3. high schooler lot homework type word process... know that's right word, docs, huge amount. research, hour hours. elementary school guy, homework computer all. middle schooler little, like type one thing something like that. little computer. myself, term family, lock video game little access video game week.\",\n",
       " \"go. kid probably more... sound like online than, far, rest them. daughter high school definitely almost homework... assigned school, definitely homework, feel like, good amount. nothing like now, definitely using it. middle schooler also, much. play game use instagram, daughter really instagram, much anymore definitely games. like free-for-all. mean, they're room door shut, idea they're day. know they're homework, getting good grades. think they're electronics entire day.\",\n",
       " \"right, exactly, seems beneficial important part life grow older high school clearly, middle school parent 4 said, there's use computer. first seemed like good thing, i'm sure good thing, just, like said, become overwhelming use technology sit front day. another thing, use youtube, understand teacher post thing look youtube, can't block youtube it'll used tool one class have.\",\n",
       " \"youtube bottomless hole of... can't even begin, bad. transition looking google classroom google meet here's youtube, easy. always there, still job thing do, can't sit top them. accessibility technology type content, like youtube, that's beneficial kids, that's problematic, say least.\",\n",
       " 'us, desktop laptop, sufficient pre-pandemic. everybody needed device, sudden everybody needed device needed basically time got one device school, bought laptop, found old laptop. like gathering devices. teaching... nine-year-old, teaching use it, really used computer way. teaching use it. middle son prone migraine screen caused migraine caused aversion want on.',\n",
       " 'schedule kind crazy, like place. truth hard 17-year-old, knew thing kind stayed room months. just... feel bad saying it, kind left devices. big social medium guy, social medium still, way communicate peer became use... it? discord, things, became way communicate people. almost, feel like... like parent 1 said, like black hole suck in.',\n",
       " 'sure, lock happened first grader, school great job pivoting remote quite quickly. used seesaw platform. get two laptop doe lot tech old kind fragile i\\'m like, they\\'re going wreck all. nothing insured. we\\'re self-employed could see whole business wiped take doe ipads lifesaver. also, again, got one point computer teacher youtube... youtube sent one link figured out, \"oh, get youtube link.\"',\n",
       " \"like said, kid day. they're fully remote, them. neither one full schedule hear son... desk right next room, i'm hearing like 10:00 morning, profanity game friend. i'm like, understand, school day. well, school, break. definitely made day... especially since i'm working right next him, difficult try job next him. lives, they're rooms, school playing games. son play lot game friends, daughter she'll youtube also lot, ticktock, whatever, kind see lot day. thankfully okay ask... used involved i'm really involved. free all.\",\n",
       " \"absolutely. want, finish that. actually i'm problem son school said... requested, say it, depending size class might able go one day. go one day week now. able get two days. fit got, screaming five day day came, horrible. [inaudible 00:33:53] worst, making go two day instead one, imagine what's going happen next year.\",\n",
       " \"i'm sorry. sons, actually go school five day week. initially blended issues, issue constantly youtube, constantly playing roblox. supposed work would go would see clicking youtube window. received two laptop doe well, kind limited desktop laptop. take turn going computer allowed to. went computer mainly weekend play roblox often.\",\n",
       " 'everything shut much entertainment besides television talking friend roblox discord whatever. yeah, consumed lot time. daughter, three, send babysitter anymore, home working home would force look youtube too, watch television. kind really felt bad try always turn something educational, like sesame street. started telling wanted watch while, nick jr. netflix, became problem.',\n",
       " \"yeah, thing google classroom. probably recent, past month two maybe, get daily update end day middle schooler. least see he's doing, probably ever got school, definitely school, before. so, that's nice least get daily rundown of... might sentence two each, least something. so, that's nice. also like get immediate response basically teacher question homework questions... whatever might be, everyone pretty receptive responding quick time period.\",\n",
       " 'son, google classroom, good experience would put camera on, would hide behind camera. would participate. reach g chat message whatever. part working u well. however, someone idea overheard, found tutor, another teacher another school tutoring. ability tutoring fingertip without commute travel, go wait sit car been... that, ability one one connects like this.',\n",
       " 'daughter never gaming before. mean, year ago minecraft, anything recently. stuff online lot, far gaming, no. son, positive thing guess fight much. used to... necessarily positive, kind contradictory, even tell get anymore. used fight used tell get off, don\\'t... fighting actually gone down. flip side he\\'s much more. language got worse game i\\'m constantly stop it. also gotten little closer friend can\\'t see anymore they\\'re online talking. mom say, \"did hear this,\" there\\'s problem. mean, he\\'s definitely playing more, played lot beforehand too, like this.',\n",
       " \"beginning. beginning this, was. realized there's nothing else do... mean, one point broken ankle even play sports, even take walk. i'm like, going to... unless i'm prepared sit entertain teach myself, can't i'm working, choice.\",\n",
       " 'mentioned, play lot, certainly video involve multiplayer whatever, chat together. exposure that. can\\'t again, seven year old, he\\'s eight now, maybe little early. year blossomed, right word, he\\'s playing lot. think it, again, may fault good babysitter. trying get work, jobs, demanding jobs, easier to... instead standing trying control gaming, just, \"okay, he\\'s busy i\\'m going get work done.\" think lasted think kind gave convenience. truly... also get job done. know?',\n",
       " 'say initially high schooler felt like got almost depressed. good academic student, social issue academically well, 90-something average. started failing classes. attending, going, bed day. would talk one said, \"well, try maybe connecting friends?\" social medium like, \"why try connecting friends? maybe play game send message see they\\'re doing.\" helped, connect, connection this...',\n",
       " 'even know, play minecraft think. like two hours, four hours, six hours, eight hours, 10 hours, 12 hours, 14 hours, 16 hours. crazy. much so, took boy florida, drove florida summer see brother even brother\\'s house, cousin pool stuff like would spend 10 hours, 12 hours, 14 hour day stuck room. would get upset would say, \"no, we\\'re going go out. we\\'re going take walk. let\\'s go swimming.\" became... still hard get handle 17. very, hard, feel, get handle it. i\\'m like, \"you need go bed.\" even last night, 1:00 morning go, \"you\\'re going go school tomorrow, go bed.\"',\n",
       " 'like, going rip laptop room we\\'re going fight? want that, tried couple time worth me. say him, \"look, becoming young adult. going make decision yourself. life, can\\'t game 16 hour day.\" younger one also feel control, feel control 17-year-old he\\'s like six foot tall, hasâ€“',\n",
       " \"i'm saying... i'm specifying one two children, different come this. her, even textbook online good thing. still requested paper textbook her, anytime screen never really positive. smartboards, thing like school aside, obviously technology think they're positive. literally... school went complete zoom, like did... although went faster know district did. friday school closed monday school zoom. whole time run full day, exactly like would school, know increase zoom fatigue, her... shut down. stopped work. stopped anything. would sign zooms, like even there. would screen open device open. she'd buzzfeed something else. would show extracurriculars zoom, she'd thing. present. present family. present anything. completely shut\",\n",
       " \"think son, he's asking... right he's smaller school, he's asking go back, go one big, huge, school staten island? i'm worried put larger school he'll lose gain he's made far academic concerned. counter that, see maturity has... see growth last year, wonder he's going handle big school, small school, whatever\",\n",
       " 'okay. three children. one turned two march. he\\'s going remote learning, use technology. already tablet. already know work everybody\\'s phone. facetime. that. alone used get up, \"let\\'s watch channel 13, sesame street,\" educational programs, stuff like that. he\\'s like, \"no.\" need tablet homework, tell us. need tablet homework. he\\'s mimicking everybody else, see everybody else device, something device. daughter, 10. go [success academy 00:33:15], remote learning longest. success academy one first charter go remote, even mayor everybody else going remote thing. stopped kid coming in. already going remote. daughter\\'s 10 remote learning year half now. feel like counter productive, obsessively stay back. give kid itinerary want... like go college, get course itinerary. got go on. got responsible assignment turning time. stuff like that. that\\'s they\\'re running program. feel like self starters... kid ball give direction go follow without anything... good type kids. kid guide five minute get side tracked, counter productive got stay back. know exactly supposed know properly. stuff like that. got kid staring screen day. they\\'re zoom class, staying attentive',\n",
       " \"yeah. she'll whatever they're doing. feel like adult too. guiding off. looking youtube. checking facebook. looking instagram. supposed math\",\n",
       " 'kid zoom full time like everybody else last year march june. definitely huge adjustment. definitely able attend well far school. screen usage rally picked up. screen. kids, then, access screen capacity. many screen house. one tv. two kid phones. intense access awareness could screen zoom started. zoom incredibly difficult keep up, study, learn, know. know flying. like mother spoke said, top everybody\\'s zoom. six kids. impossible remote work supposed working. year kid school. go private school. they\\'re school. save quarantine go through, school go zoom short while. goal get kid back classroom. screen situation really get much better. feel comfortable communicating screen, now, friends. need facetime. need zoom. actually zoom group friend now. study can\\'t study own. [crosstalk 00:38:12] roblox. 11 year old definitely far hardest them. went computer watcher becoming... roblox thing want time. she\\'ll grab phone say, \"i want text friends\", inevitably end gaming. hate it. say gaming friends. she\\'ll game facetime time. one else play roblox her. allow direct message anybody accept anybody\\'s message roblox. became huge... battle. there\\'s saving that. allow social medium kids. just... everyday. came point week ago actually removed computer house, need zoom. they\\'re zoom... 11 year old deal it. like... literally took... three laptops, sent husband\\'s office. said keep there. told he\\'s allowed bring home laptop unless really work. help, but... we\\'re ultimate technologically connected',\n",
       " \"he's not... covid, now, he's much prone pick computer anything, make transactions, scroll, anything like that, phone... gave phone, cell phone carrier app phone control access. still place. bedtime can't access internet. shut site one one can't access. we've still that. good thing social medium covid could still access friends... could talk still share life with. found positive\",\n",
       " 'well would like say something that. believe name miss parent 6, spoke me. mentioned 11 year old roblox... oh goodness. covid, social medium daughter reward. like, \"oh mommy, finished whatever do. took care responsibilities. use phone now?\" would give phone, would able to, \"can make tiktok? this, that?\" now, device consistently, me, slightly took away piece control level. screen time, click see. see clicking. see doing. see looking at. see watch, i\\'m presumption sitting thing work. looking screen supposed work, homework, that... could looking screen guiding off. there\\'s couple time class guiding off, teacher\\'s texting me, \"we\\'re math [genesis 00:43:55] trying get instagram.\" feel like changed dynamic they\\'re doing. like, mean? need phone. need this, got that. got check now. i\\'m like, none important. what\\'s important work? check work? submit work? done that, free time want phone. want play roblox... even incorporated school. school used little time get reward, maybe play game, something like could interact other. got phone. got roblox club. got this, got that. give kid 20 minutes-30 minute free time kid could inter... even though we\\'re zoom... could\\'ve done class, play math game. while. everybody\\'s breakout room, guy got 30 minute play roblox together. feel like taken over. [crosstalk 00:45:23] feel like there\\'s separation time supposed certain thing',\n",
       " 'let pick device whatever want it. even asking permission anymore. i\\'ve really noticed change two year old, baby... whenever put phone down, pick phone up. call roblox, says, \"i want play mario.\" phone, mind you, phone password everything. baby pick phone somehow get roblox every time. playing roblox phone. learn behavior 10 year old',\n",
       " '[crosstalk 00:47:00] allow kid instagram, tiktok, that. also consider whatsapp social medium [crosstalk 00:47:10] intense pressure. 16 year old 14 year old. find that... grew without phone understand home, necessarily communicate world every little thing we\\'re doing. every little thing we\\'re eating. find intense pressure time. time something relevant, something that\\'s cool, something they\\'re eating, something they\\'re showing, selfie. find endless. definitely became worse now, zoom spending time room. they\\'re still spending time room. i\\'m like, come up? two oldest girls, room basement. i\\'m like, \"i need come upstairs. come room, put phone down.\" actually started new policy need charge phone kitchen they\\'re sleeping. even screen time boundary put them, somehow manage something other. one daughter actually... i\\'m sabbath observant, use phone friday night saturday night. touch phones. turn phones. use electric. use television. use anything. actually biggest blessing world religious requirement. we\\'re bound. it. something that... able release phone birth. use phone friday night saturday. find nights, there\\'s much more... there\\'s calm. see there\\'s le pressure connect. never connected like society kids. run home homework. could barely get phone sister phone. [crosstalk 00:49:24] negative use phone. there\\'s lot battle. lot battle. minute sabbath ends, 11 year old device, coming saying, \"can please unlock device use it?\" worry they\\'re going turn out, think devices... said something good social media. think there\\'s good social media,',\n",
       " 'also think we\\'ve relaxed... least relaxed rule time i\\'ve allowed kid thing [crosstalk 00:51:00] least. point, entertains instead entertain them, especially beginning covid friends. right? neighbor kept making fun much time spending outside playing basketball, relay races, collecting golf balls, thing keep busy devices. point, work, dish wash, need five minute entertaining anyone, okay, use little bit more. ridiculous streak things. message snapchat every day break streak. he\\'ll sit streaks. my... one us device less, watch simpson walk steps, get social medium talk friend particularly funny part. i\\'m like, \"you\\'re going fall get killed.\" lot fight left home year. know mean? i\\'m wondering even going possible pull back covid, go back normal life. loosened restriction much harder tighten again? see good them. see it. know, sometimes energy anything about. completely uncontrollable. [crosstalk 00:52:22] best therapist world instagram. follows every anxiety instagram page is, think they\\'re wisest [crosstalk 00:52:27] come that. again, i\\'ve locked phone... phone... screen time things. she\\'ll sit hour trying figure [crosstalk 00:52:38]',\n",
       " 'know relate much saying. feel like daughter addicted telephone. like, \"give phone\", like, \"why? happened?\" found video phone daughter cry could find telephone. like, \"the baby phone, can\\'t find it.\" utterly distraught. age even phone. grow cell phones. addicted device itself. \"let take picture food. let take picture this. let make tiktok right quick. let say we\\'re going here.\" put phone down. can\\'t even eat food without phone hand. tell device phone. like miss parent 4 said, house. texting back room? wrong you? becoming obsession. feel like generation, grow phone. remember used ask permission even use house phone certain time. chores, homework. know i\\'m saying? picking phone, using it. really believe covid, pandemic era, causing problems. they\\'re foreseen problem right now, definitely causing problem',\n",
       " \"really quickly. another concern mine social medium son 16, there's element sexuality that's coming play saw last year. know circulate picture young ladies. i've talked about... would considered trafficking, felony. [crosstalk 01:00:49] tried warn called one parent daughter. they're sending picture back forth lead bullying kind trauma drama. wanted mention that's concern\",\n",
       " \"think that's two part challenge. teach kid careful social footprint. understand anything put social medium never, ever, going away. [crosstalk 01:01:34] college see it, job see it, friend see it. never going away. thoughtful careful share, hard lesson teach them. discover already something really smart, already. right? think extra hard grow this. grow opportunity ruin life kid\",\n",
       " 'okay, i\\'ll go again. forgot say child 3 who\\'s one i\\'m speaking today, diagnosed ocd anxiety stuff like that. he\\'s 18, like said. thinking reevaluated honestly think may spectrum due behaviors. anyway, wrote positive stay touch friends, play games, use brain bit, schoolwork. negatively he\\'s constantly device. really want leave house much anymore. worst summer last year he, guess, know computer phone, almost breakdown honestly thought going admit hospital became obsessed thinking cursed. watching video people would say, \"if see doll look thing, going cursed.\" he\\'s always issues, was, mean, honestly, bizarre that. get psychiatric help. started taking medication. see therapist psychiatrist just, like said, always issues, think whole corona stuck house on, therefore then, computer phone much, know mental state. mean, really bad effect.',\n",
       " \"yes, actually, really resonated me. say positives, want segue talking found things. child 1 youngest six boy have. health issues, exposing covid real concern. one positive computer could least talk people age. something fill time, filling hour nerve-wracking me. sometimes would hear laugh stuff, would bring joy. thing happened child 1 never seen also became obsessed vampire going come get him. thing that, forget woman's name me, apologize. saw thing scared. took technology away got better. know online. i'm older mom, know much technology they're doing, really concerning scary. time, feel bad they're depressed anybody, anyone talk to. input, interesting heard somebody say thing dealt covid.\",\n",
       " \"share it. i'm going make connection said, son really anything specific brought out, nightmare afraid sleep alone. i'm wondering may something came across online. give positive negatives, positive actually negative well. he's savvy, technologically savvy. good need help much. interfere much that. he's creative. sometimes get online, really creative things. thought sometimes good occupying him, get hair. i'm trying work remote he's here. good. negative he's little savvy little sneaky little manipulative it. know, i'm savvy is. get easily frustrated. he's playing game he's winning going way wants, he'll start banging thing hitting computer computer's fault. again, much time. spent much time it.\",\n",
       " \"well, start that. child 1â€™s problem problem. thing he's good cognitive disorder, he's unable retain information. worst subject math. think while, learned calculator, cheating way math, became problem. now, he's homeschooled seems use modern tech, google everything instead learning. becomes harder unfortunately i'm medical fields. i'm able work home. i'm home him. he's basically alone day. that's started pandemic. pandemic, became worse he's again, googling everything, using computers. he's really learning much. that's have.\",\n",
       " \"frustrating. frustrating. child 1 withdrew lot, lost frends. child 1 social. couple friend have, kind separated. started see le le child 1. mean, live really big house, three floor day would come home work even come room, unless went in. go in, agitated. came back vacation guess week away me, past two days, we've closer. know long that's going last. pandemic, definitely become withdrawn.\",\n",
       " \"i'll say child 3, graduated high school last june. started, kind back question one. started college last fall, did. signed semester got overwhelmed computer. dropped withdrew without u even knowing. point refunds. also getting excelsior scholarship, able get anymore. really know he's ... point, we're trying figure he's going do. daughter well, go college. couple class person. really well though, love school. younger daughter, high school. they're worst program right now, whether blended virtual, everything virtual. kid go class go in, computer headset teacher home. believe even daughter issues, junior next year difficult. anxiety starting already go back, even though sick home\",\n",
       " \"feel like really finished covid. even son go school two day week, say sits front computer school headphones. teacher come room. technology blessing curse yeah, allowed go school physically. they're physically school, feel like technology going play even important role want. think really important continue learning physical mean like teacher. know technology big subject too. technology subject kid, huge now. hope find way really use best ability kid get need do. even home, mentioned thing technology home difference going back school stuff. i'm actually really shocked he's able view youtube school computer. really put many firewall lot thing would've thought. he's class, i'm monitoring, he's got another window open watching stupid youtube videos, hope he's school\",\n",
       " \"agree child 1's mom said. found prior pandemic, son would play like video game friends. pandemic hit, exploded like world social medium video games. let kid really reach use whatsapp messenger much, afraid might come contact people know can't stop ... three them, can't control three them. definitely agree child 1's mom said much control. even try limit time, like can't tame beast.\",\n",
       " \"thing feel kind guilty try tame activity have? they're going outside. able vaccinate child 1 yet. i'm kind iffy going around kids. like no-win situation. take away them, else going do?\",\n",
       " \"think son like excited first. kind lost novelty. things. mean, he's still devices, find new thing time kind get obsessed with.\",\n",
       " \"i'm sorry. know is. i've seen matter disorder have, reason genius handle whatever internet throw way. swear child 1 cannot remember left right sometimes, sure remember every app, every song. crazy. understand it.\",\n",
       " \"kind agree parents. prior covid, kept son child 1, sheltered. feel like social medium made aware angry, new technology, like said, he's eight going 40. mind's like sponge. made aware world i've sheltered from. angry scared worried panicky. know that's thing, i'm still learning everything he's diagnosed with. make panicky.\",\n",
       " \"yeah. wanted buy like whatever could well famous youtuber. mean, really find that, understand whole feeling like, else give do? do, feel guilty. they're sitting house, disability. son himself. lot brothers, they're here. really, really hard. yeah, wanted buy likes. know much costs, buying likes.\",\n",
       " \"speak. i- [crosstalk 00:17:26] sure. 16-year-old, think he... mean, guess must've used technology before, but... guess used google classroom stuff, much obviously pandemic. always pupilpath different thing to, sort of, follow assignments. think pandemic, son, 16-year-old, adhd. so, two screen room. one screen, there's zoom class. screen, he's playing video games. me, frustrating been... can't take technology away need school. that's frustrating part whole experience. feel like he's really learning much because, well, mean, would say he's playing video game they're trying teach him. say conducive learning, so.\",\n",
       " \"never, definitely eight-year-old, never used technology school. even homework age. 13-year-old, also add, adhd iep stuff. so, would homework school. even know he'd used term technology, use home. that's sure. afterwards, use for... situation two screens, would chromebook open school. can't helicopter him, so-\",\n",
       " 'i\\'m working full-time home. i\\'m jumping mom hat, work hat. go room keep door open can\\'t trust him. sneak another screen going. like crack, like drug, like it. i\\'m like, \"you play school, video games. would think school hour home?\" he\\'s, \"of course. course, mom.\" anyway. there\\'s sneaking. really, can\\'t put limit that\\'s school. also socialize. he\\'s seeing friend school. how... anyway, think almost non-existence term education.',\n",
       " \"that's outlet, even now. that's it... know that's would normally anyway, age era live in. digital age, maybe would anyway, but-\",\n",
       " 'sure. pretty similar guy saying. son oldest, he\\'s 12, really use technology much school. he\\'s adhd, well dyslexic. purposefully, avoided technology process writing better. use apps reading, like learning ally reading stuff. basically, one positive thing he\\'s definitely learned type, amazing. first would watch him. tedious. painful. him, part would positive, like learning type, actually helped little organized work google classroom instead forgetting paper school. so, would get notification teacher work do, would right there. something he\\'d written forgotten whatever. lot ways, sort helped little accountable face. could look see help him. know say something, could see something forgot had, right? negative definitely saying, far distracted like... hopeless. tried hard beginning keep video games. him, total distraction, somehow able manage keep schoolwork. know it, definitely schoolwork, addition watching video teacher talking, make sense. would drive batty, thing. i\\'m trying work can\\'t stay top him. tried screen time things, would find way around it. greatest one figuring google docs. figured could import youtube video google doc watch them, even though turned youtube. one day, i\\'m like, \"how watching that?\" even told me, like, \"oh!\" like proud, like, \"that\\'s great figured out, it\\'s-\" [crosstalk 00:25:52]',\n",
       " 'mean school, made u feel better they\\'re like, \"we\\'re dealing kids.\" like, well, they\\'re well aware. lot ways, changed strategy make thing little interactive instead little less, sort cut dry, engage more, helped somewhat, but. anyways, daughter, know, totally addicted device, didn\\'t... well, family ipad before, happened, 10, almost 11. eventually... device could school stuff addicted it. little tricky definitely become... hard motivate thing like... gratification there. easy. know. feel like that\\'s hardest part scale back, it?',\n",
       " 'well, since i\\'m already muting, think going forward, never, going ever way was. we\\'re whole new life, basically. think i\\'m ever going get ipad back. that\\'s gone, that\\'s bummer. think i\\'m going see saw end year went back. think i\\'m going see fall, meaning towards end year, son failing school. think anybody monitoring him. least home, monitoring little bit. enough like, \"did this? that?\" well, i\\'m helicopter parent anyway, i\\'m certainly not... i\\'m thinking he\\'s got special ed teacher keep track make write need end every day. found missing entire homeworks, entire reports, would ask him, would, \"oh yeah, no, turned everything in. everything\\'s totally fine.\" get email school saying, \"he\\'s got f,\" \"he\\'s got d.\" he\\'s missing six things. anticipate going similar that. parent 2 mentioned dyslexia, son also dyslexia. diagnosed, though. one flip things. much weird. know. anyways, he\\'s wilson book programs. so, summer school it, he\\'s grade three, really bad. so, mean, guy barely read, even reads, can\\'t comprehend. feel like i\\'m already starting huge catch-up unrealistic. he\\'s going catch up. going take year get there. going take lot summer school, so. mean, think actually anticipate he\\'s going need outside help tutors, people specifically trained deficiencies, adhd focusing reading comprehension, really, unlearn way learned relearn different way wilson, think i\\'m anticipating shit show',\n",
       " \"another thing, everybody going struggle. kids, need little bit support, going hard time. just, he's going get extra support sport school, sure, make sure iep. fight him, like nobody's business. he's going need even outside support including me, husband, sort thing.\",\n",
       " \"absolutely, absolutely. think i'm going fight tooth nail get everything needs. i'm going get second job pay tutoring he's going need, so. yeah. that's, i'm anticipating [crosstalk 00:33:33].\",\n",
       " 'one thing would say sort add thing daughter\\'s 10, fifth grade. one negative thing saw teacher actually helped little think lot stare screen time. daughter started, know, worrying thing herself, like \"my eyebrow too... can\\'t see my...\", crazy things. looking friend straight time. become sort conversation teacher. pretty strict first trying get turn camera on. think became little lenient sort realized kid camera still paying attention, case daughter, probably son. daughter definitely, would needed time. think kind of, know, weird... became little self-conscious, think, know. interesting also see difference daughter\\'s teacher son\\'s teachers. way daily, time class in-person zoom. whereas daughter little more... in-person, was, know, half amount time probably. know. work independently did. know.',\n",
       " \"think far challenges, think social. right now, hard get kid to... know. feel like they've become hermit ways. hard get think outside video game box. could give list thing can't. know, seem want anything, think that's going challenge. mean, really social, think.\",\n",
       " 'i\\'ll go. think, first all, need remember \\'80s, we\\'re totally different era without covid. think online time. talk adult young, much younger us, still 20 things. that\\'s live. they\\'re constantly online, work gaming. cases, think that\\'s going new norm, bad gaming teach strategy conflict resolution sort things. mean, go one way other. term gaming, though. think i\\'m seeing youngest, eight-year-old much addicted ever was. before. yes, ipad. yes, could easily take away him. now, almost like chromebooks became gateway drug whole, like video game addiction thing. kid complete addicts, complete. take away them, go absolute withdrawals, point i\\'m like, \"whoa, appropriate reaction what\\'s going on.\" world ending apparently, turn internet, so. think sort chromebooks came figured use them, teacher taught use them. they\\'ve got skill use good evil. they\\'re using both. find eight-year-old, would never lie sneak, sneak ipad, sneak video games. still admits it, soon, admit it. say, \"oh mom, last night woke up, played video.\" point, he\\'s going start lying. one actually cut knees. say, \"everything\\'s shut 10 minutes.\" otherwise, 3:00 am, every single night playing, playing, playing. friends. that\\'s understand. i\\'m like, \"who playing with?\" he\\'s like, \"all friends.\" they\\'re all. know what\\'s going on, everybody constantly video games. that\\'s interact socially. we\\'re play date yet anymore, getting together socially outside yet. maybe everybody tends younger kids. want expose vaccinated. mean, vaccinated anything, totally get. still need socialization. i\\'m stuck, i\\'m stuck both. think going get worse, think need embrace try figure they\\'re really getting positive way emphasize that. so, think lot... can\\'t think now, read article really, pro-gaming, awesome. like, \"oh, never thought that.\" strategy conflict resolution. brother lived... really close, moved far away other. one thing common video games, talk every day. that\\'s good thing. think there\\'s lot positive people forget about. flip side, need be-',\n",
       " \"flip side, need able control it. hard. almost like... like drugs, cigarettes, something like that, like sugar caffeine. can't avoid sugar. little bit it, got able control it. so, could look like, sugar good thing, sometimes make thing taste good, balance way figuring out. so, would love help school child mind institute.\",\n",
       " 'no, mean, think son way addicted covid still addicted video gaming before.',\n",
       " 'pandemic, use technology education home kids. left school. mean, trusted school. knew computer class, think that\\'s important, left them. me, used almost exclusively entertainment. like needed get something done, like make dinner, something, needed occupied, it. mean, context, think, matters, i\\'m single parent lived apartment brooklyn. so, option saying, \"go outside play,\" there. inside something, inside me. that\\'s changed too, we\\'ve moved since then. mean, rarely would use maybe youtube, something, show something asked about, answer words. use much education, so...',\n",
       " 'beforehand, seven year old use computer all. delighted, read american academy pediatrics said screen time child age 10. so, stood that. zero screen time. 11 year old... 10 year old, oh god, to, would use computer typing handing assignment sometimes, start using google classroom, think, least print thing out, think. started using computer. research, say nice. remember, school, like 15 year old encyclopedia, half thing needed. suddenly internet meant could really get information needed. that, actually, nice that. however, open worms, struggle. still, ladies, difference like getting hit tsunami. suddenly, everything computer. so...',\n",
       " 'yeah. similar parent 5 parent 3, mean, kid would borrow phone. chromebook ipad happened. got chromebooks school. mean, i\\'m working home. three kid juggle. conference calls, keep quiet. got ipads roadblock happened, that\\'s communicate friends. went downhill, huge thing can\\'t... they\\'re addicted. run home school, \"where\\'s ipad? want play.\" it\\'s, i\\'m still working. give little bit, want get off. mean, really... would play game or... school, minimal start. then, home chromebooks, knew, like parent 3 said, knew get around things. get games, thing want on. yeah, like lost battle. wanted clarify, said technology, felt better, guess referring me, job. like, \"oh, man, i\\'ve always wanted work home.\" so, great thing home. lot better they\\'re back school. kids, mean, just, much them. way much.',\n",
       " 'pandemic, beginning eighth grade, pandemic started second semester, whatever, eighth grade. used rarely, rarely, home... would computer school, definitely. group project research, learning research computer. mean, would mostly look like, math concept? know as? something help figure this, something like that. size it. already games, talking friend games, video games, pandemic. 24/7, know?',\n",
       " \"wanted respond parent 3's comment, son also adhd, differently, anxiety odd well. so, online learning complete disaster him. i've actually done two separate schools, brooklyn, spring, we've moved north carolina. so, even one school, computer, that's big issue, tablet. can't focus live asynchronous activity computer option available. mean, think, theory, single... kind computer device teacher controlled, freedom student change could look at, could work, maybe, doesn't. that. click around anything except he's supposed looking at. parent 5, child 2, need leave right now, making much noise. out.\",\n",
       " 'no, out. out. find pencil there. also would get walk away middle things. problem lying me. he\\'d tell me, \"oh yeah, class done. signed meeting over.\" one school, pair call try work one-on-one phone. terrible cell phone bad connection, difficult, would hang constantly. rude, would call say, \"he hung me.\" second school, nothing, absolutely nothing take iep translate online. said, \"we can\\'t. there\\'s nothing do.\" extremely frustrating. so, he\\'s falling behind, way behind, term was. know he\\'s lost learning, making progress. is, need in-person. now, returned in-person four day week, stopped using computers. sit desk, six foot apart, every activity computer-based, except recess lunch. surprise, favorite subject now. mean, think there\\'s tendency always would be. so, maybe technology part. shame, computer, screen great learning tool. like parent 4 saying, old school, pandemic, kindergarten research project, parent helped with. one them. kid going get fish tank, kid chose fish wanted research. so, that\\'s did. sat next computer said, \"what\\'s fish?\" found picture together computer, print out. looked information. find big fish. cut string, click fish tank see would fit fish tank. amazing use computer adult assistance. that\\'s we\\'re getting though, point. so, i\\'ll stop.',\n",
       " 'yeah. mean, online learning disaster end eighth grade, ninth grade now, first year high school, better people presence, classroom, that\\'s motivating, engaging, thing, lying. constantly monitoring, trying find the, \"hey?\" teacher acting like, \"you this. letting know.\" nightmare. nightmare. also, burgeoning independence, used walk school house brooklyn, like five blocks. he\\'d go come home himself, eighth grade. term, roof u 24/7, mean, horrible. he\\'s started back two day every friday week. so, he\\'s taking subway astoria, high school. that\\'s helpful. thing, they\\'re room laptop. whether home school, taught... there\\'s roomy zoomies, class screen, screen, screen. so, problematic. problematic. iep. add. counselor call up, they\\'ve offered help blow this, return email. keep trying positively engage reminding this, other. much. follow through. think would get lot help school, motivated and... know?',\n",
       " \"daughter, oldest odd, anxiety. so, yeah, learning home, disaster. went fifth grade one teacher every subject, really smart kid. easy. put minimum, got bs. fine. now, terrible transition middle school. terrible. mean, there's like six different teachers. got executive functioning. disaster, that. longer organized. real challenge. able to... september, went back school every day. twice week. march, got start going school every day, helped get routine. iep meeting set friday, gather extra help. mean, still battle even back school. getting extra attention needs. just, awful transition. maybe kid's like... say it? normal, extra needs, maybe went okay. know daughter, transition well. want say one younger daughter, anxious. like stay close me, always want know am, leave house, i'm going. so, home school, actually, best one. great school, home, actually, time every meeting. knew sign log in. think much comfortable home time. so, nice, see her.\",\n",
       " \"mean, would say us, like mentioned before, long swipe homework, that's definitely helpful. that, agree there's really need, take care everything school. so, neatness' sake, type print out. that's fine. report research, i'm okay that, anything else feel like unnecessary. speak parent 5 parent 2 said, son also odd, gotten worse we've home. like said, weird thing dyslexia improving thing he's managing, stuff falling apart. so, attentive, would send school. forgets, loses mask, put hand mouth. get bored, start jumping, rolling around. feasible send now, much would love in-person experience. yeah, definitely agree computer must go, least home. care school. sometimes behave better school. they're compliant, come home, want relax. so, little bit different. would perfectly happy keep bare, always catch up, there's computer everywhere. so, fear they'll falling behind knowledge experience.\",\n",
       " 'absolutely want thing house, especially school issued one can\\'t institute control over. really bugged me. understood, sense want also messing machine, can\\'t keep kid inappropriate youtube sites, appropriate tool. however, i\\'m trying think about, could positive way? feel obligated. medium study thing. i\\'m anti medium studies, teach college students, teach ignorance. there\\'s big difference. think problem online reading program mass problems, me, well, there\\'s two things. one, think kid get real feedback. yes, game tell got question right wrong, think effect person relationship with, teacher, expressing that, \"wow, i\\'ve noticed improving.\" or, \"i noticed well today. sleeping?\" whatever is, kind response feedback really crucial, especially younger are, instant little... game fanfare stuff fine. two is, parent, idea they\\'re online. get print here. like getting papers. got much paper this, would get home like, \"oh child wrote sentences.\" or, \"these math worksheets. see they\\'re doing, they\\'re struggling with. see handwriting improving.\" can\\'t see anything. teacher report cards, say much. just, like, \"meet standards. approaching standards.\" mean? can\\'t see material. mean, unless work sit next day watch it, incredibly boring top everything, top possible. so, theory, program way using involved, things, like practice math online, also way parent viewing result quickly, like getting, know, weekly snapshot like, \"what child do? many problem miss? look like they\\'re walking away maybe they\\'re it?\" mean, different way get contact.',\n",
       " \"keep short, since go back, went back january, person, backslide social emotional competence, saw coming. well, take surprise, returning behavior we'd spent two year working building support system school prevent happening, eroded. anticipate back school they're really still meeting friends. need meet new friends, new school. even recess, still slow. maybe separate people long, lot difficult usually make friendship connections.\",\n",
       " \"i'm surprised son started new school, different burrow, kid talk instagram, boys. know. would share information positive covid test school, whatever. so, felt knew kids. met them. march 22nd, opted cohort one, monday tuesday, every friday. problem also, carry laptop school. wonder, new york city, everyone know every kid street laptop backpack. i'm waiting for... mention phone. positive him, even going onscreen class, he's getting know building. new high school, mean, him, met kids. so, finding way around building thing, meeting kid flesh. so, plus.\",\n",
       " 'sure. sure. yeah. well, parent 4, child 2, course, said, seven-year-old, allowed it, change. right now, think really time facility get involved it, let i\\'m around, school, people monitoring. so, issue her. parent 4, child 1, started that, playing game all, full access computer time, except homework. just, \"here\\'s time. time\\'s over. done.\" really mind. would preferred something computer game, that\\'s wanted do, really issue that. that, now, seems like always available. even, sometimes certain restrictions, could find way around it. instead using safari, restricted computer, found way use chrome, work around that. called apple spent hour phone try figure block too. so, became real struggle. think we\\'ve got control, seem much focus screen. us, think much gaming taken over, restrictions, focus computer. certainly time has, would go that. definitely become social aspect point. like, \"oh, come over?\" want sit play computer game together. so, \"dude, spent day computer, could done come over? finally front going sit front computer together?\" crazy me. so...',\n",
       " 'no. no. focused much making friend people know game. mentioned couple people, always made real... we\\'ve strong fact that, \"i want people know.\" far know, respectful that. maybe let know, think pretty respectful that. it\\'s, her, fun people already knows. already relationship them, make interpersonal, somehow.',\n",
       " 'took minute remember before. well, mean, one reason is, one thing study, i\\'m teaching class video game now, complicated house video gamer. knew that, going, kids, would part lives. mean play whenever whatever want. kid played game pandemic. lot control though, not, point, technologically able turn unlock real game themselves, completely themselves. could ask could give not. think tv access, typical stuff, much video games, play. liked that, again, curating game, comfortable that. now, is, think i\\'m still maintaining better control game youtube access. me, youtube thing everywhere can\\'t block well enough. also, just, eight year old watching, 20 year olds, eight hour day, just, even they\\'re swearing, something. thing he\\'s learning, learning yet. come home, that\\'s wants. want play minecraft, minecraft, minecraft. bad game, control. can\\'t set hour long limit would respect. like said, set limits, active way that\\'s hard i\\'m working full-time, getting... we\\'re house enough, still, guess, make easier, can\\'t think anything else do. that\\'s real problem. take away, take away like whole weekend he\\'ll start remembering, \"oh yeah, think of, come games.\" soon get day playing games, like sleep manage turn on, moping and, \"i can\\'t play. can\\'t think anything. i\\'m bored.\" guess skill they\\'re going learn though. ability respect limits, also recognize, make choices. \"i much gaming time, i\\'m going actively choose want play.\" he\\'ll something like, \"but get thing wanted do.\" like, \"well, done time right now.\" know eight year old adhd really going develop that. definitely give while, home time. give get work done. that\\'s like pandemic stuff.',\n",
       " \"yeah, definitely. mean, seeing anyone also, right? way could communicate friends. mean, other, always going okay little bit, get fights, need space. yeah, i'm trying house quiet call, would thing could bother i'm trying work.\",\n",
       " 'right. yeah. now, yeah, snowballed this... ipad them. would like, \"okay, borrow phone there, play game.\" became like, \"oh god, got keep busy school keeping busy.\" own, they\\'re young really... teacher, working, trying job keep happy. enough time energy work.',\n",
       " 'one positive was... among u again. just, know one game, kid same-age cousin, who, they\\'ve always lived opposite side country. they\\'ve met times, they\\'ve actually formed closer relationship ever before. brother motivated, seeing separate friends, connected phone could talk play game together time. yeah, private room with, with, i\\'m trying keep eye else joining. games, developers\\' side, absolutely could make safer. first all, long definitely made kid mind, youtube made youtube kids, can\\'t block out... know. maybe need pressure. feel like youtube something can\\'t say, \"look, want computer.\" can\\'t block it, block safari, get through-',\n",
       " \"think it'll hard road introduce balance life on, times, whatever hell is. because, pry away screen, that's lifeblood year, going hard get biking, hanging friends, whatever. think going hard. talk game specific friends, going step-by-step two step backwards, several times. mean, really gird for... want burst free thing, work way. usually, got keep incremental little things. that's frustrating.\"]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 0,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 2,\n",
       " 0,\n",
       " 2]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        #print(labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        #print(item)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(X_train_tokenized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([  101,  3398,  1012,  2714,  6687,  1019,  6687,  1017,  1010,  2812,\n",
      "         1010,  4845,  2052, 17781,  3042,  1012, 18546,  8654, 25249,  3047,\n",
      "         1012,  2288, 18546, 17470,  2082,  1012,  2812,  1010,  1045,  1005,\n",
      "         1049,  2551,  2188,  1012,  2093,  4845, 26536,  9354,  1012,  3034,\n",
      "         4455,  1010,  2562,  4251,  1012,  2288, 25249,  2015,  2346, 23467,\n",
      "         3047,  1010,  2008,  1005,  1055, 10639,  2814,  1012,  2253, 19448,\n",
      "         1010,  4121,  2518,  2064,  1005,  1056,  1012,  1012,  1012,  2027,\n",
      "         1005,  2128, 23042,  1012,  2448,  2188,  2082,  1010,  1000,  2073,\n",
      "         1005,  1055, 25249,  1029,  2215,  2377,  1012,  1000,  2009,  1005,\n",
      "         1055,  1010,  1045,  1005,  1049,  2145,  2551,  1012,  2507,  2210,\n",
      "         2978,  1010,  2215,  2131,  2125,  1012,  2812,  1010,  2428,  1012,\n",
      "         1012,  1012,  2052,  2377,  2208,  2030,  1012,  1012,  1012,  2082,\n",
      "         1010, 10124,  2707,  1012,  2059,  1010,  2188, 18546, 17470,  1010,\n",
      "         2354,  1010,  2066,  6687,  1017,  2056,  1010,  2354,  2131,  2105,\n",
      "         2477,  1012,  2131,  2399,  1010,  2518,  2215,  2006,  1012,  3398,\n",
      "         1010,  2066,  2439,  2645,  1012,  2359, 25037,  1010,  2056,  2974,\n",
      "         1010,  2371,  2488,  1010,  3984,  7727,  2033,  1010,  3105,  1012,\n",
      "         2066,  1010,  1000,  2821,  1010,  2158,  1010,  1045,  1005,  2310,\n",
      "         2467,  2359,  2147,  2188,  1012,  1000,  2061,  1010,  2307,  2518,\n",
      "         2188,  1012,  2843,  2488,  2027,  1005,  2128,  2067,  2082,  1012,\n",
      "         4268,  1010,  2812,  1010,  2074,  1010,  2172,  2068,  1012,  2126,\n",
      "         2172,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(0)}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = Dataset(X_val_tokenized, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    \n",
    "    pred, labels = p\n",
    "    #print(pred)\n",
    "    #print(pred.shape)\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    \"\"\"\n",
    "    labels = p.label_ids\n",
    "    pred = p.predictions.argmax(-1)\n",
    "    print(labels)\n",
    "    print(pred)\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred, average = 'weighted')\n",
    "    precision = precision_score(y_true=labels, y_pred=pred, average = 'weighted')\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred, average = 'weighted')\n",
    "    #print(f\"accuracy: {accuracy}, precision: {precision}, recall: {recall}, f1: {f1}\")\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingArguments??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"focus_groups_emotion_output\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=15,\n",
    "    num_train_epochs=10,\n",
    "    seed=0,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=1,\n",
    "    learning_rate = learning_rate\n",
    ")\n",
    "#learning_rate = learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(output_dir=focus_groups_emotion_output, overwrite_output_dir=False, do_train=False, do_eval=True, do_predict=False, evaluation_strategy=IntervalStrategy.STEPS, prediction_loss_only=False, per_device_train_batch_size=64, per_device_eval_batch_size=18, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=5, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=runs\\Dec04_19-18-40_DESKTOP-RR0FD0F, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=1, save_strategy=IntervalStrategy.STEPS, save_steps=500, save_total_limit=None, no_cuda=False, seed=0, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=10, dataloader_num_workers=0, past_index=-1, run_name=focus_groups_emotion_output, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=True, metric_for_best_model=loss, greater_is_better=False, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=0, mp_parameters=)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trainer.__init__??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingArguments??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/40 42:53 < 15:19, 0.01 it/s, Epoch 7/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.272500</td>\n",
       "      <td>1.159274</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.441964</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.473046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.234700</td>\n",
       "      <td>1.092833</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.441964</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.473046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.102700</td>\n",
       "      <td>1.065651</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.345455</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.422222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.994200</td>\n",
       "      <td>1.079866</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.495961</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.529469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.108500</td>\n",
       "      <td>1.073334</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.471429</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.509388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.948200</td>\n",
       "      <td>1.084212</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.471429</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.509388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=30, training_loss=1.1574009736378987, metrics={'train_runtime': 2648.5589, 'train_samples_per_second': 0.015, 'total_flos': 0, 'epoch': 7.5, 'init_mem_cpu_alloc_delta': 147456, 'init_mem_cpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 1603510272, 'train_mem_cpu_peaked_delta': 3570049024})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [23/60 29:06 < 51:17, 0.01 it/s, Epoch 11/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.107800</td>\n",
       "      <td>2.059592</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.105114</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.147431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.187000</td>\n",
       "      <td>2.048275</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.101240</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.153605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.077000</td>\n",
       "      <td>2.036327</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.101240</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.153605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.150000</td>\n",
       "      <td>2.029730</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.101240</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.153605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.066900</td>\n",
       "      <td>2.022511</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.101240</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.153605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.986700</td>\n",
       "      <td>2.016541</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.101240</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.153605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.031800</td>\n",
       "      <td>2.010084</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.103594</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.156300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.078600</td>\n",
       "      <td>2.006244</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.103594</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.156300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.023000</td>\n",
       "      <td>2.002145</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.100887</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.150413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.852300</td>\n",
       "      <td>1.996485</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.108852</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.159091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.018000</td>\n",
       "      <td>1.991111</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.114899</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.165455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.076100</td>\n",
       "      <td>1.986315</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.109375</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.152174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.993300</td>\n",
       "      <td>1.982179</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.120883</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.151623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.777000</td>\n",
       "      <td>1.977121</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.111570</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.140572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.984800</td>\n",
       "      <td>1.972439</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.111570</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.140572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.146800</td>\n",
       "      <td>1.969104</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.118660</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.146006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.973800</td>\n",
       "      <td>1.966264</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.118660</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.146006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.838200</td>\n",
       "      <td>1.962359</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.118660</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.146006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.974400</td>\n",
       "      <td>1.958951</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.114394</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.143647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.273300</td>\n",
       "      <td>1.956573</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.110672</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.141558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.972300</td>\n",
       "      <td>1.954504</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.107438</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.139731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.715000</td>\n",
       "      <td>1.953303</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.118660</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.146006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.55430222e-01  5.23206079e-03  7.16976151e-02  1.95126578e-01\n",
      "   2.06720665e-01 -8.07537064e-02 -4.61732745e-02  8.76906961e-02]\n",
      " [-2.38123000e-01  8.29945784e-03 -7.06706196e-03  2.18617037e-01\n",
      "   2.30133817e-01 -1.01340972e-01 -8.22402388e-02 -1.80257857e-03]\n",
      " [-2.10962832e-01  1.43473549e-02  4.01571281e-02  2.37562120e-01\n",
      "   2.16211408e-01 -1.07282706e-01 -3.60744633e-02  6.73384368e-02]\n",
      " [-2.52977580e-01  2.57163122e-03  1.14885233e-02  2.29033142e-01\n",
      "   2.17481434e-01 -1.01586983e-01 -7.61057362e-02 -2.07156166e-02]\n",
      " [-2.10639536e-01  9.72980633e-05  2.04135142e-02  2.32257470e-01\n",
      "   2.08822310e-01 -1.03895500e-01 -6.85415268e-02  4.99580950e-02]\n",
      " [-2.28587016e-01 -1.58852562e-02 -9.00641084e-04  2.69581854e-01\n",
      "   1.85750589e-01 -8.09553564e-02 -7.40135238e-02  7.46334344e-03]\n",
      " [-2.39813611e-01 -1.24258548e-02  2.57445127e-03  2.42303193e-01\n",
      "   1.94305018e-01 -1.27500877e-01 -6.96335807e-02  1.44356638e-02]\n",
      " [-2.29257271e-01 -4.02236693e-02 -5.09312041e-02  2.22777903e-01\n",
      "   2.38873601e-01 -1.49488181e-01 -2.37029232e-02  8.07975978e-03]\n",
      " [-2.35495225e-01 -4.97560017e-03 -4.10153121e-02  2.46568024e-01\n",
      "   2.21746758e-01 -1.25838101e-01 -2.75911093e-02  1.10947862e-02]\n",
      " [-2.34525159e-01  3.74768954e-03  5.46730421e-02  2.39658237e-01\n",
      "   2.15750784e-01 -9.88490805e-02 -6.85985908e-02  7.16634542e-02]\n",
      " [-2.34189346e-01  1.70584470e-02 -2.77392864e-02  2.57644564e-01\n",
      "   1.94573954e-01 -9.37221572e-02 -3.82679030e-02  2.57703587e-02]\n",
      " [-2.72829324e-01 -2.55777445e-02 -2.16660202e-02  2.52829671e-01\n",
      "   2.51322925e-01 -1.35943353e-01 -5.31505942e-02  2.29785442e-02]\n",
      " [-2.35207662e-01  3.76657955e-03  2.19552331e-02  2.18415156e-01\n",
      "   2.29404181e-01 -1.27666250e-01 -6.60651624e-02  1.15522593e-02]\n",
      " [-2.12557375e-01 -3.85366380e-04 -6.37540221e-03  2.53280342e-01\n",
      "   2.31783554e-01 -1.54855371e-01 -1.87025554e-02  3.19004059e-03]\n",
      " [-2.50391841e-01 -9.62975062e-03  1.11488402e-02  2.63407558e-01\n",
      "   2.42290571e-01 -8.62616077e-02 -9.32862684e-02  4.95338887e-02]\n",
      " [-2.17494041e-01  2.16592960e-02 -4.24294919e-03  2.38512576e-01\n",
      "   1.97405100e-01 -1.22519642e-01 -6.15704060e-02  2.01769397e-02]\n",
      " [-2.39561200e-01 -1.46024544e-02 -4.40255515e-02  1.98261112e-01\n",
      "   2.13937908e-01 -1.36737108e-01 -2.66098902e-02  1.78233162e-02]\n",
      " [-2.35358953e-01 -2.11164281e-02 -6.60362840e-03  2.50516772e-01\n",
      "   2.28234425e-01 -8.87111574e-02 -6.19535260e-02  2.78247148e-03]\n",
      " [-2.16447338e-01 -4.37162817e-05  2.90134139e-02  2.47804850e-01\n",
      "   2.23712444e-01 -1.29920512e-01 -6.63696602e-02 -4.96365130e-04]\n",
      " [-2.68413454e-01 -3.45930345e-02 -4.96871471e-02  2.69404501e-01\n",
      "   2.43767560e-01 -9.89166051e-02 -7.10196570e-02 -7.16684759e-03]\n",
      " [-2.19989553e-01  1.50965527e-03 -3.77997383e-03  2.36365318e-01\n",
      "   2.52301395e-01 -1.17699444e-01 -1.80409215e-02  1.72564983e-02]\n",
      " [-2.22767323e-01 -1.05978921e-04  3.37875932e-02  2.65191257e-01\n",
      "   2.18941540e-01 -9.57978219e-02 -8.78602713e-02 -3.47629189e-03]\n",
      " [-2.05405623e-01  6.39752299e-03 -1.72898173e-02  2.49635532e-01\n",
      "   2.30398357e-01 -1.20757192e-01 -1.92728788e-02  3.46603170e-02]\n",
      " [-2.45524228e-01 -1.01182964e-02 -2.04705000e-02  2.32996568e-01\n",
      "   2.39346981e-01 -1.14052601e-01 -6.19973950e-02  1.38947517e-02]\n",
      " [-2.29708940e-01 -2.52987165e-02 -3.79624926e-02  2.35497087e-01\n",
      "   2.36517817e-01 -1.16624400e-01 -1.38531998e-02  3.09490114e-02]\n",
      " [-2.33842432e-01 -3.65909189e-03  8.17026198e-03  2.50933379e-01\n",
      "   2.25425571e-01 -1.08380079e-01 -1.49339810e-02  2.88145617e-02]\n",
      " [-2.43941903e-01 -5.04830293e-03  1.33729056e-02  2.59810179e-01\n",
      "   2.38737643e-01 -1.25680298e-01 -5.15338033e-02 -2.39872187e-03]\n",
      " [-2.38462433e-01  1.38143860e-02  5.94888851e-02  3.03695798e-01\n",
      "   2.28325248e-01 -8.43066871e-02 -2.79631317e-02  6.10124618e-02]\n",
      " [-2.38670588e-01  1.19727198e-02 -2.09318995e-02  2.22183734e-01\n",
      "   2.07354486e-01 -1.01367876e-01 -6.62979335e-02  1.81868970e-02]\n",
      " [-2.46893898e-01 -3.32833491e-02 -9.35843587e-03  2.25254595e-01\n",
      "   2.40146160e-01 -9.50232297e-02 -6.34661987e-02  1.48088709e-02]\n",
      " [-2.43151665e-01 -1.32943038e-02 -3.26539725e-02  2.48394698e-01\n",
      "   2.41600096e-01 -8.84697139e-02 -5.57993390e-02  1.67166144e-02]\n",
      " [-2.14462385e-01  1.86458454e-02  2.50055492e-02  2.30993584e-01\n",
      "   2.09217668e-01 -1.13178536e-01 -6.83956891e-02  4.29036617e-02]\n",
      " [-2.61013716e-01 -2.06936151e-04 -8.84280354e-03  2.36252800e-01\n",
      "   2.22900853e-01 -1.18028790e-01 -7.65763670e-02  2.95254588e-03]\n",
      " [-2.61954248e-01 -1.06852893e-02 -4.78549302e-03  2.01715529e-01\n",
      "   2.15773106e-01 -1.27857357e-01 -2.62437910e-02  6.17713481e-03]\n",
      " [-2.20336065e-01 -2.17557475e-02  5.07427529e-02  2.20323101e-01\n",
      "   1.80696383e-01 -1.27828628e-01 -5.30724563e-02  4.23773825e-02]\n",
      " [-1.90074041e-01 -1.62011236e-02 -5.31006344e-02  2.15096831e-01\n",
      "   2.36293584e-01 -1.15842193e-01 -1.83849521e-02 -7.52165169e-03]\n",
      " [-2.05659509e-01  1.44029055e-02  1.42854899e-02  2.65213102e-01\n",
      "   1.91143245e-01 -1.15118161e-01 -7.11809099e-02  2.68690735e-02]\n",
      " [-2.31467709e-01  1.36205424e-02  3.63849699e-02  2.46640027e-01\n",
      "   2.11250275e-01 -8.86457562e-02 -6.57607391e-02  8.05827975e-03]\n",
      " [-2.07575887e-01  2.10045632e-02 -2.40608156e-02  2.48103172e-01\n",
      "   2.04559401e-01 -1.22430071e-01 -7.80043304e-02  1.72564685e-02]\n",
      " [-2.45335460e-01  1.56550612e-02  3.44633982e-02  2.40128011e-01\n",
      "   2.12292716e-01 -1.15077913e-01 -4.49978672e-02  2.66281590e-02]\n",
      " [-2.45969594e-01  7.22257420e-03 -7.89886713e-03  2.47266501e-01\n",
      "   2.37948477e-01 -1.36414856e-01 -4.57143076e-02  2.89399177e-02]\n",
      " [-2.52344847e-01 -1.50350668e-03 -1.51528567e-02  2.34917715e-01\n",
      "   2.44593710e-01 -1.36660784e-01 -4.09085676e-02  1.34916306e-02]\n",
      " [-2.17533857e-01 -1.17178652e-02  3.40723768e-02  2.42155880e-01\n",
      "   2.10538521e-01 -1.19594701e-01 -4.03621048e-02  1.77881792e-02]\n",
      " [-2.16715574e-01 -8.40788428e-03  4.38248366e-03  2.33458325e-01\n",
      "   1.85824811e-01 -1.15041986e-01 -3.16145457e-02  5.20523936e-02]]\n",
      "(44, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.25, precision: 0.10511363636363637, recall: 0.25, f1: 0.1474308300395257\n",
      "[[-2.31532976e-01 -1.65545736e-02  9.61091146e-02  1.94729924e-01\n",
      "   1.82518870e-01 -9.84241217e-02 -3.80140543e-03  5.18569276e-02]\n",
      " [-2.14018732e-01 -1.72333065e-02  1.89514533e-02  2.16189221e-01\n",
      "   2.04298645e-01 -1.20588452e-01 -3.70064117e-02 -3.97562161e-02]\n",
      " [-1.85813889e-01 -8.97854194e-03  6.37502968e-02  2.38005847e-01\n",
      "   1.89622015e-01 -1.27683118e-01  8.23188201e-03  3.13110277e-02]\n",
      " [-2.27006495e-01 -2.28872467e-02  3.40416469e-02  2.25916103e-01\n",
      "   1.89020172e-01 -1.24110982e-01 -3.14549990e-02 -5.86730987e-02]\n",
      " [-1.85245559e-01 -2.48228759e-02  4.60463911e-02  2.30051666e-01\n",
      "   1.81226388e-01 -1.22390762e-01 -1.97695419e-02  1.32618621e-02]\n",
      " [-2.01604992e-01 -3.96379307e-02  2.38833539e-02  2.68069237e-01\n",
      "   1.54532224e-01 -1.01031899e-01 -2.81405151e-02 -2.83430666e-02]\n",
      " [-2.15465575e-01 -3.66927385e-02  2.51892731e-02  2.40684330e-01\n",
      "   1.68280110e-01 -1.47367477e-01 -2.75212638e-02 -2.01524645e-02]\n",
      " [-2.05693871e-01 -6.54268712e-02 -2.93314718e-02  2.20839396e-01\n",
      "   2.12068468e-01 -1.69911474e-01  2.36402228e-02 -2.56346017e-02]\n",
      " [-2.10183710e-01 -2.88209654e-02 -1.75053328e-02  2.40919545e-01\n",
      "   1.94552183e-01 -1.48275912e-01  1.59846433e-02 -2.34962180e-02]\n",
      " [-2.09901825e-01 -1.96384452e-02  7.98462480e-02  2.38317162e-01\n",
      "   1.88410431e-01 -1.17242835e-01 -2.32234150e-02  3.42104584e-02]\n",
      " [-2.07328975e-01 -6.73503429e-03  1.87631696e-04  2.51585245e-01\n",
      "   1.64161250e-01 -1.15672067e-01  7.08722696e-03 -1.19691938e-02]\n",
      " [-2.45574132e-01 -4.92054410e-02  4.15703654e-03  2.52069294e-01\n",
      "   2.19920382e-01 -1.57554746e-01 -5.48497960e-03 -1.55819580e-02]\n",
      " [-2.07805455e-01 -2.23173425e-02  4.53864187e-02  2.16103226e-01\n",
      "   2.00650096e-01 -1.47806525e-01 -1.77369565e-02 -2.77807936e-02]\n",
      " [-1.85452223e-01 -2.55487282e-02  1.81322843e-02  2.47854888e-01\n",
      "   2.02661827e-01 -1.76765591e-01  2.80276723e-02 -3.22007909e-02]\n",
      " [-2.25956917e-01 -3.35789025e-02  3.67320701e-02  2.59670794e-01\n",
      "   2.15340316e-01 -1.07033335e-01 -4.80030775e-02  1.13665760e-02]\n",
      " [-1.92877293e-01 -3.11511382e-03  2.23886147e-02  2.38344088e-01\n",
      "   1.68776914e-01 -1.44152462e-01 -1.52865686e-02 -1.81643516e-02]\n",
      " [-2.15428770e-01 -3.91224846e-02 -2.36212388e-02  1.92084640e-01\n",
      "   1.88659653e-01 -1.58482939e-01  1.48942545e-02 -1.41521022e-02]\n",
      " [-2.08578646e-01 -4.63725068e-02  1.83365531e-02  2.46728554e-01\n",
      "   1.96926057e-01 -1.09663367e-01 -1.65744945e-02 -3.41756791e-02]\n",
      " [-1.88989490e-01 -2.49865204e-02  5.46197630e-02  2.45275766e-01\n",
      "   1.92462251e-01 -1.51726693e-01 -1.93712059e-02 -3.95659208e-02]\n",
      " [-2.41533637e-01 -6.05340227e-02 -2.47299299e-02  2.64716327e-01\n",
      "   2.14130372e-01 -1.23767123e-01 -2.42352635e-02 -4.47116643e-02]\n",
      " [-1.93382710e-01 -2.23478973e-02  1.95302665e-02  2.32266530e-01\n",
      "   2.23830611e-01 -1.38797492e-01  2.52591893e-02 -1.74829066e-02]\n",
      " [-1.94800228e-01 -2.51733251e-02  6.16598316e-02  2.62544930e-01\n",
      "   1.88756943e-01 -1.18848234e-01 -3.84935662e-02 -4.56926972e-02]\n",
      " [-1.79270297e-01 -1.74695179e-02  7.18677789e-03  2.47795075e-01\n",
      "   2.02642336e-01 -1.40871555e-01  2.62631550e-02  8.35999846e-04]\n",
      " [-2.20599800e-01 -3.29340957e-02  1.31994858e-03  2.27591097e-01\n",
      "   2.14899600e-01 -1.34811491e-01 -1.79200023e-02 -1.98891535e-02]\n",
      " [-2.05508471e-01 -4.70243171e-02 -1.72811486e-02  2.31057346e-01\n",
      "   2.12725446e-01 -1.39736146e-01  2.86856964e-02 -4.77956235e-03]\n",
      " [-2.09049016e-01 -2.59238482e-02  3.24479155e-02  2.47056618e-01\n",
      "   1.98781684e-01 -1.29901335e-01  2.92053446e-02 -7.82478601e-03]\n",
      " [-2.15385675e-01 -3.08170468e-02  3.86471823e-02  2.57524312e-01\n",
      "   2.08885252e-01 -1.47238091e-01 -3.06015462e-03 -4.14208472e-02]\n",
      " [-2.13569731e-01 -1.09022558e-02  8.44344497e-02  3.00954103e-01\n",
      "   2.00778753e-01 -1.05570599e-01  1.42949522e-02  2.55205035e-02]\n",
      " [-2.15712905e-01 -1.07669728e-02  2.44080275e-03  2.17623204e-01\n",
      "   1.80839688e-01 -1.22088701e-01 -2.23036446e-02 -1.60491765e-02]\n",
      " [-2.21022367e-01 -5.82363233e-02  1.52901560e-02  2.20388994e-01\n",
      "   2.11356401e-01 -1.16723388e-01 -1.71484873e-02 -2.46914327e-02]\n",
      " [-2.17150047e-01 -4.04528342e-02 -6.22060150e-03  2.43694618e-01\n",
      "   2.12523401e-01 -1.12735279e-01 -1.06883347e-02 -2.15766728e-02]\n",
      " [-1.91848576e-01 -4.56516072e-03  4.84882295e-02  2.28206381e-01\n",
      "   1.83787584e-01 -1.33536234e-01 -2.56734081e-02  9.50969756e-03]\n",
      " [-2.36436307e-01 -2.38087997e-02  1.56942159e-02  2.29903042e-01\n",
      "   1.96612000e-01 -1.38671383e-01 -2.92764679e-02 -3.51716578e-02]\n",
      " [-2.36096486e-01 -3.54404449e-02  1.95386559e-02  1.96138099e-01\n",
      "   1.89891800e-01 -1.51454210e-01  1.79947615e-02 -3.09064537e-02]\n",
      " [-1.95059612e-01 -4.33398560e-02  7.44618177e-02  2.18548238e-01\n",
      "   1.56140640e-01 -1.45151451e-01 -9.77282971e-03  7.39518553e-03]\n",
      " [-1.65109411e-01 -3.98162827e-02 -3.06402072e-02  2.10859612e-01\n",
      "   2.10324541e-01 -1.38324499e-01  1.96394846e-02 -3.91469374e-02]\n",
      " [-1.77863747e-01 -9.11520608e-03  4.28727120e-02  2.63600528e-01\n",
      "   1.61989301e-01 -1.37559891e-01 -2.32270956e-02 -1.25966519e-02]\n",
      " [-2.06496477e-01 -7.69972708e-03  5.95616326e-02  2.41740376e-01\n",
      "   1.85516298e-01 -1.10965982e-01 -2.17269361e-02 -3.08909416e-02]\n",
      " [-1.84124023e-01 -9.38029960e-04  1.58310309e-03  2.43216574e-01\n",
      "   1.76262647e-01 -1.43075645e-01 -3.05756778e-02 -1.88948289e-02]\n",
      " [-2.18472332e-01 -1.08364914e-02  5.72772101e-02  2.37700745e-01\n",
      "   1.85714632e-01 -1.36723340e-01  6.75275922e-04 -1.14732236e-02]\n",
      " [-2.20580727e-01 -1.64007545e-02  1.51242539e-02  2.42134601e-01\n",
      "   2.11146593e-01 -1.58978254e-01 -1.27011538e-03 -5.52535057e-03]\n",
      " [-2.23176956e-01 -2.47923695e-02  9.05331224e-03  2.30263770e-01\n",
      "   2.16068208e-01 -1.56542391e-01  6.70674443e-03 -2.41562873e-02]\n",
      " [-1.92567289e-01 -3.54637578e-02  5.80581911e-02  2.39209399e-01\n",
      "   1.81723699e-01 -1.38939828e-01  6.89518079e-03 -1.87557638e-02]\n",
      " [-1.88426852e-01 -3.16835083e-02  2.85193324e-02  2.31297508e-01\n",
      "   1.58549502e-01 -1.36099726e-01  1.81155801e-02  1.90465674e-02]]\n",
      "(44, 8)\n",
      "accuracy: 0.3181818181818182, precision: 0.1012396694214876, recall: 0.3181818181818182, f1: 0.1536050156739812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.20046136 -0.04029752  0.1313908   0.20267813  0.15619501 -0.11811877\n",
      "   0.03000014  0.01493212]\n",
      " [-0.18251672 -0.04361192  0.05582533  0.22258204  0.17696346 -0.14305362\n",
      "  -0.00198835 -0.07937073]\n",
      " [-0.15225056 -0.03510235  0.0964971   0.24798292  0.16152078 -0.1500975\n",
      "   0.04282644 -0.00524816]\n",
      " [-0.19121112 -0.0492795   0.06731699  0.2300027   0.15946193 -0.1483896\n",
      "   0.0027568  -0.09751599]\n",
      " [-0.15131326 -0.05156069  0.08307822  0.23797709  0.15220688 -0.14373408\n",
      "   0.01882353 -0.02257092]\n",
      " [-0.16666752 -0.06532631  0.05962511  0.27628398  0.12396482 -0.12361541\n",
      "   0.00813302 -0.06591161]\n",
      " [-0.18524757 -0.06446107  0.05910485  0.24696317  0.14056435 -0.16925415\n",
      "   0.00478362 -0.05455588]\n",
      " [-0.17208159 -0.09149607  0.00180708  0.22598074  0.18268257 -0.19129908\n",
      "   0.06063133 -0.05730386]\n",
      " [-0.17635307 -0.05582854  0.01610931  0.24423918  0.1639766  -0.17201558\n",
      "   0.04920106 -0.05826944]\n",
      " [-0.17670847 -0.0439418   0.11599606  0.2447948   0.1601345  -0.13862614\n",
      "   0.01198915 -0.00428418]\n",
      " [-0.17280963 -0.03218406  0.04103258  0.25294444  0.1332725  -0.13950847\n",
      "   0.0426209  -0.05042063]\n",
      " [-0.20963392 -0.07704479  0.04221321  0.25838637  0.18618739 -0.18231365\n",
      "   0.03060719 -0.05475932]\n",
      " [-0.17179784 -0.04946818  0.07940768  0.2214818   0.17109233 -0.17069808\n",
      "   0.01928876 -0.06685397]\n",
      " [-0.15007037 -0.05296712  0.05287336  0.25098404  0.17113502 -0.19946508\n",
      "   0.06389278 -0.06783237]\n",
      " [-0.19328156 -0.05944669  0.07566346  0.26407638  0.18671046 -0.13117106\n",
      "  -0.01310453 -0.02793984]\n",
      " [-0.15825132 -0.03076883  0.06040426  0.24524051  0.13810578 -0.16804454\n",
      "   0.01787926 -0.05707115]\n",
      " [-0.18418959 -0.06423354  0.00572965  0.19374378  0.16094472 -0.181774\n",
      "   0.04650374 -0.0463348 ]\n",
      " [-0.1745491  -0.07384443  0.05569019  0.25028223  0.16431127 -0.13223174\n",
      "   0.01638504 -0.07101582]\n",
      " [-0.15309916 -0.05063372  0.09089763  0.24990404  0.15992801 -0.1761004\n",
      "   0.0159811  -0.0781076 ]\n",
      " [-0.20691825 -0.08793224  0.01231167  0.2667408   0.18157628 -0.15075946\n",
      "   0.0106814  -0.08304178]\n",
      " [-0.1593656  -0.0480767   0.05356922  0.23693386  0.19320348 -0.16231501\n",
      "   0.05964582 -0.05352374]\n",
      " [-0.15913324 -0.05260964  0.10113787  0.26798984  0.15770847 -0.14346442\n",
      "  -0.0013897  -0.0886807 ]\n",
      " [-0.14484057 -0.04428331  0.04234159  0.25270626  0.1738734  -0.16384974\n",
      "   0.06043227 -0.03374735]\n",
      " [-0.18759464 -0.0571344   0.03355974  0.23012817  0.18838371 -0.15707053\n",
      "   0.01623093 -0.0550947 ]\n",
      " [-0.17264374 -0.07183884  0.01302041  0.23385108  0.18804541 -0.1643436\n",
      "   0.06027582 -0.04076107]\n",
      " [-0.1763593  -0.05096478  0.06811753  0.2512263   0.17110491 -0.15349492\n",
      "   0.06268992 -0.04439324]\n",
      " [-0.17738464 -0.0589047   0.07547098  0.26224676  0.17867659 -0.17251986\n",
      "   0.03348696 -0.07948704]\n",
      " [-0.18207788 -0.0382284   0.12107062  0.3068012   0.1721687  -0.1279184\n",
      "   0.04686311 -0.01053979]\n",
      " [-0.18476793 -0.03482196  0.03677847  0.22233324  0.15389678 -0.1440784\n",
      "   0.01149344 -0.05034516]\n",
      " [-0.1874949  -0.08460136  0.05314385  0.2249069   0.17995858 -0.14091587\n",
      "   0.01779071 -0.06477295]\n",
      " [-0.18324223 -0.06912987  0.03306245  0.24634624  0.18224113 -0.13931076\n",
      "   0.02375122 -0.05861538]\n",
      " [-0.16145992 -0.02817352  0.08111727  0.2334333   0.15702435 -0.15562788\n",
      "   0.00730507 -0.02431231]\n",
      " [-0.20273453 -0.04826608  0.05126773  0.2316471   0.1668298  -0.16235664\n",
      "   0.00859163 -0.07365119]\n",
      " [-0.20330857 -0.06330774  0.05416902  0.20107196  0.16262028 -0.1771923\n",
      "   0.05069044 -0.06892145]\n",
      " [-0.16209093 -0.06715576  0.107991    0.22494908  0.13086954 -0.1652608\n",
      "   0.02471223 -0.02910384]\n",
      " [-0.1319405  -0.06480794  0.00052983  0.21311817  0.18237928 -0.16184396\n",
      "   0.04918335 -0.07112133]\n",
      " [-0.14133689 -0.03358078  0.08212154  0.26986662  0.13286687 -0.16264406\n",
      "   0.0119265  -0.05219331]\n",
      " [-0.17328322 -0.0314627   0.09459972  0.24482274  0.15895438 -0.1347358\n",
      "   0.01309051 -0.06991553]\n",
      " [-0.15281908 -0.02562024  0.037526    0.24646036  0.14711922 -0.16552292\n",
      "   0.00663168 -0.05514512]\n",
      " [-0.18178764 -0.03930217  0.09115567  0.24375066  0.1563849  -0.16040097\n",
      "   0.03583657 -0.05031519]\n",
      " [-0.1860379  -0.04090176  0.0480028   0.24462597  0.18233268 -0.18254799\n",
      "   0.03255429 -0.0398833 ]\n",
      " [-0.18614902 -0.05123065  0.04434356  0.234546    0.1865845  -0.17958224\n",
      "   0.04476307 -0.06343001]\n",
      " [-0.16026741 -0.06124111  0.0938961   0.24413651  0.15261489 -0.16102079\n",
      "   0.04392971 -0.05650191]\n",
      " [-0.15215558 -0.05821769  0.06387442  0.23710762  0.13175768 -0.16055544\n",
      "   0.05675672 -0.01465592]]\n",
      "(44, 8)\n",
      "accuracy: 0.3181818181818182, precision: 0.1012396694214876, recall: 0.3181818181818182, f1: 0.1536050156739812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.17651245 -0.02979245  0.15363133  0.2041961   0.13191743 -0.13609225\n",
      "   0.05031872 -0.01849182]\n",
      " [-0.15842372 -0.03278044  0.08109947  0.22422636  0.15048644 -0.16248298\n",
      "   0.01988445 -0.11426286]\n",
      " [-0.12863062 -0.023828    0.1187459   0.25089753  0.13441366 -0.16936818\n",
      "   0.06389654 -0.03885755]\n",
      " [-0.16459751 -0.03741337  0.0902143   0.22772923  0.13113579 -0.16915476\n",
      "   0.0226874  -0.13238907]\n",
      " [-0.12611598 -0.03931374  0.10699861  0.2392734   0.12449516 -0.16272879\n",
      "   0.04301801 -0.05399216]\n",
      " [-0.1403263  -0.05342101  0.08274913  0.27924025  0.09652937 -0.1430092\n",
      "   0.03075302 -0.09909432]\n",
      " [-0.16203809 -0.05413011  0.08204479  0.24605002  0.11454409 -0.18778531\n",
      "   0.02393368 -0.08438265]\n",
      " [-0.14721917 -0.08053236  0.0221744   0.2251707   0.1551857  -0.20992506\n",
      "   0.08350503 -0.08646558]\n",
      " [-0.15131894 -0.04422419  0.03833017  0.24222468  0.13481127 -0.19188796\n",
      "   0.0690625  -0.0896951 ]\n",
      " [-0.15238653 -0.03254415  0.13976136  0.24520917  0.1338825  -0.15642738\n",
      "   0.03381352 -0.0387602 ]\n",
      " [-0.14687932 -0.01831478  0.06838005  0.24843384  0.10556865 -0.15965323\n",
      "   0.06487302 -0.08415919]\n",
      " [-0.18189609 -0.06378342  0.06763983  0.25673172  0.15544194 -0.20195468\n",
      "   0.05124083 -0.08908752]\n",
      " [-0.14420922 -0.03761322  0.10140655  0.21993497  0.14288379 -0.18997356\n",
      "   0.04087149 -0.10217384]\n",
      " [-0.12289017 -0.04337094  0.07698851  0.24954607  0.14148632 -0.2186368\n",
      "   0.08550049 -0.10077103]\n",
      " [-0.16817598 -0.04867589  0.10258909  0.262835    0.15968117 -0.15092407\n",
      "   0.00846478 -0.06198695]\n",
      " [-0.13245198 -0.0190623   0.0857191   0.24561852  0.10904558 -0.18712944\n",
      "   0.0370838  -0.09193977]\n",
      " [-0.16175748 -0.0529549   0.0246608   0.19020477  0.13442566 -0.20076932\n",
      "   0.06562349 -0.07614492]\n",
      " [-0.14996281 -0.06248791  0.08159181  0.24824141  0.1349764  -0.15046471\n",
      "   0.03576174 -0.10316592]\n",
      " [-0.12605743 -0.03879708  0.1150106   0.24841906  0.12978733 -0.19602804\n",
      "   0.037736   -0.11259399]\n",
      " [-0.18087535 -0.07507057  0.03771602  0.26293647  0.15119241 -0.17330125\n",
      "   0.03119683 -0.11750481]\n",
      " [-0.1346061  -0.03708794  0.07665943  0.23616919  0.16401339 -0.18214327\n",
      "   0.0805358  -0.08606333]\n",
      " [-0.13280953 -0.03820342  0.12772751  0.26716402  0.12790917 -0.16332369\n",
      "   0.02029828 -0.1269645 ]\n",
      " [-0.11982811 -0.03392394  0.0659608   0.25174904  0.14662828 -0.18338214\n",
      "   0.08099856 -0.06553753]\n",
      " [-0.16396533 -0.0447936   0.05530376  0.22721496  0.16272286 -0.17635666\n",
      "   0.03629214 -0.08696637]\n",
      " [-0.14827406 -0.06042789  0.03315986  0.23134837  0.16369131 -0.18524258\n",
      "   0.07814537 -0.07221711]\n",
      " [-0.15194067 -0.03735278  0.09157578  0.24805436  0.14374965 -0.17239875\n",
      "   0.08194632 -0.07718878]\n",
      " [-0.1492959  -0.04685432  0.10000466  0.25917578  0.15006036 -0.19391617\n",
      "   0.05324338 -0.1145917 ]\n",
      " [-0.15839425 -0.02874438  0.1464387   0.30601582  0.14592871 -0.14678425\n",
      "   0.06738803 -0.04303464]\n",
      " [-0.160845   -0.02219264  0.05986975  0.2206357   0.12784855 -0.1639533\n",
      "   0.03120504 -0.08201554]\n",
      " [-0.16230723 -0.07320407  0.07852468  0.22349305  0.15161304 -0.16139847\n",
      "   0.03835965 -0.09982831]\n",
      " [-0.1580447  -0.05714484  0.05955506  0.24336687  0.15367049 -0.16111186\n",
      "   0.04507333 -0.09228633]\n",
      " [-0.13980481 -0.01692382  0.10268806  0.2332539   0.13167119 -0.17399666\n",
      "   0.02699444 -0.05508714]\n",
      " [-0.1768358  -0.03447355  0.0748296   0.22745402  0.1388357  -0.18328868\n",
      "   0.0316093  -0.10833233]\n",
      " [-0.1782744  -0.05152987  0.07667169  0.19962533  0.13558376 -0.19677383\n",
      "   0.06917344 -0.10257253]\n",
      " [-0.13776673 -0.05535851  0.13083294  0.22584778  0.10678609 -0.18366502\n",
      "   0.0466639  -0.06189193]\n",
      " [-0.10752363 -0.05371435  0.02138765  0.21045697  0.1566123  -0.18209887\n",
      "   0.06625447 -0.10062316]\n",
      " [-0.11490713 -0.02004314  0.10774422  0.27014634  0.10615495 -0.1841329\n",
      "   0.032166   -0.08696268]\n",
      " [-0.1485289  -0.0191431   0.11767906  0.2422169   0.13422807 -0.15509135\n",
      "   0.0336044  -0.10450484]\n",
      " [-0.12908159 -0.01263613  0.06207221  0.24424176  0.11942505 -0.18494941\n",
      "   0.02919308 -0.08827543]\n",
      " [-0.15371892 -0.02871378  0.11374158  0.24239165  0.12900694 -0.18110232\n",
      "   0.05782933 -0.08480674]\n",
      " [-0.16027    -0.02845351  0.07047743  0.24227864  0.15447022 -0.20197688\n",
      "   0.05253532 -0.07198815]\n",
      " [-0.15937272 -0.03889951  0.06923752  0.23281118  0.15895902 -0.1996329\n",
      "   0.06759341 -0.09915471]\n",
      " [-0.13518442 -0.04900113  0.11861944  0.2417486   0.12586248 -0.179508\n",
      "   0.06614882 -0.08943833]\n",
      " [-0.12642422 -0.04370849  0.08733948  0.23587564  0.10601219 -0.18171367\n",
      "   0.07883914 -0.0479227 ]]\n",
      "(44, 8)\n",
      "accuracy: 0.3181818181818182, precision: 0.1012396694214876, recall: 0.3181818181818182, f1: 0.1536050156739812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.14636126 -0.02451794  0.1820192   0.20935652  0.10503259 -0.1555377\n",
      "   0.06474657 -0.05353335]\n",
      " [-0.1276086  -0.02652723  0.11278939  0.2293274   0.12260367 -0.18351157\n",
      "   0.03580535 -0.15114643]\n",
      " [-0.09761156 -0.01750119  0.14661853  0.25791785  0.10610223 -0.18974587\n",
      "   0.07891134 -0.07374579]\n",
      " [-0.13097078 -0.03035257  0.11881123  0.22909313  0.10186119 -0.18961173\n",
      "   0.03579194 -0.16893305]\n",
      " [-0.0951632  -0.03245007  0.1363912   0.2444869   0.09542984 -0.18239933\n",
      "   0.05981086 -0.08652678]\n",
      " [-0.10785613 -0.04708196  0.11089748  0.286432    0.06787013 -0.1627046\n",
      "   0.04754421 -0.13269818]\n",
      " [-0.13275844 -0.04807575  0.11001791  0.24863647  0.08789141 -0.20705202\n",
      "   0.03732369 -0.11703897]\n",
      " [-0.11689956 -0.07536575  0.04794879  0.22987105  0.1253646  -0.22986776\n",
      "   0.10058065 -0.11827499]\n",
      " [-0.12057878 -0.03775891  0.06639767  0.24460804  0.10415402 -0.21192667\n",
      "   0.08212736 -0.12284081]\n",
      " [-0.12095085 -0.02597717  0.16962269  0.24919137  0.10580911 -0.17620283\n",
      "   0.04897485 -0.07623491]\n",
      " [-0.11420947 -0.00955049  0.10096005  0.24832733  0.07631238 -0.18067339\n",
      "   0.07951243 -0.11968516]\n",
      " [-0.14865607 -0.05497972  0.09914155  0.25848246  0.12250233 -0.22203577\n",
      "   0.064449   -0.12443373]\n",
      " [-0.1108963  -0.03133482  0.12985125  0.22319292  0.11265215 -0.20971707\n",
      "   0.05584454 -0.13930567]\n",
      " [-0.08898073 -0.03884346  0.10769586  0.25221595  0.109901   -0.2380808\n",
      "   0.10116278 -0.1342057 ]\n",
      " [-0.1359058  -0.04304583  0.13504465  0.26565337  0.13045588 -0.17030093\n",
      "   0.02377936 -0.09806998]\n",
      " [-0.09958746 -0.01343876  0.11603101  0.24829122  0.07888766 -0.20769426\n",
      "   0.04874924 -0.1287922 ]\n",
      " [-0.13359709 -0.04676462  0.0489516   0.19109415  0.10663645 -0.21997786\n",
      "   0.07951906 -0.10784616]\n",
      " [-0.11925134 -0.05764155  0.11373881  0.2502575   0.1043046  -0.16901776\n",
      "   0.04825133 -0.13684826]\n",
      " [-0.09189335 -0.03261845  0.1447121   0.25265664  0.09706201 -0.21618843\n",
      "   0.05274355 -0.1477471 ]\n",
      " [-0.14898247 -0.06735833  0.06986462  0.26328564  0.11860944 -0.19478765\n",
      "   0.04491334 -0.15297051]\n",
      " [-0.10348096 -0.03172513  0.10491284  0.2407072   0.13334164 -0.20233709\n",
      "   0.09576868 -0.12094877]\n",
      " [-0.09871066 -0.0298496   0.16081454  0.27006495  0.09702018 -0.18452251\n",
      "   0.03403019 -0.16744556]\n",
      " [-0.08905566 -0.02862018  0.09412593  0.2543608   0.11681677 -0.20315361\n",
      "   0.09641092 -0.09909414]\n",
      " [-0.13381688 -0.03706932  0.08262616  0.228482    0.13576463 -0.19570456\n",
      "   0.05096828 -0.12064784]\n",
      " [-0.11857288 -0.0544478   0.05889605  0.23331954  0.13803834 -0.20627278\n",
      "   0.08996455 -0.10540627]\n",
      " [-0.12181278 -0.02968939  0.12162594  0.24909163  0.11512668 -0.19184929\n",
      "   0.09496816 -0.11096559]\n",
      " [-0.11330944 -0.03993662  0.13048184  0.25955886  0.11915846 -0.2159567\n",
      "   0.06639553 -0.15202858]\n",
      " [-0.12753618 -0.02492134  0.17716634  0.3086838   0.11807902 -0.16621715\n",
      "   0.08046081 -0.07685927]\n",
      " [-0.13101132 -0.01496501  0.08851179  0.22336288  0.10060421 -0.18405473\n",
      "   0.04532707 -0.11567845]\n",
      " [-0.13007055 -0.06718516  0.1103248   0.22757521  0.12060061 -0.18211119\n",
      "   0.05269652 -0.13755737]\n",
      " [-0.12675191 -0.05031382  0.09269783  0.24517004  0.12286304 -0.18305148\n",
      "   0.06048618 -0.12761009]\n",
      " [-0.112175   -0.01058263  0.13069707  0.23695143  0.10440054 -0.1921798\n",
      "   0.04052279 -0.08788937]\n",
      " [-0.14477248 -0.02573523  0.10393874  0.22766168  0.10920941 -0.20467335\n",
      "   0.04756662 -0.14570315]\n",
      " [-0.14588083 -0.04466473  0.10370362  0.20169376  0.10724682 -0.21856146\n",
      "   0.08090083 -0.13809015]\n",
      " [-0.10667405 -0.04792354  0.15713985  0.23069188  0.08119675 -0.20461775\n",
      "   0.06288874 -0.09653923]\n",
      " [-0.07728463 -0.04765641  0.04708761  0.21236823  0.12861432 -0.20203301\n",
      "   0.07841613 -0.13201536]\n",
      " [-0.08093553 -0.0116253   0.13739555  0.27309868  0.07889555 -0.20750012\n",
      "   0.0451561  -0.12347306]\n",
      " [-0.11992023 -0.01168346  0.14701054  0.24394855  0.10811125 -0.17510164\n",
      "   0.04835157 -0.14151117]\n",
      " [-0.09939229 -0.00568124  0.09072813  0.24591589  0.09111436 -0.2046669\n",
      "   0.04542023 -0.12330265]\n",
      " [-0.11825149 -0.02277977  0.14197834  0.24470516  0.10024396 -0.20192647\n",
      "   0.07334246 -0.12173267]\n",
      " [-0.12857026 -0.0217552   0.09839742  0.24416251  0.12513208 -0.22157538\n",
      "   0.06626008 -0.10652127]\n",
      " [-0.12657169 -0.03221053  0.09916225  0.23527275  0.12961164 -0.21935645\n",
      "   0.08362803 -0.13622624]\n",
      " [-0.1041608  -0.04209013  0.14980449  0.24445286  0.09696293 -0.19825527\n",
      "   0.08053549 -0.12446555]\n",
      " [-0.09446014 -0.03431539  0.11709654  0.23798957  0.08007258 -0.20340648\n",
      "   0.0944714  -0.08385038]]\n",
      "(44, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.3181818181818182, precision: 0.1012396694214876, recall: 0.3181818181818182, f1: 0.1536050156739812\n",
      "[[-0.12369311 -0.02449699  0.20097251  0.2087731   0.08273332 -0.17317736\n",
      "   0.09098746 -0.08643039]\n",
      " [-0.10445555 -0.02665102  0.13494495  0.22888282  0.09848558 -0.20164326\n",
      "   0.06399238 -0.18400332]\n",
      " [-0.07387732 -0.01686471  0.16598386  0.25961328  0.08174402 -0.20856819\n",
      "   0.10666734 -0.10615346]\n",
      " [-0.10579285 -0.02947158  0.13840178  0.22560528  0.07668648 -0.20698619\n",
      "   0.0616389  -0.20216222]\n",
      " [-0.07169984 -0.03105826  0.15609208  0.24439196  0.07063077 -0.20026971\n",
      "   0.08788753 -0.11698622]\n",
      " [-0.08370902 -0.04632241  0.13029434  0.287736    0.04290816 -0.1800862\n",
      "   0.07555447 -0.16332746]\n",
      " [-0.11112543 -0.04754581  0.1293132   0.24672224  0.0653792  -0.22458503\n",
      "   0.06265545 -0.14676717]\n",
      " [-0.09428233 -0.07558517  0.0649724   0.2287769   0.10101777 -0.24791332\n",
      "   0.12857358 -0.14785182]\n",
      " [-0.09800752 -0.03686709  0.08548685  0.24119847  0.07864328 -0.22956896\n",
      "   0.10733856 -0.15345336]\n",
      " [-0.09683286 -0.02479461  0.1900037   0.24766247  0.08226302 -0.19447522\n",
      "   0.0751912  -0.11060937]\n",
      " [-0.09023686 -0.00717959  0.12397449  0.2435041   0.05115387 -0.19886927\n",
      "   0.10718761 -0.15185535]\n",
      " [-0.12411636 -0.05245541  0.12114875  0.25472647  0.09442429 -0.23905446\n",
      "   0.0898802  -0.1572963 ]\n",
      " [-0.08588451 -0.03105424  0.14933369  0.22060286  0.08680631 -0.22761223\n",
      "   0.08394071 -0.17272522]\n",
      " [-0.06350514 -0.03960184  0.12880443  0.24948233  0.08329304 -0.25559816\n",
      "   0.12848216 -0.16467498]\n",
      " [-0.11234067 -0.04323552  0.15760534  0.26266277  0.10551292 -0.18770856\n",
      "   0.05008308 -0.13087651]\n",
      " [-0.07487434 -0.01358121  0.13663365  0.24555323  0.05344376 -0.2258692\n",
      "   0.07286615 -0.16207832]\n",
      " [-0.11229166 -0.0458661   0.06532975  0.18730852  0.08205526 -0.23707572\n",
      "   0.10514346 -0.13632813]\n",
      " [-0.09624074 -0.05814387  0.13642864  0.24776733  0.07794887 -0.18548894\n",
      "   0.07341488 -0.16717501]\n",
      " [-0.06573218 -0.03222366  0.16565073  0.2512024   0.06929681 -0.23381293\n",
      "   0.07964157 -0.17947634]\n",
      " [-0.12488459 -0.06605119  0.09172875  0.25788087  0.09177738 -0.2141886\n",
      "   0.07217599 -0.18576345]\n",
      " [-0.08084776 -0.03260135  0.12358619  0.23866203  0.10751895 -0.220664\n",
      "   0.12399831 -0.15309739]\n",
      " [-0.072979   -0.02862181  0.18364176  0.26811153  0.07049786 -0.20317405\n",
      "   0.06174152 -0.20400125]\n",
      " [-0.06639597 -0.02832714  0.11342027  0.25186625  0.09197111 -0.22071983\n",
      "   0.12316824 -0.12995225]\n",
      " [-0.11144748 -0.03486628  0.10127272  0.22494213  0.11277379 -0.21314065\n",
      "   0.07805792 -0.15147558]\n",
      " [-0.09618598 -0.05349045  0.07637385  0.22965178  0.11641027 -0.22481838\n",
      "   0.11363508 -0.13541345]\n",
      " [-0.09971292 -0.02782781  0.1426625   0.24474463  0.09093496 -0.20955408\n",
      "   0.1210385  -0.14264584]\n",
      " [-0.08583614 -0.03932898  0.15163784  0.2547902   0.09332374 -0.2355051\n",
      "   0.09321992 -0.18651159]\n",
      " [-0.10450041 -0.02612549  0.1985694   0.30557534  0.09547681 -0.18422174\n",
      "   0.10551775 -0.10883711]\n",
      " [-0.10961904 -0.01392803  0.10818137  0.22073564  0.07753356 -0.20188287\n",
      "   0.07180655 -0.14680353]\n",
      " [-0.10582653 -0.06707454  0.13130865  0.22560439  0.09547687 -0.20074955\n",
      "   0.08027741 -0.17251264]\n",
      " [-0.10315315 -0.04885659  0.11577726  0.24196921  0.09689204 -0.20287487\n",
      "   0.08796497 -0.15955685]\n",
      " [-0.09204587 -0.00966481  0.15007013  0.23500037  0.08132121 -0.20888129\n",
      "   0.066295   -0.1180777 ]\n",
      " [-0.12080324 -0.0234231   0.12290068  0.22282887  0.08467885 -0.22395256\n",
      "   0.07684995 -0.17960636]\n",
      " [-0.12185317 -0.04419272  0.12132286  0.1983907   0.08405641 -0.23715098\n",
      "   0.10625732 -0.17083895]\n",
      " [-0.08306611 -0.04582249  0.17438468  0.23003128  0.05939648 -0.223047\n",
      "   0.08951882 -0.12795076]\n",
      " [-0.05479667 -0.0467611   0.06481266  0.20969668  0.10518838 -0.21950674\n",
      "   0.1018633  -0.16102815]\n",
      " [-0.05481496 -0.00890279  0.15765679  0.2712018   0.05616371 -0.22859855\n",
      "   0.07081664 -0.15685809]\n",
      " [-0.09839736 -0.01011538  0.16683047  0.24039862  0.08523642 -0.19310871\n",
      "   0.07625451 -0.17544818]\n",
      " [-0.07794789 -0.00472767  0.11001062  0.24255767  0.06729281 -0.22164498\n",
      "   0.07244063 -0.15467016]\n",
      " [-0.09141441 -0.0232449   0.1611571   0.24112223  0.07609497 -0.2202427\n",
      "   0.10179938 -0.15587895]\n",
      " [-0.10480945 -0.02073698  0.11727449  0.24031058  0.09981421 -0.23950884\n",
      "   0.09232879 -0.13780068]\n",
      " [-0.1017804  -0.03122807  0.11916543  0.23217763  0.10470928 -0.23709348\n",
      "   0.112594   -0.16989039]\n",
      " [-0.08140387 -0.04111753  0.17080773  0.24152842  0.07284965 -0.21552227\n",
      "   0.10694242 -0.1568285 ]\n",
      " [-0.07134736 -0.03088238  0.13729414  0.2340537   0.05857636 -0.22229499\n",
      "   0.12316225 -0.11711992]]\n",
      "(44, 8)\n",
      "accuracy: 0.3181818181818182, precision: 0.1012396694214876, recall: 0.3181818181818182, f1: 0.1536050156739812"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[-0.0976889  -0.02828862  0.22488043  0.21234567  0.05818113 -0.19220242\n",
      "   0.11330293 -0.12060716]\n",
      " [-0.07776388 -0.03062592  0.1619253   0.23338287  0.07156535 -0.22083217\n",
      "   0.08789791 -0.21790998]\n",
      " [-0.04532571 -0.01951305  0.19059075  0.26570478  0.05510781 -0.22864828\n",
      "   0.13021503 -0.13981916]\n",
      " [-0.07636467 -0.03311164  0.16265419  0.22670722  0.04955607 -0.22520018\n",
      "   0.082767   -0.23650484]\n",
      " [-0.04397523 -0.03392124  0.18070601  0.24773929  0.04405929 -0.21896781\n",
      "   0.11004881 -0.14822291]\n",
      " [-0.05655201 -0.04903451  0.15411186  0.2928506   0.01602512 -0.198323\n",
      "   0.09932584 -0.19444567]\n",
      " [-0.08620304 -0.05046946  0.153393    0.249705    0.04098173 -0.24233586\n",
      "   0.0832704  -0.17763762]\n",
      " [-0.06869102 -0.07981449  0.08653191  0.23194396  0.07424198 -0.26648927\n",
      "   0.15212798 -0.17786247]\n",
      " [-0.07173284 -0.04006358  0.10941169  0.24177086  0.05110766 -0.24721669\n",
      "   0.1280851  -0.18459192]\n",
      " [-0.06937295 -0.02719409  0.21482497  0.25049806  0.05745631 -0.21421714\n",
      "   0.09730962 -0.14651461]\n",
      " [-0.06273372 -0.00998281  0.15115732  0.24358483  0.02394775 -0.21825634\n",
      "   0.13011798 -0.18455733]\n",
      " [-0.096766   -0.05409856  0.1475444   0.25533196  0.0648509  -0.25672626\n",
      "   0.1096959  -0.19036612]\n",
      " [-0.05718237 -0.03441217  0.17377762  0.22222733  0.05863852 -0.24582982\n",
      "   0.10705985 -0.2073651 ]\n",
      " [-0.03478567 -0.0452209   0.1538521   0.2507874   0.05367718 -0.27317974\n",
      "   0.15091683 -0.19544123]\n",
      " [-0.08573345 -0.04740385  0.18497421  0.26370764  0.07862828 -0.20567977\n",
      "   0.07175126 -0.16485494]\n",
      " [-0.04547572 -0.01756926  0.16170786  0.24642707  0.02547632 -0.24438572\n",
      "   0.09200448 -0.19588274]\n",
      " [-0.08760679 -0.048401    0.08661037  0.18777882  0.05570466 -0.25381255\n",
      "   0.1273368  -0.16594452]\n",
      " [-0.0693378  -0.0627903   0.16414785  0.25011796  0.0490699  -0.20300993\n",
      "   0.09404119 -0.19804403]\n",
      " [-0.03574289 -0.03599565  0.19056277  0.25425076  0.03938641 -0.25178373\n",
      "   0.10113234 -0.21249834]\n",
      " [-0.09717624 -0.06919314  0.11860511  0.25764227  0.06344678 -0.23380053\n",
      "   0.09494143 -0.21955235]\n",
      " [-0.0540473  -0.03754912  0.14649817  0.24076179  0.07884543 -0.23914571\n",
      "   0.14779958 -0.18588236]\n",
      " [-0.04273725 -0.03097412  0.21123931  0.2707759   0.04223576 -0.22274062\n",
      "   0.08402275 -0.24140881]\n",
      " [-0.0397758  -0.03174316  0.1370464   0.2526871   0.06483571 -0.23829311\n",
      "   0.14486979 -0.16112112]\n",
      " [-0.08590035 -0.03658753  0.12468158  0.225992    0.08864789 -0.2311536\n",
      "   0.10117029 -0.1835465 ]\n",
      " [-0.07132399 -0.05615037  0.09779197  0.23037542  0.09304631 -0.24301118\n",
      "   0.13318093 -0.16502126]\n",
      " [-0.0739283  -0.03015544  0.16886255  0.24394459  0.06575197 -0.22801779\n",
      "   0.14157243 -0.17501245]\n",
      " [-0.05486304 -0.04233636  0.17764714  0.2542596   0.06550119 -0.2556578\n",
      "   0.115124   -0.22255488]\n",
      " [-0.07843959 -0.03125063  0.2246494   0.30639583  0.07064138 -0.20275581\n",
      "   0.12568532 -0.14174041]\n",
      " [-0.08566707 -0.01683983  0.13279882  0.22261423  0.05182464 -0.21945348\n",
      "   0.09414655 -0.17883883]\n",
      " [-0.07784174 -0.07100548  0.15701893  0.22765073  0.06801444 -0.21983725\n",
      "   0.10286786 -0.20886526]\n",
      " [-0.0760071  -0.05107326  0.1429759   0.24330008  0.06908151 -0.22265482\n",
      "   0.11022141 -0.19202094]\n",
      " [-0.06824129 -0.01236029  0.17395341  0.23792721  0.0565727  -0.22677836\n",
      "   0.08837355 -0.14882928]\n",
      " [-0.09308242 -0.02458704  0.14710075  0.2226225   0.05780403 -0.2433763\n",
      "   0.10146149 -0.21472016]\n",
      " [-0.09441109 -0.04740277  0.14333473  0.19917074  0.0588125  -0.2563069\n",
      "   0.12726904 -0.204441  ]\n",
      " [-0.05572934 -0.04675274  0.19594258  0.23282397  0.03563739 -0.24260607\n",
      "   0.11121707 -0.15975659]\n",
      " [-0.02839347 -0.04958212  0.08708368  0.2106648   0.07932287 -0.23747952\n",
      "   0.12095699 -0.19100395]\n",
      " [-0.02370457 -0.01010494  0.18226622  0.273588    0.03113748 -0.2505843\n",
      "   0.09147927 -0.19127938]\n",
      " [-0.07355379 -0.01304254  0.19183286  0.24144952  0.0600401  -0.21153228\n",
      "   0.09912322 -0.21044075]\n",
      " [-0.05368533 -0.0074525   0.13470326  0.24322633  0.0409553  -0.23909768\n",
      "   0.09457959 -0.18643238]\n",
      " [-0.06077204 -0.02778716  0.18451434  0.24190855  0.04979212 -0.23971085\n",
      "   0.12547855 -0.19134337]\n",
      " [-0.07758699 -0.02406624  0.14043984  0.2410127   0.0723393  -0.2575561\n",
      "   0.11398637 -0.16901429]\n",
      " [-0.07346046 -0.03433783  0.14418982  0.23336114  0.07747427 -0.25501657\n",
      "   0.13711803 -0.20379563]\n",
      " [-0.05605035 -0.04481343  0.19656028  0.24268517  0.04751233 -0.23377448\n",
      "   0.12890656 -0.19057143]\n",
      " [-0.04463416 -0.03132282  0.16224104  0.23415215  0.03554738 -0.2416201\n",
      "   0.14697403 -0.15109316]]\n",
      "(44, 8)\n",
      "accuracy: 0.3181818181818182, precision: 0.10359408033826639, recall: 0.3181818181818182, f1: 0.15629984051036683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.07937971 -0.03585978  0.24318078  0.21220684  0.05697636 -0.2129086\n",
      "   0.12992628 -0.15071318]\n",
      " [-0.05919598 -0.03855769  0.18248595  0.2327441   0.0689761  -0.24293953\n",
      "   0.10556726 -0.2482579 ]\n",
      " [-0.02604573 -0.02573624  0.20854789  0.26733598  0.0540566  -0.2508483\n",
      "   0.14766586 -0.17012909]\n",
      " [-0.05434408 -0.04049346  0.18137865  0.22494963  0.04662548 -0.24654049\n",
      "   0.09898834 -0.2674438 ]\n",
      " [-0.02450818 -0.04095389  0.19866809  0.24746597  0.04153187 -0.24013743\n",
      "   0.12633607 -0.17720436]\n",
      " [-0.03750018 -0.0554045   0.17246822  0.2932757   0.01277485 -0.2184652\n",
      "   0.11638694 -0.2225906 ]\n",
      " [-0.06819884 -0.05687578  0.1714259   0.24855341  0.03871613 -0.2619958\n",
      "   0.09833326 -0.20528896]\n",
      " [-0.05012924 -0.0871568   0.10303994  0.23089942  0.07047917 -0.28722566\n",
      "   0.17031492 -0.20525196]\n",
      " [-0.05304911 -0.04690537  0.12746873  0.23912573  0.04694141 -0.266917\n",
      "   0.14417912 -0.21304822]\n",
      " [-0.050087   -0.03402544  0.23367059  0.2493954   0.05624505 -0.2366397\n",
      "   0.11357842 -0.17864284]\n",
      " [-0.0427442  -0.01656205  0.1720225   0.2403714   0.01968877 -0.23964265\n",
      "   0.14663112 -0.21477382]\n",
      " [-0.07783783 -0.06041837  0.16773285  0.25203592  0.05999485 -0.27693376\n",
      "   0.12434826 -0.22013943]\n",
      " [-0.03617962 -0.04167236  0.19283514  0.2203343   0.05498157 -0.2675057\n",
      "   0.12428451 -0.23891589]\n",
      " [-0.01419622 -0.05452057  0.17299156  0.24847502  0.04784771 -0.29302996\n",
      "   0.16719505 -0.22388083]\n",
      " [-0.0671719  -0.05606493  0.20572501  0.26111257  0.07701337 -0.22613847\n",
      "   0.08749917 -0.19594432]\n",
      " [-0.02442356 -0.02486179  0.18061146  0.24396852  0.02253399 -0.26570666\n",
      "   0.10626538 -0.22686662]\n",
      " [-0.06949554 -0.05460794  0.10313929  0.18519524  0.05097088 -0.272775\n",
      "   0.1441713  -0.19242969]\n",
      " [-0.05040662 -0.07103432  0.18535629  0.24864258  0.04441302 -0.22302553\n",
      "   0.10898597 -0.22565791]\n",
      " [-0.01475868 -0.04354265  0.2091712   0.2528971   0.03608546 -0.27273768\n",
      "   0.11739651 -0.2426408 ]\n",
      " [-0.07704154 -0.0761487   0.13891262  0.25373104  0.05873531 -0.25582322\n",
      "   0.11286283 -0.24992298]\n",
      " [-0.03465948 -0.04616009  0.1634766   0.23866196  0.07597965 -0.26049876\n",
      "   0.16586018 -0.21600066]\n",
      " [-0.02099619 -0.03803081  0.23218016  0.26873466  0.04016805 -0.2457965\n",
      "   0.1008568  -0.27479035]\n",
      " [-0.02086209 -0.03883031  0.15457389  0.24990013  0.06104665 -0.25779808\n",
      "   0.16094398 -0.18938653]\n",
      " [-0.06786005 -0.0420532   0.14234203  0.2242797   0.08673433 -0.25104246\n",
      "   0.11880767 -0.21243073]\n",
      " [-0.05346079 -0.06152603  0.11381885  0.22778614  0.09116766 -0.2627747\n",
      "   0.14822806 -0.19206598]\n",
      " [-0.05494549 -0.03604731  0.18923447  0.24005532  0.062962   -0.24885501\n",
      "   0.15687725 -0.20461777]\n",
      " [-0.03296967 -0.04987961  0.19736978  0.25066707  0.06327023 -0.27804333\n",
      "   0.13144284 -0.25483704]\n",
      " [-0.06047702 -0.04003793  0.24482542  0.30372232  0.06905314 -0.22377194\n",
      "   0.14082599 -0.17178439]\n",
      " [-0.06866164 -0.0232446   0.15183495  0.22091152  0.04883039 -0.23893714\n",
      "   0.11099796 -0.20812419]\n",
      " [-0.05865617 -0.07942706  0.17629148  0.22502874  0.06626466 -0.24113591\n",
      "   0.12027232 -0.24099338]\n",
      " [-0.05604608 -0.05753685  0.16367614  0.24010175  0.06484599 -0.24436297\n",
      "   0.12647969 -0.22149536]\n",
      " [-0.05122615 -0.01900176  0.1922366   0.23712657  0.05373299 -0.24718684\n",
      "   0.10471372 -0.17642482]\n",
      " [-0.07272146 -0.03007123  0.1657786   0.21921596  0.05478935 -0.2652949\n",
      "   0.12006404 -0.24639997]\n",
      " [-0.07461549 -0.05498305  0.1593438   0.19635113  0.05803993 -0.27778733\n",
      "   0.14269765 -0.23509648]\n",
      " [-0.03649015 -0.05143608  0.21188693  0.23174064  0.03497001 -0.2637104\n",
      "   0.12665744 -0.1891949 ]\n",
      " [-0.00867973 -0.05571629  0.10322988  0.20775416  0.07490819 -0.25707036\n",
      "   0.13538007 -0.21822986]\n",
      " [-0.00178043 -0.01535771  0.20049311  0.27284193  0.02996906 -0.27448425\n",
      "   0.10604379 -0.22332038]\n",
      " [-0.05541889 -0.01973021  0.21170628  0.2390626   0.05770699 -0.23252332\n",
      "   0.11657108 -0.24171823]\n",
      " [-0.03618911 -0.01356025  0.15381429  0.24092656  0.037818   -0.25914827\n",
      "   0.11095843 -0.21532932]\n",
      " [-0.03881664 -0.03572813  0.20274562  0.23940158  0.04756147 -0.26211715\n",
      "   0.1437209  -0.22254026]\n",
      " [-0.05806187 -0.03113317  0.15795104  0.23817772  0.06792377 -0.2787838\n",
      "   0.13058695 -0.19742207]\n",
      " [-0.05362784 -0.04188481  0.16271116  0.23054253  0.07596242 -0.27592808\n",
      "   0.1557715  -0.23445514]\n",
      " [-0.0384255  -0.05194081  0.216057    0.24036705  0.04555324 -0.25496235\n",
      "   0.14606898 -0.22158568]\n",
      " [-0.02567909 -0.03581332  0.18133982  0.23121262  0.03449253 -0.26346862\n",
      "   0.16521028 -0.18161857]]\n",
      "(44, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.3181818181818182, precision: 0.10359408033826639, recall: 0.3181818181818182, f1: 0.15629984051036683\n",
      "[[-0.05751476 -0.04532795  0.26499915  0.21299967  0.05236182 -0.2324005\n",
      "   0.14309268 -0.18154304]\n",
      " [-0.0366569  -0.04890661  0.20610315  0.23274744  0.06297305 -0.26481187\n",
      "   0.12031061 -0.28000367]\n",
      " [-0.0036205  -0.03400836  0.22907472  0.27027404  0.05023941 -0.27252984\n",
      "   0.16214645 -0.2011704 ]\n",
      " [-0.02846737 -0.05020882  0.20461673  0.22452947  0.0400985  -0.26801446\n",
      "   0.11266124 -0.30000034]\n",
      " [-0.00180903 -0.05012894  0.22090712  0.24877357  0.03599462 -0.26085\n",
      "   0.140511   -0.20704412]\n",
      " [-0.01486046 -0.06403038  0.19433194  0.29602638  0.00600228 -0.2383791\n",
      "   0.1309667  -0.25114748]\n",
      " [-0.04697328 -0.06569986  0.19299412  0.24829166  0.03313358 -0.2808124\n",
      "   0.11108732 -0.23409961]\n",
      " [-0.02790643 -0.09675836  0.12346562  0.23078582  0.06366593 -0.30744994\n",
      "   0.1855643  -0.23374079]\n",
      " [-0.03081846 -0.05619026  0.14974004  0.23766202  0.03948877 -0.28612542\n",
      "   0.15786317 -0.24280511]\n",
      " [-0.02652135 -0.04332486  0.2567094   0.24968429  0.05181237 -0.25819564\n",
      "   0.12753826 -0.21262117]\n",
      " [-0.01955806 -0.02544982  0.19780843  0.23846719  0.01188869 -0.2610965\n",
      "   0.16009948 -0.24617669]\n",
      " [-0.0549745  -0.06842253  0.19233693  0.25006786  0.05138527 -0.29713705\n",
      "   0.13675907 -0.25136375]\n",
      " [-0.0114473  -0.0519086   0.21623841  0.22025272  0.04778927 -0.28836715\n",
      "   0.13892266 -0.27190444]\n",
      " [ 0.00956139 -0.06671564  0.19618896  0.24824114  0.03820569 -0.31235915\n",
      "   0.18054807 -0.25329155]\n",
      " [-0.04478548 -0.06711435  0.23032564  0.2600085   0.07170013 -0.24628998\n",
      "   0.10073014 -0.22931339]\n",
      " [ 0.00031295 -0.03458864  0.20419562  0.2427356   0.0161308  -0.28652602\n",
      "   0.11785127 -0.2590608 ]\n",
      " [-0.04800097 -0.06330874  0.1231066   0.18383358  0.04352281 -0.29170287\n",
      "   0.15842454 -0.22051746]\n",
      " [-0.02810458 -0.08196501  0.21140474  0.24829084  0.03645609 -0.24309987\n",
      "   0.12151819 -0.2550811 ]\n",
      " [ 0.0100216  -0.05368066  0.23223945  0.25289387  0.02907135 -0.29329914\n",
      "   0.13074462 -0.2742664 ]\n",
      " [-0.05342155 -0.08572334  0.16392043  0.25089154  0.05016118 -0.2770757\n",
      "   0.12760627 -0.2808864 ]\n",
      " [-0.01201911 -0.05705018  0.18476816  0.23797657  0.07013451 -0.28103834\n",
      "   0.18085742 -0.2471287 ]\n",
      " [ 0.0044751  -0.04801075  0.25772685  0.26780987  0.03455987 -0.26828635\n",
      "   0.11400971 -0.30931467]\n",
      " [ 0.00131372 -0.04774764  0.1765246   0.24916342  0.05314821 -0.27691287\n",
      "   0.17426169 -0.21848676]\n",
      " [-0.04645021 -0.0495995   0.16333751  0.22339664  0.08183318 -0.26987743\n",
      "   0.1342931  -0.24261296]\n",
      " [-0.03199717 -0.0692118   0.13353288  0.22629929  0.08617333 -0.28147903\n",
      "   0.16086622 -0.22013962]\n",
      " [-0.03301185 -0.04467504  0.21310851  0.23752102  0.05707335 -0.269039\n",
      "   0.16979396 -0.23560381]\n",
      " [-0.00648521 -0.06028478  0.22143576  0.24882093  0.05675746 -0.29931873\n",
      "   0.14448331 -0.28817236]\n",
      " [-0.03934998 -0.05137075  0.2688449   0.3026243   0.06390435 -0.24375784\n",
      "   0.15308678 -0.20265163]\n",
      " [-0.04771762 -0.031787    0.17468576  0.22052272  0.04258937 -0.25860617\n",
      "   0.12526315 -0.23837958]\n",
      " [-0.03565557 -0.09054866  0.20034607  0.22362716  0.06134309 -0.26214162\n",
      "   0.13488275 -0.27438277]\n",
      " [-0.03302661 -0.06622705  0.18860525  0.2384254   0.05771801 -0.2653832\n",
      "   0.13954376 -0.2522919 ]\n",
      " [-0.03068135 -0.02820376  0.21364877  0.23806241  0.04744142 -0.2668538\n",
      "   0.117801   -0.20526852]\n",
      " [-0.04876561 -0.03794299  0.18876024  0.21740493  0.04810723 -0.286816\n",
      "   0.13565443 -0.2794028 ]\n",
      " [-0.05096095 -0.06539603  0.17967817  0.19452697  0.05364063 -0.2992636\n",
      "   0.15543309 -0.26694804]\n",
      " [-0.01367119 -0.05835497  0.23145694  0.23228607  0.03142674 -0.2838603\n",
      "   0.14012267 -0.21940565]\n",
      " [ 0.01391165 -0.06433365  0.1231805   0.20587422  0.0676585  -0.2759792\n",
      "   0.1474549  -0.2462036 ]\n",
      " [ 0.02405014 -0.02341334  0.22275494  0.273693    0.02507763 -0.29780233\n",
      "   0.11813038 -0.25696397]\n",
      " [-0.03337729 -0.0286315   0.23552017  0.23782     0.0524449  -0.2523806\n",
      "   0.1315301  -0.27462512]\n",
      " [-0.01501625 -0.02199717  0.17701861  0.2397579   0.03156079 -0.278622\n",
      "   0.12483306 -0.245601  ]\n",
      " [-0.01242703 -0.04674148  0.22496441  0.23854391  0.04203523 -0.2837783\n",
      "   0.1593338  -0.25508   ]\n",
      " [-0.03595337 -0.04083489  0.17954266  0.23667735  0.06068382 -0.29936337\n",
      "   0.14439484 -0.22663757]\n",
      " [-0.03029642 -0.05175129  0.18545866  0.22989914  0.07078958 -0.29568338\n",
      "   0.17124571 -0.26623505]\n",
      " [-0.0176341  -0.06142039  0.23949483  0.2388215   0.04059837 -0.27508783\n",
      "   0.16045505 -0.25427336]\n",
      " [-0.00363352 -0.04325991  0.20452762  0.22971907  0.03075354 -0.28489238\n",
      "   0.18112433 -0.21364568]]\n",
      "(44, 8)\n",
      "accuracy: 0.29545454545454547, precision: 0.1008869179600887, recall: 0.29545454545454547, f1: 0.15041322314049588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.03983711 -0.0554316   0.28236657  0.22505817  0.04664636 -0.2513674\n",
      "   0.15158825 -0.20857914]\n",
      " [-0.01944908 -0.06015187  0.22432789  0.24629949  0.05638549 -0.285923\n",
      "   0.13080005 -0.30813402]\n",
      " [ 0.0141869  -0.04281423  0.24493141  0.2845133   0.04557498 -0.29342896\n",
      "   0.17239156 -0.22866993]\n",
      " [-0.00775667 -0.06040499  0.22282869  0.23723935  0.03267398 -0.28865936\n",
      "   0.12246887 -0.32863593]\n",
      " [ 0.01638476 -0.05977219  0.23834527  0.26243767  0.02989054 -0.2810686\n",
      "   0.1505996  -0.23272051]\n",
      " [ 0.00344197 -0.0730467   0.21147676  0.3101366  -0.0011419  -0.25748366\n",
      "   0.14131136 -0.2757765 ]\n",
      " [-0.02964091 -0.0752011   0.20993611  0.26145852  0.02594045 -0.29916027\n",
      "   0.11953492 -0.25955516]\n",
      " [-0.01091454 -0.1067988   0.13921012  0.24479797  0.05566243 -0.32662594\n",
      "   0.19587864 -0.2580491 ]\n",
      " [-0.01309313 -0.06621451  0.16718961  0.24934748  0.03198012 -0.3050418\n",
      "   0.16770591 -0.2687082 ]\n",
      " [-0.00734175 -0.05336475  0.27448913  0.2625884   0.04669322 -0.27869165\n",
      "   0.13739806 -0.24220118]\n",
      " [-0.0013376  -0.03499658  0.21825266  0.24937071  0.0038815  -0.28185248\n",
      "   0.16943842 -0.2734378 ]\n",
      " [-0.0369063  -0.07731137  0.2112782   0.26328468  0.0433491  -0.31732225\n",
      "   0.14522624 -0.27934974]\n",
      " [ 0.008458   -0.06267238  0.23446408  0.23329604  0.04016712 -0.30810863\n",
      "   0.14944042 -0.30069545]\n",
      " [ 0.02835126 -0.0790587   0.21437848  0.26131842  0.02811947 -0.33108655\n",
      "   0.19010925 -0.2784425 ]\n",
      " [-0.02700821 -0.07859614  0.24881369  0.27270806  0.06528202 -0.26613307\n",
      "   0.10976744 -0.25818086]\n",
      " [ 0.0196777  -0.04462642  0.22261935  0.2554778   0.00871618 -0.30634013\n",
      "   0.12564716 -0.28719735]\n",
      " [-0.03110791 -0.0726677   0.13889477  0.19449447  0.03650669 -0.31026262\n",
      "   0.16880275 -0.24522895]\n",
      " [-0.0106029  -0.09366253  0.23154743  0.26136404  0.02830633 -0.26314497\n",
      "   0.13001268 -0.2810028 ]\n",
      " [ 0.02950195 -0.06434903  0.25022966  0.26655084  0.02156449 -0.3128413\n",
      "   0.13961689 -0.30191904]\n",
      " [-0.03442982 -0.09611892  0.18310246  0.2632954   0.04077141 -0.29748285\n",
      "   0.1374936  -0.30795538]\n",
      " [ 0.0056537  -0.06822156  0.20107484  0.24901442  0.06389345 -0.30051202\n",
      "   0.19179603 -0.27406472]\n",
      " [ 0.02479164 -0.05927575  0.27769548  0.28108957  0.02718895 -0.29039\n",
      "   0.12273353 -0.33910564]\n",
      " [ 0.01889504 -0.05682001  0.19414943  0.26222298  0.04454468 -0.29536098\n",
      "   0.18335833 -0.2435052 ]\n",
      " [-0.02923873 -0.05857828  0.18000346  0.23446956  0.07648294 -0.28894025\n",
      "   0.14534284 -0.2693302 ]\n",
      " [-0.01436583 -0.07736089  0.14881124  0.23811936  0.07999458 -0.29953253\n",
      "   0.16908054 -0.24464099]\n",
      " [-0.01565639 -0.05429187  0.23165312  0.24780825  0.05105447 -0.28835964\n",
      "   0.17913124 -0.26287296]\n",
      " [ 0.01439173 -0.07155918  0.24018449  0.26130575  0.04945279 -0.31986862\n",
      "   0.15285328 -0.3182106 ]\n",
      " [-0.02190888 -0.06344541  0.28717238  0.3141567   0.0573118  -0.26297694\n",
      "   0.16109677 -0.22950624]\n",
      " [-0.03071687 -0.04069707  0.19274649  0.23220186  0.0352516  -0.2778346\n",
      "   0.13526197 -0.2648388 ]\n",
      " [-0.01783822 -0.10231125  0.21887162  0.23576382  0.05523872 -0.2824511\n",
      "   0.14472276 -0.30366763]\n",
      " [-0.01477144 -0.07575743  0.20768167  0.25038487  0.04987344 -0.28585494\n",
      "   0.14830272 -0.27931792]\n",
      " [-0.01425302 -0.03787855  0.23001957  0.250244    0.04034515 -0.2864288\n",
      "   0.1268748  -0.23064359]\n",
      " [-0.03001824 -0.04703518  0.20708916  0.22895356  0.04077117 -0.30741718\n",
      "   0.14660627 -0.30818856]\n",
      " [-0.03183203 -0.07664896  0.19578204  0.20518044  0.04821303 -0.3202899\n",
      "   0.16409162 -0.29483083]\n",
      " [ 0.00463779 -0.06570461  0.24700183  0.24500237  0.02763028 -0.30347717\n",
      "   0.14967391 -0.24567242]\n",
      " [ 0.03195556 -0.07310639  0.13942677  0.2166234   0.05977957 -0.29432398\n",
      "   0.15504359 -0.2699437 ]\n",
      " [ 0.04479266 -0.03235298  0.24058801  0.2879577   0.01919238 -0.31925267\n",
      "   0.12611893 -0.28577694]\n",
      " [-0.01557252 -0.03797622  0.2544129   0.24905631  0.04659282 -0.27149397\n",
      "   0.14206141 -0.3030941 ]\n",
      " [ 0.0019197  -0.03106489  0.19534224  0.25140607  0.0245777  -0.29743135\n",
      "   0.13458079 -0.2718669 ]\n",
      " [ 0.00876748 -0.05865559  0.2418198   0.25049308  0.03600552 -0.3042723\n",
      "   0.17076363 -0.28340152]\n",
      " [-0.01826981 -0.05132397  0.19621566  0.24849996  0.0529025  -0.31890574\n",
      "   0.1539527  -0.25235263]\n",
      " [-0.01209466 -0.06196871  0.20334254  0.24290481  0.06460936 -0.31479758\n",
      "   0.1819293  -0.2941445 ]\n",
      " [-0.00140185 -0.07185581  0.25736308  0.2508682   0.03458767 -0.29454315\n",
      "   0.1702789  -0.2822916 ]\n",
      " [ 0.01452308 -0.0517421   0.22292466  0.24087794  0.02557156 -0.30515045\n",
      "   0.19197308 -0.2414654 ]]\n",
      "(44, 8)\n",
      "accuracy: 0.29545454545454547, precision: 0.10885167464114832, recall: 0.29545454545454547, f1: 0.1590909090909091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.24239938e-02 -6.68533444e-02  3.03585321e-01  2.37191632e-01\n",
      "   3.92363295e-02 -2.69889325e-01  1.58708394e-01 -2.36236423e-01]\n",
      " [-1.83088705e-03 -7.21538439e-02  2.46399164e-01  2.59348482e-01\n",
      "   4.85265292e-02 -3.06933820e-01  1.39342025e-01 -3.36851686e-01]\n",
      " [ 3.27942260e-02 -5.29932044e-02  2.64864773e-01  2.98125774e-01\n",
      "   3.95940244e-02 -3.13463718e-01  1.81066141e-01 -2.56140292e-01]\n",
      " [ 1.37731116e-02 -7.21241310e-02  2.45215774e-01  2.49704435e-01\n",
      "   2.40403935e-02 -3.08649004e-01  1.30921096e-01 -3.57323289e-01]\n",
      " [ 3.53209786e-02 -7.08807632e-02  2.59736121e-01  2.75716037e-01\n",
      "   2.18361598e-02 -3.00567687e-01  1.58820301e-01 -2.58275449e-01]\n",
      " [ 2.26100069e-02 -8.26877207e-02  2.32007548e-01  3.23784471e-01\n",
      "  -1.01894774e-02 -2.76681602e-01  1.49731591e-01 -3.01404595e-01]\n",
      " [-1.17147323e-02 -8.57637376e-02  2.30763555e-01  2.74026632e-01\n",
      "   1.70125403e-02 -3.16648483e-01  1.26388505e-01 -2.85299599e-01]\n",
      " [ 7.50590861e-03 -1.17910877e-01  1.58916309e-01  2.59046674e-01\n",
      "   4.61454317e-02 -3.44848633e-01  2.04559118e-01 -2.82414317e-01]\n",
      " [ 5.16214408e-03 -7.78799951e-02  1.88302934e-01  2.60767788e-01\n",
      "   2.30746288e-02 -3.23638022e-01  1.75818264e-01 -2.94870108e-01]\n",
      " [ 1.22691384e-02 -6.45362362e-02  2.96437144e-01  2.74730623e-01\n",
      "   4.05251309e-02 -2.98376769e-01  1.45812571e-01 -2.71733314e-01]\n",
      " [ 1.75551213e-02 -4.55265567e-02  2.42318243e-01  2.60001183e-01\n",
      "  -6.00585714e-03 -3.02478045e-01  1.76927000e-01 -3.00883383e-01]\n",
      " [-1.70924291e-02 -8.75334144e-02  2.34562650e-01  2.76118457e-01\n",
      "   3.36812735e-02 -3.37256432e-01  1.51851535e-01 -3.07456523e-01]\n",
      " [ 2.86501907e-02 -7.41280168e-02  2.56768763e-01  2.46108815e-01\n",
      "   3.08920220e-02 -3.27004075e-01  1.58066720e-01 -3.29408646e-01]\n",
      " [ 4.82640788e-02 -9.25540328e-02  2.36347616e-01  2.74663389e-01\n",
      "   1.65001191e-02 -3.49932611e-01  1.98609963e-01 -3.04016739e-01]\n",
      " [-9.05412808e-03 -9.11911428e-02  2.71561205e-01  2.85140753e-01\n",
      "   5.71752712e-02 -2.85106003e-01  1.16707757e-01 -2.86725521e-01]\n",
      " [ 3.98832113e-02 -5.59383146e-02  2.45256349e-01  2.68374622e-01\n",
      "  -5.51752746e-05 -3.25321168e-01  1.31911963e-01 -3.15487117e-01]\n",
      " [-1.32266749e-02 -8.34416449e-02  1.58113003e-01  2.05691740e-01\n",
      "   2.83913109e-02 -3.28237802e-01  1.77728549e-01 -2.70738095e-01]\n",
      " [ 7.57936761e-03 -1.06424645e-01  2.55675554e-01  2.74420679e-01\n",
      "   1.82680283e-02 -2.82977223e-01  1.36921436e-01 -3.07076901e-01]\n",
      " [ 5.03016002e-02 -7.58577213e-02  2.72796690e-01  2.80483365e-01\n",
      "   1.26985461e-02 -3.31758708e-01  1.46932602e-01 -3.29565614e-01]\n",
      " [-1.43711865e-02 -1.08015239e-01  2.06760868e-01  2.75156498e-01\n",
      "   2.96576675e-02 -3.17998856e-01  1.45820171e-01 -3.35030317e-01]\n",
      " [ 2.37247031e-02 -8.05071592e-02  2.21530095e-01  2.60266840e-01\n",
      "   5.57283573e-02 -3.18359852e-01  2.00917840e-01 -3.00681502e-01]\n",
      " [ 4.55972478e-02 -7.14906752e-02  3.02030325e-01  2.94178873e-01\n",
      "   1.80366449e-02 -3.11886668e-01  1.29692122e-01 -3.68647516e-01]\n",
      " [ 3.70291844e-02 -6.73253164e-02  2.16110021e-01  2.74468958e-01\n",
      "   3.40829231e-02 -3.12602580e-01  1.91070467e-01 -2.68577278e-01]\n",
      " [-1.11181978e-02 -6.93741292e-02  2.00727880e-01  2.45386124e-01\n",
      "   6.97547644e-02 -3.07128996e-01  1.54554442e-01 -2.96426922e-01]\n",
      " [ 4.85695340e-03 -8.70032758e-02  1.68192163e-01  2.49225616e-01\n",
      "   7.26111382e-02 -3.17485034e-01  1.76296860e-01 -2.69421875e-01]\n",
      " [ 2.51840055e-03 -6.49894029e-02  2.54029512e-01  2.58233100e-01\n",
      "   4.35948148e-02 -3.07028532e-01  1.86869293e-01 -2.90184021e-01]\n",
      " [ 3.58817019e-02 -8.35267454e-02  2.63871461e-01  2.73196459e-01\n",
      "   4.03210372e-02 -3.40097547e-01  1.59779578e-01 -3.48507911e-01]\n",
      " [-4.43182187e-03 -7.67702311e-02  3.09819013e-01  3.25526416e-01\n",
      "   4.91276123e-02 -2.81546533e-01  1.67746693e-01 -2.56859183e-01]\n",
      " [-1.27674341e-02 -5.09477258e-02  2.14142010e-01  2.44006634e-01\n",
      "   2.61904355e-02 -2.95964003e-01  1.43711314e-01 -2.91420966e-01]\n",
      " [ 4.86914068e-04 -1.14957348e-01  2.42030233e-01  2.48578578e-01\n",
      "   4.70566973e-02 -3.02041590e-01  1.52782395e-01 -3.32751930e-01]\n",
      " [ 3.64778563e-03 -8.70278329e-02  2.31345147e-01  2.62042195e-01\n",
      "   4.04896885e-02 -3.05666447e-01  1.55959070e-01 -3.06215227e-01]\n",
      " [ 2.87661329e-03 -4.84231934e-02  2.49857068e-01  2.62272179e-01\n",
      "   3.15899067e-02 -3.05115521e-01  1.33938059e-01 -2.56275058e-01]\n",
      " [-1.02797654e-02 -5.79126664e-02  2.29656607e-01  2.40773529e-01\n",
      "   3.10118422e-02 -3.27745646e-01  1.55810431e-01 -3.37392449e-01]\n",
      " [-1.23248491e-02 -8.91323388e-02  2.15995625e-01  2.16210902e-01\n",
      "   4.10335027e-02 -3.40857029e-01  1.71061367e-01 -3.22392255e-01]\n",
      " [ 2.35294737e-02 -7.47603402e-02  2.66423643e-01  2.57810414e-01\n",
      "   2.30932645e-02 -3.22617471e-01  1.57771632e-01 -2.73014039e-01]\n",
      " [ 5.14538661e-02 -8.26012045e-02  1.58852488e-01  2.27734506e-01\n",
      "   4.98008803e-02 -3.11738342e-01  1.61172748e-01 -2.93452919e-01]\n",
      " [ 6.63859993e-02 -4.20055576e-02  2.62699217e-01  3.01956058e-01\n",
      "   1.23573784e-02 -3.40132087e-01  1.31815806e-01 -3.15043211e-01]\n",
      " [ 3.10121663e-03 -4.87411879e-02  2.77467906e-01  2.60309905e-01\n",
      "   3.90465632e-02 -2.89991736e-01  1.50676250e-01 -3.31951290e-01]\n",
      " [ 1.89918242e-02 -4.19716947e-02  2.17168152e-01  2.63410777e-01\n",
      "   1.56139657e-02 -3.15518916e-01  1.43180758e-01 -2.98421025e-01]\n",
      " [ 2.98660137e-02 -7.19049647e-02  2.62358785e-01  2.61798829e-01\n",
      "   2.89341919e-02 -3.23697150e-01  1.80864811e-01 -3.11329722e-01]\n",
      " [ 1.58255920e-04 -6.32325262e-02  2.17131019e-01  2.60562062e-01\n",
      "   4.33567911e-02 -3.37436646e-01  1.61740929e-01 -2.78146267e-01]\n",
      " [ 7.32208416e-03 -7.37140328e-02  2.25205302e-01  2.55965769e-01\n",
      "   5.64469993e-02 -3.33228886e-01  1.91003934e-01 -3.22251976e-01]\n",
      " [ 1.55324098e-02 -8.37865993e-02  2.79405057e-01  2.62553751e-01\n",
      "   2.67514437e-02 -3.13560843e-01  1.78605989e-01 -3.10853720e-01]\n",
      " [ 3.45608778e-02 -6.14641309e-02  2.44931996e-01  2.51291692e-01\n",
      "   1.87233239e-02 -3.24595839e-01  2.00649440e-01 -2.69909650e-01]]\n",
      "(44, 8)\n",
      "accuracy: 0.29545454545454547, precision: 0.11489898989898989, recall: 0.29545454545454547, f1: 0.16545454545454547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0037211  -0.07757786  0.31798574  0.24541089  0.03016576 -0.2889893\n",
      "   0.16341618 -0.26020002]\n",
      " [ 0.02276257 -0.08350785  0.26206547  0.26839542  0.03990006 -0.3269115\n",
      "   0.14518076 -0.36169153]\n",
      " [ 0.05945155 -0.06280505  0.27857035  0.30750218  0.03240065 -0.33292657\n",
      "   0.18681604 -0.2800941 ]\n",
      " [ 0.04225165 -0.08347282  0.2602108   0.2581114   0.01482194 -0.3272885\n",
      "   0.13674484 -0.38194948]\n",
      " [ 0.06232792 -0.08098568  0.27457297  0.28501672  0.01284668 -0.31946346\n",
      "   0.16407238 -0.28014976]\n",
      " [ 0.04917042 -0.09191433  0.24639724  0.33382994 -0.01953563 -0.2956125\n",
      "   0.15564868 -0.32342798]\n",
      " [ 0.0142736  -0.09540233  0.24543107  0.28264737  0.007096   -0.3343336\n",
      "   0.13079818 -0.3077746 ]\n",
      " [ 0.0331294  -0.12826109  0.17243686  0.26931986  0.03648135 -0.36265892\n",
      "   0.2104986  -0.3040551 ]\n",
      " [ 0.03094284 -0.08881564  0.20224273  0.2679977   0.01379902 -0.34196177\n",
      "   0.18083902 -0.31776267]\n",
      " [ 0.04058674 -0.07541607  0.31153452  0.28258142  0.03298427 -0.31801233\n",
      "   0.15188943 -0.29768473]\n",
      " [ 0.04504254 -0.05579595  0.2588735   0.26758865 -0.01757312 -0.32217914\n",
      "   0.18188335 -0.32491317]\n",
      " [ 0.0114917  -0.09721704  0.24964525  0.2849668   0.02273172 -0.35706037\n",
      "   0.15634008 -0.3315742 ]\n",
      " [ 0.0570423  -0.08458327  0.27228174  0.2551746   0.02023256 -0.3447665\n",
      "   0.16349308 -0.3540654 ]\n",
      " [ 0.07629243 -0.10495615  0.25113907  0.28386962  0.00417906 -0.3686391\n",
      "   0.20447423 -0.32604182]\n",
      " [ 0.01667332 -0.10263949  0.28735352  0.29374826  0.04752621 -0.3039096\n",
      "   0.12135141 -0.3115068 ]\n",
      " [ 0.06787134 -0.06679332  0.26057363  0.2771824  -0.00985765 -0.3441982\n",
      "   0.13562658 -0.33987087]\n",
      " [ 0.01174841 -0.09359092  0.17178783  0.21357206  0.01936781 -0.3450723\n",
      "   0.18411377 -0.29330045]\n",
      " [ 0.0345359  -0.11769618  0.27188373  0.28313363  0.00709871 -0.30210495\n",
      "   0.14159825 -0.32915372]\n",
      " [ 0.07924426 -0.0866487   0.2875693   0.29049715  0.00314911 -0.35066077\n",
      "   0.15200141 -0.35337853]\n",
      " [ 0.01376386 -0.11933525  0.22318897  0.2832964   0.01808628 -0.33794716\n",
      "   0.1518129  -0.35850725]\n",
      " [ 0.05054311 -0.09203441  0.23559241  0.2676639   0.046354   -0.33590814\n",
      "   0.20726678 -0.32360584]\n",
      " [ 0.07495822 -0.08295139  0.31865323  0.30341178  0.00765367 -0.33250087\n",
      "   0.13505235 -0.39411002]\n",
      " [ 0.06299414 -0.07697067  0.23109232  0.28290883  0.02278415 -0.32938206\n",
      "   0.19640371 -0.29059517]\n",
      " [ 0.01433587 -0.07950016  0.21509016  0.25284162  0.06196456 -0.32447258\n",
      "   0.16066435 -0.3199509 ]\n",
      " [ 0.03099611 -0.09683107  0.18166359  0.2563682   0.06413409 -0.33480862\n",
      "   0.18138197 -0.29073822]\n",
      " [ 0.02850207 -0.07511     0.26970175  0.2648391   0.0346318  -0.3247812\n",
      "   0.19201148 -0.31395102]\n",
      " [ 0.06686513 -0.09488903  0.27962366  0.28140703  0.02930978 -0.35998648\n",
      "   0.16445157 -0.37470827]\n",
      " [ 0.02265175 -0.0891477   0.32543883  0.33343902  0.03938157 -0.30004162\n",
      "   0.17200918 -0.28053916]\n",
      " [ 0.01174513 -0.06066458  0.22915605  0.25242406  0.01668263 -0.31281945\n",
      "   0.14963098 -0.31423303]\n",
      " [ 0.02773126 -0.12671629  0.25764     0.25765237  0.03697293 -0.32110167\n",
      "   0.1578371  -0.35748857]\n",
      " [ 0.03013337 -0.09842736  0.2480696   0.2701423   0.03030371 -0.32520017\n",
      "   0.16145182 -0.3301674 ]\n",
      " [ 0.02656144 -0.05829258  0.2641854   0.2707141   0.021745   -0.32285345\n",
      "   0.13864622 -0.27874714]\n",
      " [ 0.01699425 -0.068268    0.24525389  0.24868052  0.02008902 -0.34704447\n",
      "   0.1622366  -0.36266038]\n",
      " [ 0.01459612 -0.10071935  0.22965592  0.22338921  0.03249161 -0.36092228\n",
      "   0.17525485 -0.3463897 ]\n",
      " [ 0.04976176 -0.08336663  0.27947462  0.2670899   0.01681657 -0.34067118\n",
      "   0.16361919 -0.29618168]\n",
      " [ 0.07742138 -0.09202891  0.17223448  0.23508933  0.03956398 -0.3283168\n",
      "   0.16519418 -0.31413382]\n",
      " [ 0.0960105  -0.05124407  0.2780748   0.31168133  0.0041988  -0.36048245\n",
      "   0.13534246 -0.3399551 ]\n",
      " [ 0.02863221 -0.05857469  0.29400024  0.26783317  0.03038318 -0.30769578\n",
      "   0.1566908  -0.35679325]\n",
      " [ 0.04326681 -0.05214782  0.23210257  0.27204323  0.00579173 -0.33290726\n",
      "   0.14950678 -0.32119438]\n",
      " [ 0.0586621  -0.08409885  0.27619237  0.26920325  0.02060587 -0.34256202\n",
      "   0.18785426 -0.33604547]\n",
      " [ 0.02606424 -0.07405908  0.23181297  0.26944765  0.03311147 -0.35514092\n",
      "   0.16677117 -0.30090353]\n",
      " [ 0.03486121 -0.0850324   0.24049932  0.26584664  0.04670464 -0.3510282\n",
      "   0.19747353 -0.34673092]\n",
      " [ 0.04034285 -0.09449707  0.29465902  0.27071512  0.01766486 -0.3318357\n",
      "   0.18441096 -0.33575645]\n",
      " [ 0.06322704 -0.07099878  0.25943428  0.25843203  0.01074077 -0.3438023\n",
      "   0.20634954 -0.29458138]]\n",
      "(44, 8)\n",
      "accuracy: 0.25, precision: 0.109375, recall: 0.25, f1: 0.15217391304347827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.03004782 -0.08859837  0.3347422   0.2521309   0.01971946 -0.30674428\n",
      "   0.1663629  -0.28399658]\n",
      " [ 0.04814867 -0.09577615  0.28056037  0.2753587   0.02990964 -0.34547085\n",
      "   0.14939988 -0.38585794]\n",
      " [ 0.08636442 -0.07343441  0.29492247  0.31495315  0.02388813 -0.35128266\n",
      "   0.19108376 -0.30356538]\n",
      " [ 0.07081744 -0.09539009  0.27830836  0.26457247  0.00448884 -0.3450395\n",
      "   0.14042647 -0.40597704]\n",
      " [ 0.08969261 -0.09189506  0.29221544  0.29161462  0.00192129 -0.33751175\n",
      "   0.16786095 -0.30183625]\n",
      " [ 0.07697675 -0.10193251  0.26372543  0.34176108 -0.03020959 -0.31350902\n",
      "   0.15987591 -0.34463844]\n",
      " [ 0.04043044 -0.10615558  0.26369047  0.28940737 -0.00395952 -0.35042548\n",
      "   0.13398352 -0.33029464]\n",
      " [ 0.06005338 -0.13915327  0.18899176  0.27741405  0.02583635 -0.37956268\n",
      "   0.21491848 -0.32556838]\n",
      " [ 0.05712185 -0.10037515  0.21928203  0.27283484  0.00324626 -0.35884616\n",
      "   0.18425086 -0.3403083 ]\n",
      " [ 0.06909467 -0.08687287  0.32977176  0.28838342  0.02442422 -0.3363965\n",
      "   0.15617396 -0.32270956]\n",
      " [ 0.07338782 -0.06670979  0.27886063  0.27312618 -0.03020528 -0.34041128\n",
      "   0.18479341 -0.34839913]\n",
      " [ 0.04090971 -0.10752769  0.26859307  0.29160115  0.01055149 -0.37595016\n",
      "   0.15923372 -0.35586363]\n",
      " [ 0.08656614 -0.09596133  0.290788    0.26260546  0.00837653 -0.36111623\n",
      "   0.16792415 -0.3775647 ]\n",
      " [ 0.10476071 -0.11798026  0.26939094  0.29108632 -0.00911137 -0.3859765\n",
      "   0.20843199 -0.34779948]\n",
      " [ 0.04249788 -0.11455283  0.30555993  0.30014136  0.03658418 -0.32104206\n",
      "   0.12389074 -0.33568007]\n",
      " [ 0.09609331 -0.07859492  0.27921477  0.28311    -0.0209884  -0.3620273\n",
      "   0.13731915 -0.36365575]\n",
      " [ 0.03735046 -0.10457811  0.18812034  0.21981467  0.00906488 -0.3610772\n",
      "   0.18954311 -0.31532082]\n",
      " [ 0.06218401 -0.12983628  0.29136437  0.28961787 -0.00522372 -0.3205614\n",
      "   0.14489833 -0.3507108 ]\n",
      " [ 0.1084062  -0.09830923  0.30580541  0.29839104 -0.00827168 -0.3682993\n",
      "   0.15496825 -0.37635446]\n",
      " [ 0.04231995 -0.13183406  0.24328741  0.28968084  0.00522123 -0.3563944\n",
      "   0.15640713 -0.38137388]\n",
      " [ 0.07807765 -0.10439894  0.25223723  0.27337736  0.03516675 -0.3525209\n",
      "   0.21236764 -0.34579465]\n",
      " [ 0.10469341 -0.09564398  0.3391078   0.31055355 -0.00454574 -0.3518785\n",
      "   0.1384534  -0.41926122]\n",
      " [ 0.08900136 -0.08768466  0.2489917   0.28912294  0.01047231 -0.34461915\n",
      "   0.20029041 -0.3121527 ]\n",
      " [ 0.04012512 -0.09060279  0.23227772  0.2577662   0.05305082 -0.34065062\n",
      "   0.16502239 -0.34298953]\n",
      " [ 0.0573207  -0.10753499  0.19743372  0.26206276  0.05451947 -0.3513931\n",
      "   0.1854614  -0.31211734]\n",
      " [ 0.05455358 -0.08653857  0.28860617  0.26928067  0.02405014 -0.34132117\n",
      "   0.19565134 -0.3372194 ]\n",
      " [ 0.09793258 -0.10677472  0.29886034  0.28807992  0.01667009 -0.37857285\n",
      "   0.16703507 -0.4001462 ]\n",
      " [ 0.05007443 -0.10299536  0.34404233  0.3402468   0.02808709 -0.31755626\n",
      "   0.17452829 -0.30329916]\n",
      " [ 0.03692454 -0.07112786  0.24684948  0.2592689   0.0057301  -0.3285371\n",
      "   0.15382427 -0.33631313]\n",
      " [ 0.05565774 -0.13884634  0.27676237  0.26452923  0.02553087 -0.33871508\n",
      "   0.16162439 -0.38194603]\n",
      " [ 0.05656964 -0.1109587   0.2685716   0.27648365  0.0186369  -0.34407017\n",
      "   0.16518232 -0.3541305 ]\n",
      " [ 0.05063363 -0.06875266  0.28185415  0.2776013   0.01078751 -0.33930182\n",
      "   0.14153744 -0.30104303]\n",
      " [ 0.04464038 -0.07961451  0.2640707   0.25430527  0.00771344 -0.36503345\n",
      "   0.16715492 -0.3873892 ]\n",
      " [ 0.04218245 -0.11307734  0.2462821   0.22854164  0.02256628 -0.379866\n",
      "   0.17771424 -0.3704001 ]\n",
      " [ 0.07619512 -0.09214808  0.29494977  0.2750354   0.00933144 -0.35738665\n",
      "   0.16774105 -0.3192362 ]\n",
      " [ 0.10408856 -0.10264228  0.18890369  0.24065045  0.028313   -0.34422028\n",
      "   0.16817679 -0.33459702]\n",
      " [ 0.12588283 -0.06156411  0.2969381   0.319436   -0.00536064 -0.37988746\n",
      "   0.137027   -0.364684  ]\n",
      " [ 0.05438187 -0.06938256  0.31333366  0.27344     0.02050408 -0.32401985\n",
      "   0.16092728 -0.38115013]\n",
      " [ 0.06794211 -0.06333259  0.250161    0.27807826 -0.00485623 -0.3489504\n",
      "   0.15432546 -0.3438287 ]\n",
      " [ 0.08837101 -0.09753887  0.2925899   0.27426305  0.01028829 -0.36015594\n",
      "   0.19286561 -0.3602091 ]\n",
      " [ 0.05233163 -0.08605999  0.24956903  0.2758822   0.02141393 -0.37160683\n",
      "   0.17039734 -0.32359523]\n",
      " [ 0.0623444  -0.09740388  0.2586799   0.27398318  0.03503635 -0.36736915\n",
      "   0.20244752 -0.37050158]\n",
      " [ 0.06546737 -0.10578291  0.31295058  0.2770683   0.00702846 -0.34922767\n",
      "   0.188479   -0.36027542]\n",
      " [ 0.09243333 -0.08096192  0.277292    0.2634989   0.0016794  -0.36235604\n",
      "   0.21040905 -0.31881854]]\n",
      "(44, 8)\n",
      "accuracy: 0.22727272727272727, precision: 0.12088274044795783, recall: 0.22727272727272727, f1: 0.15162337662337663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.20535670e-02 -9.93702114e-02  3.47956300e-01  2.67195612e-01\n",
      "   9.57131386e-03 -3.24277461e-01  1.66540995e-01 -3.04242581e-01]\n",
      " [ 6.98672011e-02 -1.07372344e-01  2.94862866e-01  2.91760266e-01\n",
      "   1.99082438e-02 -3.64357769e-01  1.51210576e-01 -4.06230003e-01]\n",
      " [ 1.08985946e-01 -8.38700533e-02  3.07773530e-01  3.30933452e-01\n",
      "   1.50514189e-02 -3.69976401e-01  1.92898497e-01 -3.23287517e-01]\n",
      " [ 9.52155814e-02 -1.06712729e-01  2.92375982e-01  2.80757010e-01\n",
      "  -5.43617271e-03 -3.62930745e-01  1.42012805e-01 -4.26475793e-01]\n",
      " [ 1.12215891e-01 -1.02489367e-01  3.05929124e-01  3.07483673e-01\n",
      "  -8.62699375e-03 -3.55920374e-01  1.69035167e-01 -3.20876807e-01]\n",
      " [ 1.00391030e-01 -1.11893743e-01  2.77828217e-01  3.58826995e-01\n",
      "  -4.04616185e-02 -3.32303464e-01  1.61761433e-01 -3.62676024e-01]\n",
      " [ 6.23559095e-02 -1.16646767e-01  2.78164536e-01  3.05037856e-01\n",
      "  -1.49229802e-02 -3.66966993e-01  1.35009080e-01 -3.49806130e-01]\n",
      " [ 8.30643624e-02 -1.49696708e-01  2.01752320e-01  2.93176919e-01\n",
      "   1.52710117e-02 -3.96176398e-01  2.16975957e-01 -3.44051927e-01]\n",
      " [ 7.86258504e-02 -1.11912265e-01  2.32475847e-01  2.86297470e-01\n",
      "  -7.21551850e-03 -3.75719666e-01  1.85264140e-01 -3.59411031e-01]\n",
      " [ 9.30101275e-02 -9.80947018e-02  3.43651474e-01  3.03949833e-01\n",
      "   1.59890447e-02 -3.54967475e-01  1.57674909e-01 -3.43848467e-01]\n",
      " [ 9.72529948e-02 -7.75309727e-02  2.94733167e-01  2.89148808e-01\n",
      "  -4.23710570e-02 -3.59041601e-01  1.85295537e-01 -3.68620485e-01]\n",
      " [ 6.60419539e-02 -1.17779911e-01  2.83261627e-01  3.07791114e-01\n",
      "  -9.92730260e-04 -3.95084381e-01  1.59235790e-01 -3.76879126e-01]\n",
      " [ 1.11306503e-01 -1.07620366e-01  3.04918915e-01  2.79640138e-01\n",
      "  -3.10771912e-03 -3.78065616e-01  1.69731379e-01 -3.97627294e-01]\n",
      " [ 1.29037887e-01 -1.30134284e-01  2.83716559e-01  3.07072282e-01\n",
      "  -2.17812434e-02 -4.03296053e-01  2.09657818e-01 -3.66718650e-01]\n",
      " [ 6.45872429e-02 -1.25441894e-01  3.19673657e-01  3.15583229e-01\n",
      "   2.63950806e-02 -3.38705778e-01  1.24474041e-01 -3.56584460e-01]\n",
      " [ 1.19906187e-01 -8.97670686e-02  2.93689847e-01  2.98432082e-01\n",
      "  -3.20318304e-02 -3.80140781e-01  1.36922777e-01 -3.84081304e-01]\n",
      " [ 5.89531511e-02 -1.15346357e-01  2.00881645e-01  2.33581424e-01\n",
      "  -1.04527920e-03 -3.77195746e-01  1.92264229e-01 -3.34543586e-01]\n",
      " [ 8.54836255e-02 -1.41575754e-01  3.06813657e-01  3.06152672e-01\n",
      "  -1.68942809e-02 -3.38955939e-01  1.45919874e-01 -3.69080961e-01]\n",
      " [ 1.33165359e-01 -1.09884754e-01  3.19751978e-01  3.15674782e-01\n",
      "  -1.92432255e-02 -3.85865659e-01  1.55547768e-01 -3.96248370e-01]\n",
      " [ 6.64047077e-02 -1.44109458e-01  2.58869469e-01  3.05852741e-01\n",
      "  -6.84861839e-03 -3.74830961e-01  1.57989725e-01 -4.01194662e-01]\n",
      " [ 1.00959286e-01 -1.16420820e-01  2.64673114e-01  2.87858218e-01\n",
      "   2.47030798e-02 -3.69376212e-01  2.15000153e-01 -3.65192473e-01]\n",
      " [ 1.29245996e-01 -1.08221345e-01  3.55117381e-01  3.27395976e-01\n",
      "  -1.67628489e-02 -3.71573091e-01  1.38796568e-01 -4.41024482e-01]\n",
      " [ 1.11733153e-01 -9.79972333e-02  2.62849987e-01  3.02783400e-01\n",
      "  -9.26021487e-04 -3.59660983e-01  2.01920733e-01 -3.30776215e-01]\n",
      " [ 6.18192628e-02 -1.01269856e-01  2.45661765e-01  2.71432459e-01\n",
      "   4.39316556e-02 -3.57271612e-01  1.67071775e-01 -3.62741947e-01]\n",
      " [ 7.94798359e-02 -1.17984742e-01  2.09680259e-01  2.76126325e-01\n",
      "   4.44041416e-02 -3.67936909e-01  1.87084049e-01 -3.30386668e-01]\n",
      " [ 7.63459504e-02 -9.81466621e-02  3.03262115e-01  2.83378214e-01\n",
      "   1.36180837e-02 -3.57840389e-01  1.96402624e-01 -3.57227236e-01]\n",
      " [ 1.23421580e-01 -1.18492253e-01  3.13521624e-01  3.04716319e-01\n",
      "   4.73586470e-03 -3.97006363e-01  1.66721389e-01 -4.21271145e-01]\n",
      " [ 7.35797659e-02 -1.16390057e-01  3.58282804e-01  3.56198370e-01\n",
      "   1.73979234e-02 -3.35443795e-01  1.74628079e-01 -3.22317302e-01]\n",
      " [ 5.79841509e-02 -8.13884512e-02  2.60652095e-01  2.74980009e-01\n",
      "  -4.99233231e-03 -3.44433188e-01  1.55605763e-01 -3.55277717e-01]\n",
      " [ 7.97253400e-02 -1.49720460e-01  2.91555762e-01  2.80651212e-01\n",
      "   1.46945994e-02 -3.56762350e-01  1.62843868e-01 -4.03243124e-01]\n",
      " [ 7.86814094e-02 -1.22785293e-01  2.84198701e-01  2.91687489e-01\n",
      "   7.95528106e-03 -3.63289416e-01  1.66111723e-01 -3.74602348e-01]\n",
      " [ 7.07584247e-02 -7.91101605e-02  2.95868814e-01  2.92853057e-01\n",
      "   3.89708206e-04 -3.55898112e-01  1.42570034e-01 -3.20465922e-01]\n",
      " [ 6.79950342e-02 -9.04980153e-02  2.79372871e-01  2.69829154e-01\n",
      "  -4.24099900e-03 -3.82987320e-01  1.69583037e-01 -4.08795595e-01]\n",
      " [ 6.57427013e-02 -1.24702930e-01  2.59592354e-01  2.43071675e-01\n",
      "   1.33403540e-02 -3.98622632e-01  1.77622616e-01 -3.91145289e-01]\n",
      " [ 9.88517031e-02 -1.00759901e-01  3.06763887e-01  2.91287839e-01\n",
      "   1.92039460e-03 -3.74536037e-01  1.69276088e-01 -3.39110076e-01]\n",
      " [ 1.26074940e-01 -1.12764537e-01  2.01771989e-01  2.55265564e-01\n",
      "   1.68935973e-02 -3.60142469e-01  1.68480411e-01 -3.51928830e-01]\n",
      " [ 1.51222542e-01 -7.22292066e-02  3.11607718e-01  3.36652935e-01\n",
      "  -1.49880573e-02 -3.98981988e-01  1.36396036e-01 -3.86055499e-01]\n",
      " [ 7.60880634e-02 -8.02097544e-02  3.28466922e-01  2.89325297e-01\n",
      "   1.02680279e-02 -3.41268957e-01  1.62761331e-01 -4.01658177e-01]\n",
      " [ 8.86128694e-02 -7.42136464e-02  2.64085621e-01  2.92966783e-01\n",
      "  -1.50332078e-02 -3.65514994e-01  1.56406805e-01 -3.63452971e-01]\n",
      " [ 1.13165170e-01 -1.10320330e-01  3.05433124e-01  2.89578587e-01\n",
      "   1.30217522e-05 -3.78262401e-01  1.95468813e-01 -3.80716324e-01]\n",
      " [ 7.47596323e-02 -9.75539982e-02  2.63478786e-01  2.90958583e-01\n",
      "   9.74684395e-03 -3.88079375e-01  1.71765208e-01 -3.43422085e-01]\n",
      " [ 8.54738653e-02 -1.09470375e-01  2.72268355e-01  2.90396094e-01\n",
      "   2.41829231e-02 -3.83736372e-01  2.04706013e-01 -3.90561581e-01]\n",
      " [ 8.62712711e-02 -1.16925821e-01  3.27016652e-01  2.92752266e-01\n",
      "  -3.27608362e-03 -3.66721183e-01  1.89847350e-01 -3.81528080e-01]\n",
      " [ 1.17151260e-01 -9.13617164e-02  2.91001707e-01  2.78872520e-01\n",
      "  -7.83879496e-03 -3.81139398e-01  2.11505935e-01 -3.39655906e-01]]\n",
      "(44, 8)\n",
      "accuracy: 0.20454545454545456, precision: 0.11157024793388427, recall: 0.20454545454545456, f1: 0.14057239057239057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.07499629 -0.11166995  0.36409518  0.28153592 -0.00221539 -0.33997434\n",
      "   0.1663169  -0.3244757 ]\n",
      " [ 0.09322125 -0.12070661  0.31158453  0.30701923  0.00882862 -0.38154066\n",
      "   0.1519279  -0.42663854]\n",
      " [ 0.1325534  -0.09583221  0.32335347  0.34649727  0.00495267 -0.3874631\n",
      "   0.19432226 -0.34285575]\n",
      " [ 0.12074802 -0.11968702  0.30928326  0.29666996 -0.01724277 -0.37977946\n",
      "   0.14288364 -0.44666895]\n",
      " [ 0.13569875 -0.11531164  0.32235876  0.32299334 -0.02068552 -0.37289158\n",
      "   0.16974598 -0.33974922]\n",
      " [ 0.12516487 -0.12388715  0.2946959   0.37479    -0.05231774 -0.3496982\n",
      "   0.1630181  -0.38113514]\n",
      " [ 0.0853347  -0.12922643  0.29525393  0.31999278 -0.02693705 -0.38164866\n",
      "   0.13532136 -0.3698151 ]\n",
      " [ 0.10718121 -0.16206363  0.21696466  0.30757397  0.0038668  -0.4113068\n",
      "   0.21839413 -0.3628202 ]\n",
      " [ 0.10145164 -0.12505296  0.24813867  0.29879394 -0.01910781 -0.39150238\n",
      "   0.1860824  -0.37799692]\n",
      " [ 0.11832237 -0.11043528  0.3604321   0.31860965  0.00571061 -0.372428\n",
      "   0.15852925 -0.3652965 ]\n",
      " [ 0.12252871 -0.08995329  0.31346726  0.30402058 -0.05610209 -0.37646806\n",
      "   0.18488748 -0.38883808]\n",
      " [ 0.09193294 -0.13054752  0.30128536  0.32266635 -0.01354563 -0.41246402\n",
      "   0.15847299 -0.39853972]\n",
      " [ 0.1375446  -0.12071426  0.32220528  0.29594925 -0.01621704 -0.39382833\n",
      "   0.17100516 -0.41768402]\n",
      " [ 0.15487184 -0.14428273  0.30078965  0.3223483  -0.03595307 -0.41957426\n",
      "   0.21027324 -0.3854233 ]\n",
      " [ 0.08820961 -0.13854608  0.33696628  0.32984334  0.01471847 -0.35484952\n",
      "   0.12450908 -0.37784633]\n",
      " [ 0.14492899 -0.10264827  0.3109439   0.31268382 -0.04408026 -0.3969851\n",
      "   0.13565327 -0.40453628]\n",
      " [ 0.08215749 -0.12796837  0.21641144  0.24641412 -0.01248405 -0.3926808\n",
      "   0.19434164 -0.3535514 ]\n",
      " [ 0.10972412 -0.15536407  0.32501546  0.32169926 -0.02993729 -0.3559615\n",
      "   0.1464632  -0.38741863]\n",
      " [ 0.15939629 -0.12336852  0.33657378  0.33178622 -0.03214774 -0.40231934\n",
      "   0.1553344  -0.4161085 ]\n",
      " [ 0.0919135  -0.1582666   0.27742225  0.3211461  -0.02030682 -0.39169922\n",
      "   0.15893649 -0.42111033]\n",
      " [ 0.12465148 -0.1302157   0.27971458  0.30112997  0.01279618 -0.3850429\n",
      "   0.21759762 -0.38466138]\n",
      " [ 0.1550839  -0.12232207  0.3739085   0.34324563 -0.0300182  -0.3903864\n",
      "   0.1382566  -0.462144  ]\n",
      " [ 0.13552709 -0.1102286   0.27896583  0.31514412 -0.01378246 -0.37367755\n",
      "   0.20266092 -0.3496609 ]\n",
      " [ 0.08444907 -0.11393061  0.26118788  0.28379926  0.03412372 -0.37269256\n",
      "   0.16907129 -0.38258553]\n",
      " [ 0.10258806 -0.13053587  0.2239156   0.28901824  0.03330878 -0.38294086\n",
      "   0.18839397 -0.3487688 ]\n",
      " [ 0.09944989 -0.11151302  0.3207631   0.29661825  0.00202574 -0.3730645\n",
      "   0.19664067 -0.37716097]\n",
      " [ 0.15014502 -0.13244131  0.3312838   0.3206537  -0.00893007 -0.41452998\n",
      "   0.16544108 -0.44204378]\n",
      " [ 0.09802572 -0.13141572  0.3752986   0.3713765   0.00565248 -0.3518405\n",
      "   0.17389372 -0.3412981 ]\n",
      " [ 0.07991105 -0.09383767  0.2766782   0.2897159  -0.01684852 -0.3592036\n",
      "   0.15679364 -0.37419873]\n",
      " [ 0.10487907 -0.16257508  0.30904722  0.2957008   0.00217012 -0.37368664\n",
      "   0.16322094 -0.42424878]\n",
      " [ 0.10223346 -0.13616188  0.30288744  0.30545774 -0.00411892 -0.38161135\n",
      "   0.16675924 -0.39442906]\n",
      " [ 0.0923356  -0.09076181  0.31216925  0.30776754 -0.0114896  -0.3713995\n",
      "   0.14355658 -0.33935907]\n",
      " [ 0.09254277 -0.10302906  0.2972889   0.28444892 -0.01719267 -0.399675\n",
      "   0.17160922 -0.43043992]\n",
      " [ 0.0904833  -0.13863277  0.2750869   0.25620398  0.00317368 -0.41596466\n",
      "   0.1772212  -0.41216987]\n",
      " [ 0.12270223 -0.11185249  0.3210352   0.30698973 -0.00678636 -0.39034727\n",
      "   0.17020279 -0.3595606 ]\n",
      " [ 0.14934286 -0.12493907  0.21688524  0.26895878  0.00458309 -0.37516394\n",
      "   0.16839013 -0.3691717 ]\n",
      " [ 0.17829297 -0.08515759  0.32951894  0.35293835 -0.0266071  -0.41677564\n",
      "   0.13472444 -0.407566  ]\n",
      " [ 0.09933626 -0.09289446  0.34549323  0.3041682  -0.00098979 -0.35758877\n",
      "   0.16404073 -0.4223743 ]\n",
      " [ 0.11072855 -0.08710635  0.2806197   0.30689815 -0.02646166 -0.3807255\n",
      "   0.15803802 -0.3829737 ]\n",
      " [ 0.13916728 -0.12479887  0.3208866   0.30405253 -0.01170248 -0.39478993\n",
      "   0.1972324  -0.40135938]\n",
      " [ 0.09842122 -0.11061686  0.27994123  0.30527595 -0.0031468  -0.40310073\n",
      "   0.17292248 -0.36278486]\n",
      " [ 0.10980334 -0.12341224  0.28875166  0.3052233   0.01230009 -0.39939088\n",
      "   0.20658286 -0.41069463]\n",
      " [ 0.1083349  -0.12997791  0.3435763   0.30778185 -0.01467095 -0.38295072\n",
      "   0.1908959  -0.40249816]\n",
      " [ 0.14310943 -0.10422161  0.30730894  0.29311132 -0.01873544 -0.3981664\n",
      "   0.21217687 -0.3602869 ]]\n",
      "(44, 8)\n",
      "accuracy: 0.20454545454545456, precision: 0.11157024793388427, recall: 0.20454545454545456, f1: 0.14057239057239057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.44075659e-02 -1.11148663e-01  3.76468450e-01  2.92311519e-01\n",
      "  -1.39776431e-02 -3.54752421e-01  1.63927138e-01 -3.43730599e-01]\n",
      " [ 1.12845987e-01 -1.20534413e-01  3.24195713e-01  3.18637729e-01\n",
      "  -2.89307535e-03 -3.97455454e-01  1.49773225e-01 -4.46047217e-01]\n",
      " [ 1.52759641e-01 -9.49786156e-02  3.35168540e-01  3.58135134e-01\n",
      "  -5.93894720e-03 -4.03750420e-01  1.92901030e-01 -3.61991227e-01]\n",
      " [ 1.42264530e-01 -1.19356133e-01  3.22095454e-01  3.08485806e-01\n",
      "  -2.97156069e-02 -3.95547152e-01  1.40913919e-01 -4.65903312e-01]\n",
      " [ 1.55937850e-01 -1.14277877e-01  3.34805846e-01  3.34149718e-01\n",
      "  -3.30209322e-02 -3.88836086e-01  1.68015972e-01 -3.58213454e-01]\n",
      " [ 1.46110862e-01 -1.22849181e-01  3.07590604e-01  3.86740863e-01\n",
      "  -6.41849935e-02 -3.66414845e-01  1.61570102e-01 -3.98901165e-01]\n",
      " [ 1.04778908e-01 -1.27857879e-01  3.08316827e-01  3.30986619e-01\n",
      "  -3.91276106e-02 -3.95816803e-01  1.32788762e-01 -3.89166355e-01]\n",
      " [ 1.27830684e-01 -1.61881566e-01  2.29058310e-01  3.18735272e-01\n",
      "  -7.68201053e-03 -4.25662071e-01  2.17415214e-01 -3.80757183e-01]\n",
      " [ 1.21061690e-01 -1.25307128e-01  2.60142684e-01  3.07892144e-01\n",
      "  -3.13557535e-02 -4.06307817e-01  1.84373662e-01 -3.95872921e-01]\n",
      " [ 1.40109986e-01 -1.09126009e-01  3.73114824e-01  3.29596490e-01\n",
      "  -5.16580604e-03 -3.89011741e-01  1.56706303e-01 -3.85903120e-01]\n",
      " [ 1.44137412e-01 -8.86849612e-02  3.27557176e-01  3.14705491e-01\n",
      "  -7.05068260e-02 -3.93541068e-01  1.81674153e-01 -4.07853842e-01]\n",
      " [ 1.13766372e-01 -1.28752857e-01  3.15225363e-01  3.33767146e-01\n",
      "  -2.63733976e-02 -4.29209709e-01  1.55175820e-01 -4.19160992e-01]\n",
      " [ 1.59895480e-01 -1.20273300e-01  3.35291505e-01  3.08282495e-01\n",
      "  -2.95382738e-02 -4.08983976e-01  1.69629648e-01 -4.36600089e-01]\n",
      " [ 1.77005634e-01 -1.45397812e-01  3.13920498e-01  3.34264815e-01\n",
      "  -5.00587560e-02 -4.34541941e-01  2.08396241e-01 -4.03594136e-01]\n",
      " [ 1.08408421e-01 -1.37503475e-01  3.49959612e-01  3.40162277e-01\n",
      "   2.43210047e-03 -3.69822085e-01  1.21748693e-01 -3.98021668e-01]\n",
      " [ 1.66126132e-01 -1.01505220e-01  3.24017763e-01  3.23443651e-01\n",
      "  -5.68689778e-02 -4.13359642e-01  1.32185102e-01 -4.24129128e-01]\n",
      " [ 1.01692140e-01 -1.28429368e-01  2.28933513e-01  2.56348521e-01\n",
      "  -2.45080777e-02 -4.07053351e-01  1.94026351e-01 -3.71581912e-01]\n",
      " [ 1.30156532e-01 -1.55406907e-01  3.39516640e-01  3.32842499e-01\n",
      "  -4.31881808e-02 -3.72390658e-01  1.43936425e-01 -4.04995739e-01]\n",
      " [ 1.82170942e-01 -1.22974515e-01  3.49143714e-01  3.44258219e-01\n",
      "  -4.53917310e-02 -4.18150902e-01  1.52859792e-01 -4.35125470e-01]\n",
      " [ 1.13462150e-01 -1.58463389e-01  2.91845351e-01  3.32558215e-01\n",
      "  -3.39744538e-02 -4.07476366e-01  1.57045439e-01 -4.40389633e-01]\n",
      " [ 1.44423366e-01 -1.31102115e-01  2.90974677e-01  3.10933977e-01\n",
      "   2.98174098e-04 -3.99416596e-01  2.17297748e-01 -4.03232217e-01]\n",
      " [ 1.77092642e-01 -1.21481046e-01  3.87979776e-01  3.54825795e-01\n",
      "  -4.35341410e-02 -4.08095360e-01  1.34719938e-01 -4.82344329e-01]\n",
      " [ 1.55708298e-01 -1.10071808e-01  2.91374981e-01  3.24279666e-01\n",
      "  -2.68899575e-02 -3.86935800e-01  2.01066524e-01 -3.67960393e-01]\n",
      " [ 1.03578620e-01 -1.13381900e-01  2.72935927e-01  2.92898387e-01\n",
      "   2.38167886e-02 -3.87130767e-01  1.68281898e-01 -4.01637495e-01]\n",
      " [ 1.21781774e-01 -1.29311770e-01  2.34321862e-01  2.98599958e-01\n",
      "   2.18107626e-02 -3.97222340e-01  1.86816335e-01 -3.66231799e-01]\n",
      " [ 1.18783101e-01 -1.10797301e-01  3.34079295e-01  3.06493819e-01\n",
      "  -9.99910757e-03 -3.87295872e-01  1.93993479e-01 -3.96330595e-01]\n",
      " [ 1.73419043e-01 -1.31475538e-01  3.44713151e-01  3.32379878e-01\n",
      "  -2.31326893e-02 -4.31315154e-01  1.61213562e-01 -4.62323189e-01]\n",
      " [ 1.18601777e-01 -1.33178353e-01  3.88246477e-01  3.82901609e-01\n",
      "  -6.52529299e-03 -3.67096275e-01  1.70645699e-01 -3.59342963e-01]\n",
      " [ 9.84209627e-02 -9.36037153e-02  2.88759947e-01  3.00989121e-01\n",
      "  -2.92644016e-02 -3.73139381e-01  1.54770389e-01 -3.92470151e-01]\n",
      " [ 1.26688525e-01 -1.62101880e-01  3.22682977e-01  3.07190925e-01\n",
      "  -1.10152699e-02 -3.89413893e-01  1.60924047e-01 -4.44169581e-01]\n",
      " [ 1.22074142e-01 -1.35900110e-01  3.17327797e-01  3.15297008e-01\n",
      "  -1.68416873e-02 -3.98169994e-01  1.64494529e-01 -4.13503349e-01]\n",
      " [ 1.10837072e-01 -9.00915787e-02  3.24563265e-01  3.18984509e-01\n",
      "  -2.35492066e-02 -3.85797441e-01  1.42187670e-01 -3.57376307e-01]\n",
      " [ 1.13471143e-01 -1.01431809e-01  3.10830712e-01  2.95135200e-01\n",
      "  -3.02361771e-02 -4.15123791e-01  1.70827374e-01 -4.50767606e-01]\n",
      " [ 1.11749545e-01 -1.38569146e-01  2.86813080e-01  2.66003400e-01\n",
      "  -7.84312561e-03 -4.32132542e-01  1.74420491e-01 -4.32350755e-01]\n",
      " [ 1.43404633e-01 -1.09702185e-01  3.31362665e-01  3.18220437e-01\n",
      "  -1.62603799e-02 -4.05545950e-01  1.68387637e-01 -3.79717231e-01]\n",
      " [ 1.69443473e-01 -1.24656543e-01  2.28590250e-01  2.79288322e-01\n",
      "  -7.97422603e-03 -3.89016300e-01  1.65874675e-01 -3.85843366e-01]\n",
      " [ 2.01253355e-01 -8.35590959e-02  3.43235344e-01  3.64619672e-01\n",
      "  -3.90001722e-02 -4.33386981e-01  1.30371928e-01 -4.27933693e-01]\n",
      " [ 1.19351283e-01 -9.22151506e-02  3.57888222e-01  3.14956814e-01\n",
      "  -1.21374670e-02 -3.72834474e-01  1.62449315e-01 -4.42044616e-01]\n",
      " [ 1.29329517e-01 -8.59195739e-02  2.93183863e-01  3.17170709e-01\n",
      "  -3.84754576e-02 -3.94664526e-01  1.56858265e-01 -4.02029067e-01]\n",
      " [ 1.61474437e-01 -1.25564665e-01  3.32349390e-01  3.14417720e-01\n",
      "  -2.34139562e-02 -4.10312861e-01  1.96110472e-01 -4.21021879e-01]\n",
      " [ 1.18450083e-01 -1.10375553e-01  2.92443722e-01  3.16285551e-01\n",
      "  -1.64788701e-02 -4.17060673e-01  1.71362132e-01 -3.81129265e-01]\n",
      " [ 1.30706742e-01 -1.23305336e-01  3.01024050e-01  3.16606402e-01\n",
      "  -3.54586169e-04 -4.14201200e-01  2.05792278e-01 -4.29905176e-01]\n",
      " [ 1.27039358e-01 -1.29797116e-01  3.56130123e-01  3.19135994e-01\n",
      "  -2.64387503e-02 -3.98417234e-01  1.88973784e-01 -4.22536492e-01]\n",
      " [ 1.64425582e-01 -1.01954870e-01  3.19093645e-01  3.03906739e-01\n",
      "  -3.03399153e-02 -4.14044976e-01  2.09377423e-01 -3.80177289e-01]]\n",
      "(44, 8)\n",
      "accuracy: 0.22727272727272727, precision: 0.1186602870813397, recall: 0.22727272727272727, f1: 0.14600550964187328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1142029  -0.112532    0.39045805  0.30103123 -0.02610958 -0.3684249\n",
      "   0.1615665  -0.36219805]\n",
      " [ 0.13340147 -0.12235773  0.33880803  0.32808405 -0.01503944 -0.41244173\n",
      "   0.14754707 -0.46524456]\n",
      " [ 0.17401347 -0.09584987  0.3485597   0.36797583 -0.01751913 -0.41896325\n",
      "   0.19148831 -0.38095838]\n",
      " [ 0.16469873 -0.12090849  0.33687687  0.31780043 -0.04225421 -0.4104855\n",
      "   0.13862967 -0.4845584 ]\n",
      " [ 0.1771927  -0.11487778  0.3491007   0.34286553 -0.0461405  -0.40408057\n",
      "   0.1667565  -0.37624165]\n",
      " [ 0.16784576 -0.12360421  0.32230547  0.39682454 -0.07677814 -0.38221115\n",
      "   0.16012236 -0.41657916]\n",
      " [ 0.12506819 -0.12814683  0.32326296  0.34017915 -0.05135081 -0.40963092\n",
      "   0.12970999 -0.4081995 ]\n",
      " [ 0.14912196 -0.1634126   0.24306735  0.32796538 -0.02005508 -0.43911114\n",
      "   0.21650161 -0.39805472]\n",
      " [ 0.14169154 -0.12716815  0.27398396  0.31511712 -0.04434883 -0.41995168\n",
      "   0.1824665  -0.41334182]\n",
      " [ 0.16274169 -0.10965482  0.38759506  0.33897102 -0.01673116 -0.40464813\n",
      "   0.15480304 -0.40683353]\n",
      " [ 0.16652128 -0.08947483  0.34347844  0.3223978  -0.08480695 -0.40993467\n",
      "   0.17854777 -0.42651558]\n",
      " [ 0.1364149  -0.12861112  0.3314576   0.34267035 -0.04035575 -0.44455057\n",
      "   0.1519226  -0.43865612]\n",
      " [ 0.18306443 -0.12146199  0.34996465  0.31862003 -0.0431616  -0.42321396\n",
      "   0.16824588 -0.45506978]\n",
      " [ 0.19997028 -0.14843333  0.32936037  0.34395158 -0.06482686 -0.448606\n",
      "   0.20660147 -0.4213582 ]\n",
      " [ 0.12966125 -0.13824648  0.36501473  0.3487602  -0.01047663 -0.38375038\n",
      "   0.11895415 -0.41783056]\n",
      " [ 0.18831773 -0.10239778  0.3391677   0.33221126 -0.07006223 -0.42885864\n",
      "   0.1285975  -0.44328746]\n",
      " [ 0.12226819 -0.13042873  0.24330747  0.26438013 -0.037149   -0.4202281\n",
      "   0.19362831 -0.38922173]\n",
      " [ 0.15190534 -0.15694562  0.35624635  0.34157625 -0.05700325 -0.38758224\n",
      "   0.14137475 -0.42207816]\n",
      " [ 0.20550767 -0.12457542  0.36372578  0.35500962 -0.05956502 -0.43301398\n",
      "   0.15018809 -0.4539637 ]\n",
      " [ 0.1354805  -0.1604849   0.30825725  0.3411843  -0.04804474 -0.42277527\n",
      "   0.15537119 -0.45897523]\n",
      " [ 0.16504058 -0.13406736  0.30426317  0.3188662  -0.01284097 -0.4125507\n",
      "   0.21686172 -0.4213713 ]\n",
      " [ 0.20007674 -0.12249921  0.4041481   0.36448768 -0.05799289 -0.42462796\n",
      "   0.13080832 -0.5021369 ]\n",
      " [ 0.17600134 -0.11194853  0.3058715   0.3318246  -0.04070369 -0.39916277\n",
      "   0.19947994 -0.385773  ]\n",
      " [ 0.12340595 -0.11439057  0.2864425   0.29976034  0.01312234 -0.40057087\n",
      "   0.16768657 -0.42045033]\n",
      " [ 0.14178376 -0.12974659  0.24651036  0.30622214  0.00964959 -0.41039947\n",
      "   0.18553299 -0.38335672]\n",
      " [ 0.13950737 -0.11143753  0.34942883  0.31404823 -0.02269849 -0.4010175\n",
      "   0.19175915 -0.41490734]\n",
      " [ 0.1986063  -0.13226411  0.36063403  0.34177655 -0.03773795 -0.4473649\n",
      "   0.15640488 -0.48248792]\n",
      " [ 0.13968192 -0.13667694  0.4030506   0.39205503 -0.01900332 -0.38148862\n",
      "   0.16695595 -0.37683454]\n",
      " [ 0.11817164 -0.09517312  0.30278695  0.31030846 -0.04218068 -0.3865617\n",
      "   0.15291652 -0.41082197]\n",
      " [ 0.1495393  -0.16370416  0.33836883  0.31654257 -0.02468564 -0.40379116\n",
      "   0.15869491 -0.46386746]\n",
      " [ 0.14284103 -0.13744962  0.33363634  0.32308397 -0.03035344 -0.41325498\n",
      "   0.16208616 -0.43193817]\n",
      " [ 0.1306622  -0.09108817  0.33873302  0.32807553 -0.03600392 -0.39952415\n",
      "   0.14122343 -0.37529397]\n",
      " [ 0.13511771 -0.10218763  0.3259987   0.3033795  -0.04367197 -0.42964923\n",
      "   0.1706371  -0.47083753]\n",
      " [ 0.13437428 -0.14039303  0.3007089   0.27360615 -0.01957596 -0.4471954\n",
      "   0.17176534 -0.45224956]\n",
      " [ 0.16482916 -0.10948338  0.34321636  0.32801986 -0.02612025 -0.4193874\n",
      "   0.16647775 -0.3996874 ]\n",
      " [ 0.19042334 -0.12582254  0.24227804  0.28755862 -0.02116631 -0.40205872\n",
      "   0.16362345 -0.40238962]\n",
      " [ 0.22499277 -0.08396037  0.35902146  0.37466976 -0.05195643 -0.44836807\n",
      "   0.12555368 -0.44797212]\n",
      " [ 0.14031348 -0.0931708   0.3721215   0.3236689  -0.02374976 -0.3871739\n",
      "   0.16083463 -0.46132416]\n",
      " [ 0.1486969  -0.08661506  0.3080497   0.3253909  -0.05092537 -0.40800017\n",
      "   0.1558526  -0.4210995 ]\n",
      " [ 0.18545367 -0.12777391  0.345393    0.32257292 -0.03588706 -0.42520118\n",
      "   0.19464618 -0.44051623]\n",
      " [ 0.13926406 -0.11191094  0.30690572  0.32518756 -0.03028713 -0.43004465\n",
      "   0.17026664 -0.39898258]\n",
      " [ 0.15235631 -0.12514731  0.31495765  0.32595456 -0.0137214  -0.4278571\n",
      "   0.20537195 -0.44866845]\n",
      " [ 0.14712314 -0.13118616  0.37074792  0.32844627 -0.03870095 -0.41298062\n",
      "   0.18727773 -0.44238606]\n",
      " [ 0.18638402 -0.10168684  0.33225647  0.31225452 -0.04245955 -0.4288532\n",
      "   0.20680436 -0.3996152 ]]\n",
      "(44, 8)\n",
      "accuracy: 0.22727272727272727, precision: 0.1186602870813397, recall: 0.22727272727272727, f1: 0.14600550964187328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.12989926 -0.11455944  0.40196976  0.31764126 -0.03735325 -0.38203472\n",
      "   0.15859608 -0.37830454]\n",
      " [ 0.15044    -0.1254076   0.35055935  0.34646955 -0.02684665 -0.4274203\n",
      "   0.14476955 -0.4817218 ]\n",
      " [ 0.1912888  -0.09791174  0.35931075  0.38593188 -0.02837535 -0.4341695\n",
      "   0.18900844 -0.3976578 ]\n",
      " [ 0.1827335  -0.12349804  0.3483393   0.3355044  -0.0542955  -0.425529\n",
      "   0.13588084 -0.5006645 ]\n",
      " [ 0.1941638  -0.11661668  0.36079612  0.3594078  -0.05859518 -0.4192931\n",
      "   0.16480716 -0.39188427]\n",
      " [ 0.18549168 -0.12562165  0.33431065  0.41486728 -0.0889423  -0.39811474\n",
      "   0.15801138 -0.4318853 ]\n",
      " [ 0.14156328 -0.1295017   0.3354383   0.35637835 -0.06274357 -0.42341405\n",
      "   0.12593858 -0.4246909 ]\n",
      " [ 0.16627294 -0.16589214  0.25448087  0.34387875 -0.03196765 -0.45206156\n",
      "   0.2143681  -0.41321218]\n",
      " [ 0.15848444 -0.13000211  0.2850898   0.3302502  -0.05679934 -0.43318287\n",
      "   0.17996529 -0.42882237]\n",
      " [ 0.1806633  -0.11148345  0.39908382  0.35647658 -0.02762647 -0.41963983\n",
      "   0.15194559 -0.42517534]\n",
      " [ 0.18427049 -0.09150286  0.35585296  0.33878905 -0.09806052 -0.42626286\n",
      "   0.17545709 -0.44278404]\n",
      " [ 0.15465467 -0.12986493  0.34489524  0.35975444 -0.05382996 -0.45925596\n",
      "   0.14790635 -0.45562547]\n",
      " [ 0.20160097 -0.1236371   0.36183685  0.33705133 -0.0557749  -0.43766788\n",
      "   0.16602978 -0.47116798]\n",
      " [ 0.2187889  -0.15211126  0.34196883  0.36178386 -0.07889335 -0.46267277\n",
      "   0.20400453 -0.43676403]\n",
      " [ 0.14718623 -0.140127    0.3776108   0.36542633 -0.02282833 -0.39769113\n",
      "   0.11536343 -0.43482167]\n",
      " [ 0.20656443 -0.10460836  0.3518675   0.3491654  -0.08290081 -0.44426334\n",
      "   0.12446072 -0.46012676]\n",
      " [ 0.13923946 -0.13340989  0.25482446  0.27958485 -0.04937802 -0.43324217\n",
      "   0.19207507 -0.40446422]\n",
      " [ 0.16978222 -0.15921265  0.3698787   0.3590793  -0.0700799  -0.40283012\n",
      "   0.1381766  -0.4372265 ]\n",
      " [ 0.22448546 -0.127132    0.37574875  0.3741299  -0.07326639 -0.4478371\n",
      "   0.146754   -0.47042885]\n",
      " [ 0.15315264 -0.16319574  0.32136363  0.3581317  -0.06123537 -0.43762857\n",
      "   0.15253116 -0.47546786]\n",
      " [ 0.18160129 -0.13759202  0.31537658  0.33514744 -0.02542618 -0.425242\n",
      "   0.21521088 -0.43777102]\n",
      " [ 0.21861376 -0.12501791  0.41734114  0.3836042  -0.07157376 -0.44073087\n",
      "   0.12621984 -0.5195954 ]\n",
      " [ 0.19237301 -0.11494511  0.31756157  0.34712285 -0.05342521 -0.41127294\n",
      "   0.1972843  -0.40134498]\n",
      " [ 0.13965763 -0.11602638  0.2974118   0.3141885   0.00262878 -0.41418523\n",
      "   0.16604    -0.43688938]\n",
      " [ 0.15854359 -0.13052383  0.25648162  0.32069203 -0.00187215 -0.42362124\n",
      "   0.18318602 -0.39863074]\n",
      " [ 0.15673815 -0.11294375  0.3614847   0.32944775 -0.03463138 -0.41485134\n",
      "   0.18889886 -0.4309684 ]\n",
      " [ 0.21866283 -0.13405354  0.37310857  0.36047265 -0.05147223 -0.46343374\n",
      "   0.15098415 -0.50030386]\n",
      " [ 0.15657407 -0.14094599  0.41506016  0.40940496 -0.03074141 -0.39548424\n",
      "   0.16258697 -0.39222732]\n",
      " [ 0.13406993 -0.09744106  0.31428558  0.32716405 -0.0544853  -0.40016794\n",
      "   0.15005544 -0.42666286]\n",
      " [ 0.1681498  -0.1663099   0.35089937  0.3338485  -0.03760849 -0.4178108\n",
      "   0.15533718 -0.48086834]\n",
      " [ 0.15967488 -0.13974524  0.3470506   0.33862394 -0.0430128  -0.428061\n",
      "   0.15903622 -0.44834316]\n",
      " [ 0.1465774  -0.09315405  0.3502972   0.34467173 -0.04787919 -0.41311112\n",
      "   0.13936415 -0.39120075]\n",
      " [ 0.1522303  -0.10411354  0.33837038  0.3198329  -0.05620392 -0.44409537\n",
      "   0.16927053 -0.48849744]\n",
      " [ 0.1532247  -0.14274824  0.31256866  0.2892839  -0.03140102 -0.46193832\n",
      "   0.16845436 -0.46944994]\n",
      " [ 0.18219912 -0.11067702  0.35298955  0.34480774 -0.03552925 -0.4328595\n",
      "   0.16369201 -0.41702428]\n",
      " [ 0.2078383  -0.1278675   0.25314704  0.30311352 -0.0335212  -0.41521016\n",
      "   0.16047035 -0.41708615]\n",
      " [ 0.2445111  -0.08577871  0.37198108  0.39295292 -0.06418985 -0.46328008\n",
      "   0.12014882 -0.4652228 ]\n",
      " [ 0.15699877 -0.09527635  0.3835255   0.34032935 -0.03457492 -0.40125912\n",
      "   0.15842004 -0.4781201 ]\n",
      " [ 0.16405639 -0.08794256  0.32026008  0.34130797 -0.06261326 -0.42170683\n",
      "   0.15396896 -0.43758637]\n",
      " [ 0.20503096 -0.13096172  0.3561269   0.33942327 -0.0478959  -0.43987244\n",
      "   0.19232914 -0.45752585]\n",
      " [ 0.15596682 -0.11432016  0.31835184  0.34124625 -0.04299575 -0.44299465\n",
      "   0.16851012 -0.41458187]\n",
      " [ 0.16999003 -0.12760572  0.32627     0.3429919  -0.02631791 -0.44130152\n",
      "   0.20387308 -0.465136  ]\n",
      " [ 0.16348489 -0.1336627   0.38262513  0.34600765 -0.0502225  -0.42789844\n",
      "   0.18481798 -0.46000332]\n",
      " [ 0.20453244 -0.10241842  0.34324265  0.32860127 -0.05413611 -0.44395697\n",
      "   0.20341319 -0.41685346]]\n",
      "(44, 8)\n",
      "accuracy: 0.22727272727272727, precision: 0.1186602870813397, recall: 0.22727272727272727, f1: 0.14600550964187328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.14586958 -0.11805348  0.41531557  0.33215988 -0.04944798 -0.3937784\n",
      "   0.15523098 -0.39451975]\n",
      " [ 0.16781434 -0.12944607  0.36451313  0.3628558  -0.03980876 -0.44073197\n",
      "   0.14169772 -0.4985282 ]\n",
      " [ 0.20893814 -0.10131364  0.3723322   0.40230054 -0.04038761 -0.447434\n",
      "   0.18651223 -0.4144318 ]\n",
      " [ 0.20096299 -0.12713276  0.36223087  0.35139918 -0.06756404 -0.43899328\n",
      "   0.13309921 -0.51690197]\n",
      " [ 0.2115447  -0.12027229  0.37479135  0.37408718 -0.07272825 -0.43274385\n",
      "   0.16271992 -0.40814894]\n",
      " [ 0.20356265 -0.1289644   0.3483945   0.43110538 -0.10266415 -0.4125116\n",
      "   0.15554094 -0.4477072 ]\n",
      " [ 0.15790936 -0.13240254  0.34933704  0.37054113 -0.0753291  -0.4354167\n",
      "   0.12229703 -0.4411427 ]\n",
      " [ 0.18331987 -0.16971198  0.26792687  0.35780764 -0.04520351 -0.46354604\n",
      "   0.21201852 -0.42888987]\n",
      " [ 0.1756982  -0.13423625  0.29835868  0.34386593 -0.07055531 -0.4450639\n",
      "   0.17718808 -0.444214  ]\n",
      " [ 0.19874531 -0.11454386  0.41318285  0.37176555 -0.03999527 -0.43275842\n",
      "   0.14879756 -0.44380692]\n",
      " [ 0.20222494 -0.09489985  0.37080756  0.35345402 -0.11271685 -0.44075796\n",
      "   0.17195249 -0.45871782]\n",
      " [ 0.17322697 -0.1323909   0.36075443  0.37488765 -0.06889223 -0.4717673\n",
      "   0.14392935 -0.472423  ]\n",
      " [ 0.22039285 -0.1270169   0.37603116  0.35345033 -0.06967832 -0.45072126\n",
      "   0.16378997 -0.48757356]\n",
      " [ 0.23781744 -0.15700832  0.35680595  0.37788013 -0.09412314 -0.47528\n",
      "   0.20114866 -0.4522689 ]\n",
      " [ 0.16497016 -0.14391369  0.39273077  0.37992486 -0.036801   -0.40972003\n",
      "   0.1118314  -0.45216656]\n",
      " [ 0.22484648 -0.10839808  0.36714858  0.36411545 -0.09697406 -0.45842707\n",
      "   0.12033877 -0.4770465 ]\n",
      " [ 0.1564934  -0.13740203  0.26809713  0.29335177 -0.0624776  -0.44517875\n",
      "   0.1902894  -0.41986737]\n",
      " [ 0.18747371 -0.16303083  0.38552725  0.374368   -0.08450204 -0.41656184\n",
      "   0.13485761 -0.45244533]\n",
      " [ 0.24355164 -0.13104895  0.39033318  0.39103112 -0.08825953 -0.46114084\n",
      "   0.14341474 -0.48712787]\n",
      " [ 0.17051728 -0.16733754  0.3368077   0.3729009  -0.07552452 -0.4511751\n",
      "   0.1494915  -0.49202442]\n",
      " [ 0.19851223 -0.14209819  0.3287655   0.34960988 -0.03923254 -0.43643522\n",
      "   0.21369794 -0.45425424]\n",
      " [ 0.2374522  -0.12890159  0.43351442  0.4007656  -0.08678471 -0.45537204\n",
      "   0.12169397 -0.5366829 ]\n",
      " [ 0.20875329 -0.11954072  0.33131897  0.36063442 -0.06746344 -0.42196745\n",
      "   0.19480574 -0.41688022]\n",
      " [ 0.15624417 -0.11885419  0.31022382  0.32741526 -0.00868964 -0.4264695\n",
      "   0.16442013 -0.45395786]\n",
      " [ 0.17545336 -0.13253757  0.26821125  0.3332675  -0.01432681 -0.4353216\n",
      "   0.18090248 -0.41394675]\n",
      " [ 0.17389013 -0.11564472  0.37569493  0.34341246 -0.04751566 -0.42720902\n",
      "   0.18558839 -0.44731092]\n",
      " [ 0.23858136 -0.13723636  0.3879674   0.37709013 -0.06645702 -0.47774577\n",
      "   0.14539939 -0.5180034 ]\n",
      " [ 0.17304564 -0.14639261  0.42923778  0.42485565 -0.04342294 -0.40752822\n",
      "   0.15803464 -0.40792623]\n",
      " [ 0.15002707 -0.10086708  0.32797247  0.34244445 -0.06783176 -0.41284016\n",
      "   0.14695925 -0.44265392]\n",
      " [ 0.1867637  -0.17023224  0.36567533  0.34933466 -0.05187471 -0.4305949\n",
      "   0.15166283 -0.4979256 ]\n",
      " [ 0.1766085  -0.14332683  0.36320448  0.3522471  -0.05725066 -0.4414416\n",
      "   0.15555638 -0.46491525]\n",
      " [ 0.16237654 -0.09636571  0.3639696   0.359249   -0.06062459 -0.42558277\n",
      "   0.1374735  -0.40754098]\n",
      " [ 0.16904065 -0.10713588  0.35295165  0.33448863 -0.06940145 -0.45728546\n",
      "   0.16768059 -0.5063772 ]\n",
      " [ 0.17215896 -0.14608157  0.32656837  0.3034741  -0.04437118 -0.4752814\n",
      "   0.1651118  -0.48667657]\n",
      " [ 0.19968961 -0.11311777  0.36449617  0.35989183 -0.04575351 -0.44434863\n",
      "   0.16061313 -0.43465337]\n",
      " [ 0.22546859 -0.13121219  0.26612896  0.3170321  -0.04680729 -0.42745596\n",
      "   0.15715848 -0.43188515]\n",
      " [ 0.2639317  -0.08903005  0.38702595  0.40925953 -0.07780166 -0.4766323\n",
      "   0.11431397 -0.48287293]\n",
      " [ 0.17371148 -0.09888406  0.39687797  0.35532898 -0.04667448 -0.41393748\n",
      "   0.1559217  -0.49538898]\n",
      " [ 0.17969052 -0.09081565  0.3349076   0.35576296 -0.07572083 -0.43405148\n",
      "   0.15183708 -0.45380008]\n",
      " [ 0.22480413 -0.13545705  0.36894894  0.354218   -0.06088565 -0.45271373\n",
      "   0.18962345 -0.47502658]\n",
      " [ 0.17268535 -0.1177578   0.3318731   0.35563484 -0.05669094 -0.4544476\n",
      "   0.16675876 -0.43026698]\n",
      " [ 0.18760711 -0.13125327  0.3400459   0.35798365 -0.04010268 -0.4528483\n",
      "   0.20218424 -0.48216343]\n",
      " [ 0.18015435 -0.13790318  0.39683574  0.36143556 -0.0629351  -0.44105905\n",
      "   0.1822411  -0.47760826]\n",
      " [ 0.22323734 -0.10468613  0.35634744  0.34306157 -0.06637494 -0.45780098\n",
      "   0.19972876 -0.43500614]]\n",
      "(44, 8)\n",
      "accuracy: 0.22727272727272727, precision: 0.1143939393939394, recall: 0.22727272727272727, f1: 0.14364742762308683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1571669  -0.12309908  0.42519957  0.34303176 -0.05071744 -0.40513095\n",
      "   0.15102682 -0.41021633]\n",
      " [ 0.18024707 -0.13489366  0.37482888  0.3753653  -0.04075402 -0.45375103\n",
      "   0.13716446 -0.5155712 ]\n",
      " [ 0.22168106 -0.10642496  0.38160613  0.41458827 -0.04082709 -0.4599381\n",
      "   0.18266498 -0.43138438]\n",
      " [ 0.21491241 -0.1326639   0.37245157  0.36333957 -0.06964923 -0.45210433\n",
      "   0.12907058 -0.5328825 ]\n",
      " [ 0.2244261  -0.12520777  0.38534307  0.38527107 -0.07490876 -0.44551054\n",
      "   0.15927437 -0.42454243]\n",
      " [ 0.21700561 -0.13401623  0.35895875  0.4434224  -0.10495431 -0.42624328\n",
      "   0.15133038 -0.46349517]\n",
      " [ 0.16991447 -0.13652405  0.35951456  0.38097388 -0.0769722  -0.44702628\n",
      "   0.117555   -0.45742658]\n",
      " [ 0.19621195 -0.17497252  0.27804583  0.36842638 -0.04783891 -0.4751957\n",
      "   0.20885095 -0.4442712 ]\n",
      " [ 0.18914984 -0.13980253  0.30793783  0.35416615 -0.07376457 -0.45663518\n",
      "   0.17363554 -0.45948768]\n",
      " [ 0.21207386 -0.11897133  0.42344922  0.38337767 -0.04154857 -0.44527516\n",
      "   0.14437231 -0.46142805]\n",
      " [ 0.21588168 -0.10005262  0.38225707  0.36476356 -0.11620797 -0.45449683\n",
      "   0.16753909 -0.47489032]\n",
      " [ 0.1868209  -0.13711321  0.37259287  0.3863042  -0.07156306 -0.484518\n",
      "   0.13906503 -0.4888763 ]\n",
      " [ 0.23462225 -0.13263793  0.3863431   0.366      -0.07203763 -0.46386337\n",
      "   0.16035552 -0.5036029 ]\n",
      " [ 0.25197852 -0.16329181  0.36766845  0.38955325 -0.09815468 -0.4879551\n",
      "   0.19711436 -0.467669  ]\n",
      " [ 0.17752634 -0.14944424  0.403524    0.39065307 -0.03870572 -0.42130172\n",
      "   0.10763123 -0.46902144]\n",
      " [ 0.23851237 -0.11348891  0.3783154   0.3759428  -0.09977315 -0.47209638\n",
      "   0.11545926 -0.49375942]\n",
      " [ 0.16968665 -0.1427083   0.27791783  0.30395168 -0.06581157 -0.45711187\n",
      "   0.18742111 -0.43480512]\n",
      " [ 0.20094329 -0.16832362  0.39713886  0.38589966 -0.08727576 -0.43027535\n",
      "   0.1304793  -0.46769524]\n",
      " [ 0.25778627 -0.13673785  0.40076265  0.40407097 -0.09131999 -0.47450766\n",
      "   0.138982   -0.5036098 ]\n",
      " [ 0.18391073 -0.17253771  0.3486927   0.38405746 -0.07866998 -0.46486732\n",
      "   0.145694   -0.50841177]\n",
      " [ 0.21108666 -0.14834487  0.33867365  0.36067018 -0.04146569 -0.44758287\n",
      "   0.21072787 -0.47064313]\n",
      " [ 0.2512283  -0.13452862  0.44528562  0.41417468 -0.08979207 -0.46969804\n",
      "   0.11633927 -0.55345386]\n",
      " [ 0.22068988 -0.12571603  0.34167677  0.3708049  -0.07026593 -0.43300366\n",
      "   0.19128662 -0.43218046]\n",
      " [ 0.16889238 -0.12309662  0.31958908  0.33747813 -0.00961784 -0.43831453\n",
      "   0.16147344 -0.47033292]\n",
      " [ 0.1883373  -0.1361227   0.2769456   0.34269044 -0.01678311 -0.44698972\n",
      "   0.1774975  -0.4289663 ]\n",
      " [ 0.18670528 -0.12001409  0.38627648  0.35418046 -0.04981641 -0.43953958\n",
      "   0.181529   -0.4634958 ]\n",
      " [ 0.253251   -0.14223157  0.39878565  0.38956702 -0.06889519 -0.49127132\n",
      "   0.13926443 -0.53550375]\n",
      " [ 0.18491608 -0.15284365  0.44003043  0.43636096 -0.04458584 -0.41924852\n",
      "   0.15278606 -0.42389804]\n",
      " [ 0.16210356 -0.10574225  0.3383456   0.35428792 -0.07074194 -0.42518452\n",
      "   0.14323397 -0.45813993]\n",
      " [ 0.200627   -0.17560354  0.37627655  0.36086625 -0.05404526 -0.44287583\n",
      "   0.14696518 -0.5148149 ]\n",
      " [ 0.18953061 -0.14880835  0.37511507  0.36306173 -0.06102722 -0.454669\n",
      "   0.15119873 -0.4808298 ]\n",
      " [ 0.17399234 -0.10098101  0.3739666   0.37078804 -0.06278411 -0.43793765\n",
      "   0.13437153 -0.4234046 ]\n",
      " [ 0.18183371 -0.11194013  0.36345938  0.34544006 -0.07132913 -0.4705109\n",
      "   0.16496068 -0.5233884 ]\n",
      " [ 0.18635568 -0.15078354  0.33675596  0.3140674  -0.04530558 -0.48826742\n",
      "   0.16071051 -0.503806  ]\n",
      " [ 0.21194883 -0.11748353  0.3723725   0.3711451  -0.0444377  -0.45586196\n",
      "   0.15619543 -0.4517545 ]\n",
      " [ 0.23899442 -0.13603267  0.2757377   0.32746187 -0.05014493 -0.43950078\n",
      "   0.15335019 -0.44654492]\n",
      " [ 0.27763078 -0.09377068  0.3980813   0.42179835 -0.0792807  -0.48946178\n",
      "   0.10748982 -0.50026035]\n",
      " [ 0.18600628 -0.10422095  0.40644124  0.36690325 -0.04775983 -0.4264003\n",
      "   0.15259491 -0.5123428 ]\n",
      " [ 0.19125082 -0.09582786  0.3454496   0.36676192 -0.07762314 -0.44593108\n",
      "   0.14847699 -0.47004044]\n",
      " [ 0.23922727 -0.141116    0.37763998  0.36482382 -0.06152768 -0.46519077\n",
      "   0.18606466 -0.49226716]\n",
      " [ 0.18540254 -0.12276706  0.34188914  0.36672258 -0.05965505 -0.46607506\n",
      "   0.16398689 -0.44567984]\n",
      " [ 0.20062214 -0.13655585  0.34990937  0.36974412 -0.04173515 -0.4641537\n",
      "   0.19924161 -0.49903977]\n",
      " [ 0.19248304 -0.14372143  0.40711495  0.37346503 -0.06465211 -0.4538563\n",
      "   0.17894717 -0.49451536]\n",
      " [ 0.2371813  -0.10813658  0.36577743  0.3538273  -0.06761009 -0.47182554\n",
      "   0.1948526  -0.45274848]]\n",
      "(44, 8)\n",
      "accuracy: 0.22727272727272727, precision: 0.1106719367588933, recall: 0.22727272727272727, f1: 0.14155844155844155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1701029  -0.12869859  0.43581378  0.35237604 -0.05302569 -0.4149249\n",
      "   0.14788772 -0.42574978]\n",
      " [ 0.19441018 -0.14089799  0.3857175   0.38613573 -0.04308029 -0.46517956\n",
      "   0.13337392 -0.5323505 ]\n",
      " [ 0.23581836 -0.11218637  0.39112747  0.42505288 -0.04212994 -0.47075206\n",
      "   0.17968155 -0.4478426 ]\n",
      " [ 0.23058856 -0.13873206  0.38316968  0.373416   -0.07268393 -0.46370676\n",
      "   0.1253979  -0.54840755]\n",
      " [ 0.23895524 -0.13042323  0.39640582  0.3945656  -0.07841083 -0.4567799\n",
      "   0.15680146 -0.44059208]\n",
      " [ 0.23197995 -0.13950586  0.37011623  0.45430902 -0.10868491 -0.4384696\n",
      "   0.14788523 -0.47885942]\n",
      " [ 0.18331414 -0.14103708  0.37001136  0.3895989  -0.07966995 -0.45715445\n",
      "   0.1137986  -0.47351056]\n",
      " [ 0.21049464 -0.18071051  0.2884139   0.37739837 -0.05168004 -0.48535728\n",
      "   0.20643686 -0.45967025]\n",
      " [ 0.204211   -0.14582975  0.31778294  0.3626092  -0.07784801 -0.4667578\n",
      "   0.17072462 -0.47451532]\n",
      " [ 0.22711477 -0.12419406  0.4341843   0.39301088 -0.04456954 -0.45620525\n",
      "   0.14079604 -0.47843096]\n",
      " [ 0.23122634 -0.10595715  0.3942582   0.37387452 -0.12093101 -0.46659833\n",
      "   0.16356857 -0.49054068]\n",
      " [ 0.2020596  -0.1421923   0.3846335   0.3957997  -0.07551851 -0.49570855\n",
      "   0.13510771 -0.50494224]\n",
      " [ 0.25043738 -0.13920853  0.3971283   0.37665623 -0.07565123 -0.47528297\n",
      "   0.15729192 -0.5191468 ]\n",
      " [ 0.2675825  -0.17028064  0.37926385  0.39943686 -0.10298534 -0.49916452\n",
      "   0.19381431 -0.48261178]\n",
      " [ 0.1916877  -0.15539609  0.4148586   0.39939225 -0.04185542 -0.43142366\n",
      "   0.10409585 -0.48536023]\n",
      " [ 0.253864   -0.11935681  0.39009076  0.38565975 -0.10390609 -0.4840939\n",
      "   0.1114148  -0.50993454]\n",
      " [ 0.18401074 -0.14838314  0.28787518  0.31285986 -0.06994286 -0.46776432\n",
      "   0.18524429 -0.4494303 ]\n",
      " [ 0.21590285 -0.17423368  0.40915117  0.39540473 -0.09111755 -0.4423509\n",
      "   0.12689942 -0.48251835]\n",
      " [ 0.27365208 -0.14320564  0.41169     0.41489512 -0.09571767 -0.48633075\n",
      "   0.13547239 -0.5193559 ]\n",
      " [ 0.1989906  -0.17850605  0.36107284  0.39324757 -0.08279349 -0.47709984\n",
      "   0.1428898  -0.52460885]\n",
      " [ 0.22513807 -0.15498123  0.34887898  0.3694905  -0.04476162 -0.4571921\n",
      "   0.20838024 -0.4862292 ]\n",
      " [ 0.2665848  -0.14064513  0.4576919   0.42539465 -0.09369293 -0.48237062\n",
      "   0.11159861 -0.569855  ]\n",
      " [ 0.23403102 -0.13249248  0.3525663   0.37908968 -0.07406346 -0.44256505\n",
      "   0.1884335  -0.4472564 ]\n",
      " [ 0.18289953 -0.12779792  0.32921612  0.34568152 -0.01128173 -0.44871786\n",
      "   0.15919773 -0.48657238]\n",
      " [ 0.20250729 -0.14020781  0.28558758  0.35057676 -0.02005654 -0.45716143\n",
      "   0.17482811 -0.4436894 ]\n",
      " [ 0.20072448 -0.1251458   0.39719522  0.363146   -0.05315737 -0.45026678\n",
      "   0.17816277 -0.479218  ]\n",
      " [ 0.26958424 -0.14855249  0.41009712  0.39983916 -0.07254957 -0.50271845\n",
      "   0.1341794  -0.55283225]\n",
      " [ 0.19828807 -0.1597623   0.4515489   0.4459761  -0.04681576 -0.4295223\n",
      "   0.14827181 -0.43920463]\n",
      " [ 0.17552482 -0.11127768  0.34922785  0.3643152  -0.07456941 -0.43617743\n",
      "   0.14017336 -0.47324482]\n",
      " [ 0.215875   -0.18167181  0.38766873  0.37039435 -0.05727987 -0.45389515\n",
      "   0.1434982  -0.5313201 ]\n",
      " [ 0.20385677 -0.15487593  0.38758564  0.3719752  -0.06570616 -0.4666015\n",
      "   0.14767109 -0.4963477 ]\n",
      " [ 0.18697764 -0.10616268  0.38459182  0.38059202 -0.06603005 -0.44911408\n",
      "   0.13186179 -0.4389391 ]\n",
      " [ 0.19600967 -0.1173719   0.37441486  0.3546154  -0.07428862 -0.48217702\n",
      "   0.16325279 -0.54021454]\n",
      " [ 0.20224094 -0.1560581   0.34713086  0.32297587 -0.04736463 -0.49991\n",
      "   0.15711848 -0.52046204]\n",
      " [ 0.2257936  -0.12217532  0.38064697  0.38086468 -0.04431541 -0.46598518\n",
      "   0.15267882 -0.46873677]\n",
      " [ 0.25400606 -0.14160907  0.2857539   0.3362848  -0.05433927 -0.45026857\n",
      "   0.15008163 -0.46098655]\n",
      " [ 0.29295582 -0.0990212   0.40983343  0.43237674 -0.08244071 -0.5006297\n",
      "   0.1017893  -0.5172772 ]\n",
      " [ 0.19967414 -0.11000681  0.41639626  0.376741   -0.0498058  -0.43750286\n",
      "   0.15007536 -0.52912474]\n",
      " [ 0.20445833 -0.10131107  0.35638523  0.3758341  -0.08062778 -0.4564788\n",
      "   0.14607072 -0.48613712]\n",
      " [ 0.25535065 -0.14740174  0.3866159   0.37366495 -0.06340542 -0.47615048\n",
      "   0.18335804 -0.50904536]\n",
      " [ 0.19941126 -0.12849057  0.35236228  0.37576059 -0.06340169 -0.4763724\n",
      "   0.16188112 -0.46101022]\n",
      " [ 0.21495262 -0.14245039  0.36008075  0.37951913 -0.04445899 -0.47399768\n",
      "   0.19696361 -0.5156343 ]\n",
      " [ 0.20637198 -0.15018654  0.41787663  0.38346595 -0.06736425 -0.46488085\n",
      "   0.17612144 -0.5110074 ]\n",
      " [ 0.2527094  -0.11222519  0.37556303  0.36286592 -0.06995839 -0.48384327\n",
      "   0.19088052 -0.4703372 ]]\n",
      "(44, 8)\n",
      "accuracy: 0.22727272727272727, precision: 0.10743801652892561, recall: 0.22727272727272727, f1: 0.1397306397306397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.17939964 -0.13435483  0.4521311   0.3596064  -0.05612822 -0.42465118\n",
      "   0.14289804 -0.44083667]\n",
      " [ 0.20481053 -0.14675087  0.40184847  0.39417794 -0.04631893 -0.47638908\n",
      "   0.12822188 -0.5487033 ]\n",
      " [ 0.24640788 -0.1175879   0.4061222   0.43298316 -0.04446376 -0.48119855\n",
      "   0.17521471 -0.46378523]\n",
      " [ 0.24222031 -0.14464006  0.39957845  0.38096362 -0.07662984 -0.47493696\n",
      "   0.12031317 -0.56339204]\n",
      " [ 0.24956134 -0.13554943  0.4124986   0.40156    -0.08264989 -0.46786514\n",
      "   0.1529988  -0.45639044]\n",
      " [ 0.2431571  -0.14472097  0.38646173  0.46248955 -0.11310992 -0.45016676\n",
      "   0.14293161 -0.4937638 ]\n",
      " [ 0.19282153 -0.1453598   0.3855518   0.39604542 -0.08310805 -0.46741825\n",
      "   0.10908917 -0.4890769 ]\n",
      " [ 0.22098225 -0.18610261  0.3036906   0.38421708 -0.05627968 -0.49522737\n",
      "   0.20266104 -0.47449052]\n",
      " [ 0.21543375 -0.1513539   0.3329238   0.3686035  -0.08258102 -0.4769563\n",
      "   0.16659398 -0.48901173]\n",
      " [ 0.2382538  -0.12944642  0.4507034   0.39997303 -0.04865025 -0.4667049\n",
      "   0.1356172  -0.494986  ]\n",
      " [ 0.24226858 -0.11160327  0.4116457   0.38064617 -0.12620534 -0.47854683\n",
      "   0.15861592 -0.5058725 ]\n",
      " [ 0.21320167 -0.14730921  0.402223    0.4029418  -0.08043842 -0.50690174\n",
      "   0.12960349 -0.52070653]\n",
      " [ 0.26240686 -0.14541607  0.41346943  0.384642   -0.08019981 -0.48644602\n",
      "   0.15280272 -0.5343624 ]\n",
      " [ 0.27894977 -0.17670852  0.3962229   0.4070581  -0.10852405 -0.5102176\n",
      "   0.18900645 -0.4973153 ]\n",
      " [ 0.20184162 -0.16116576  0.43214306  0.40582377 -0.04619657 -0.4415425\n",
      "   0.09900732 -0.50125   ]\n",
      " [ 0.26533535 -0.12502126  0.40785637  0.39284962 -0.10879118 -0.49579167\n",
      "   0.10573906 -0.5257254 ]\n",
      " [ 0.19473785 -0.15367463  0.30309373  0.31926885 -0.07472855 -0.47830123\n",
      "   0.18181735 -0.4635718 ]\n",
      " [ 0.22707158 -0.1797644   0.42662743  0.40251362 -0.09559138 -0.45445895\n",
      "   0.12198709 -0.49679402]\n",
      " [ 0.28512967 -0.14930749  0.4283759   0.42300853 -0.10073473 -0.49784407\n",
      "   0.13031548 -0.53455555]\n",
      " [ 0.21005619 -0.18384746  0.3792081   0.40000865 -0.087934   -0.48927182\n",
      "   0.13862275 -0.5403837 ]\n",
      " [ 0.23554324 -0.16145918  0.36419034  0.3759673  -0.04880906 -0.46660626\n",
      "   0.20437458 -0.5012733 ]\n",
      " [ 0.2777352  -0.14622264  0.47647366  0.43367496 -0.09837689 -0.49480048\n",
      "   0.10526449 -0.5859895 ]\n",
      " [ 0.24387634 -0.13883671  0.3689546   0.3850518  -0.0788026  -0.45233196\n",
      "   0.1840856  -0.46182364]\n",
      " [ 0.19317517 -0.13223103  0.3443742   0.35173672 -0.01400539 -0.45909646\n",
      "   0.15551016 -0.5021816 ]\n",
      " [ 0.21291752 -0.14450872  0.29956746  0.3560524  -0.02406042 -0.46718878\n",
      "   0.17077997 -0.45813477]\n",
      " [ 0.2107829  -0.13006705  0.41350874  0.369679   -0.05747346 -0.46096912\n",
      "   0.17339538 -0.49424458]\n",
      " [ 0.2814625  -0.154832    0.42770985  0.40774572 -0.07704052 -0.51418066\n",
      "   0.12770924 -0.5697094 ]\n",
      " [ 0.20767213 -0.1660035   0.46852133  0.4532413  -0.05013613 -0.4395489\n",
      "   0.1426006  -0.45342115]\n",
      " [ 0.18535161 -0.11647379  0.36530897  0.37176055 -0.07930066 -0.44698006\n",
      "   0.13605262 -0.4879876 ]\n",
      " [ 0.22669607 -0.18762231  0.4048      0.37743187 -0.06153389 -0.46495935\n",
      "   0.13876778 -0.5471227 ]\n",
      " [ 0.21412566 -0.160496    0.40566894  0.3786606  -0.07092781 -0.47851592\n",
      "   0.14255394 -0.51159537]\n",
      " [ 0.19643311 -0.11108813  0.40019113  0.38788825 -0.07012773 -0.46007186\n",
      "   0.12806076 -0.4537456 ]\n",
      " [ 0.20633046 -0.12256735  0.3908915   0.3614701  -0.07820567 -0.49355546\n",
      "   0.15994802 -0.5565612 ]\n",
      " [ 0.21402332 -0.16153856  0.3627836   0.32945162 -0.05029394 -0.51139885\n",
      "   0.1518778  -0.5366529 ]\n",
      " [ 0.23619804 -0.1265831   0.39428067  0.38833776 -0.04546917 -0.47556436\n",
      "   0.14779538 -0.48488683]\n",
      " [ 0.26545367 -0.14678726  0.30047613  0.342737   -0.0592343  -0.4607882\n",
      "   0.14564656 -0.47493827]\n",
      " [ 0.3038187  -0.10436082  0.42726547  0.4402195  -0.08695281 -0.5116741\n",
      "   0.09492633 -0.5337064 ]\n",
      " [ 0.20947781 -0.11544998  0.431839    0.3838772  -0.05276048 -0.44866988\n",
      "   0.14618565 -0.54531807]\n",
      " [ 0.21388355 -0.10655053  0.37217993  0.38263208 -0.08459529 -0.46694058\n",
      "   0.14211361 -0.5015191 ]\n",
      " [ 0.26734602 -0.15344502  0.40127793  0.38022098 -0.06624836 -0.48709512\n",
      "   0.17898926 -0.52528137]\n",
      " [ 0.20995212 -0.13391148  0.36814377  0.38236943 -0.0679289  -0.48664695\n",
      "   0.15834469 -0.47573996]\n",
      " [ 0.22522509 -0.14806029  0.37618545  0.38669345 -0.04835326 -0.48404086\n",
      "   0.19318758 -0.5315618 ]\n",
      " [ 0.21650276 -0.1561485   0.4344912   0.390592   -0.07100945 -0.47582397\n",
      "   0.17180264 -0.52674115]\n",
      " [ 0.2640747  -0.11612812  0.3913204   0.3698513  -0.07327221 -0.49544322\n",
      "   0.18579805 -0.4872568 ]]\n",
      "(44, 8)\n",
      "accuracy: 0.22727272727272727, precision: 0.1186602870813397, recall: 0.22727272727272727, f1: 0.14600550964187328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "## earlier run when training loss was fluctuating \n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"focus_groups_convos_emotion_analysis_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "anxious     4\n",
       "serenity    3\n",
       "disgust     1\n",
       "joy         1\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[\"emotion\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = list(test_data['emotion'].unique())\n",
    "#TODO: new labels in test data handling\n",
    "test_data['emotion_label_index'] = test_data.apply(lambda x : label_to_index[x['emotion']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['parent_answer']\n",
    "test_data =  data_preprocessing(test_data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = list(test_data[\"parent_answer\"])\n",
    "y_test = list(test_data[\"emotion_label_index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yeah similar experience eightyearold prepandemic still second grade use computer limited maybe week would type word week one option picked interaction computer course device play would go father house every weekend would exposed computer still busy busy week school getting school getting school thats trip',\n",
       " 'hockey taekwondo busy even didnt seem like enough didnt seem like enough certainly lot busier get play mostly probably weekend weekend also pretty packed dont know time play lot course able control item earn didnt well dont get play tool could use incentive week thats window available right day long getting away computer project every day',\n",
       " 'like bomb went figured figured workarounds like pandora box trying figure block phone doe phone parent coordinator calling principal like block youtube without putting timer thing driving crazy downhill finally figured son banging much he actually broken screen whatever control put could bang would whatever wanted month later exchange ipad nearly ripped hair youtube yeah addiction sunk first five month lockdown screen youtube misery sucking',\n",
       " 'also schedule special need family schedule really tight kid little guy go bed 830 900 older one bed 1000 lot structure literally like bomb hit people going bed 100 morning totally felt chaotic control felt like technology sputtered little bit gt device figured get everybody black hole sucked everybody hour hour hour hour hour felt control',\n",
       " 'yeah dont feel like there even much involved dont much especially middle schooler nothing isnt even much ask',\n",
       " 'even son started going back school five day week still became problem technology right homework go youtube go roblox something dont know theyre back tell go back come home school still work still became problem even though theyre back school five day week yeah still assignment computer like homework assignment computer yeah even go back school feel like still would problem youre issue remotely yeah thats experience',\n",
       " 'high schooler think interesting even he one happened several time go supposed go like youre going keep going he like yeah im going go im like okay personally dont anticipate issue he graduating think thats also part think missed whole senior year everything looking forward able go see teacher stuff like think want connection people supposed go kind like one day week maybe sometimes two day week weird schedule still cant figure he going go kid bought going back school',\n",
       " 'hopefully continue go program september hell smaller class size think 32 kid weve always overwhelming couldnt get along anyone daily call school nobody really ever addressed issue always like he kid type thing tear cried yeah think actually even said day oh yes im going remind advance prepare tomorrow person said oh yeah ill something else play roblox',\n",
       " 'one teacher formal way like teacher le formal way thats amazing feedback get two tutor havent long time maybe month maybe month great overall positive im happy need help ela math complete lack participation google classroom didnt even know knew learned past year one one ability without go anywhere get dressed drive great deal hope stay hope stay there demand there supply think would one good takeaway remote learning option',\n",
       " 'yes direct contact teacher school instead school sending bunch print out child folder became overwhelming paper teacher able reach immediately mean werent think became easier became platform everyone using believe school use class dojo way communicate think platform used student school work ilearn didnt really like im kind glad',\n",
       " 'guess one positive outcome fact communicating friend cousin life far away whatever better social life wouldnt talk anybody communicate keep touch regaining control back getting understand there time place everything shouldnt allow thing consume time frustrating im like wait guy computer day guy time give break theyll get upset argument yeah thats',\n",
       " 'yeah younger used look phone monitor kind stuff hard know line hard social issue similar parent 1 talking friend online friend he never crew crew want play matthew coming yeah',\n",
       " 'point son say friend moved away downtown manhattan friend left son say friend call friend hell three hour four five six seven daughter made alternative account found shes trolling roblox shes 11 inaudible 010154 people dating troll thats something proud nightmare im looking software block whole thing ive put padlock desktop computer door shell hide lock im like padlock ive thrown device ill rip router wall mean gotten ugly scream awful lot yeah pretty',\n",
       " 'he already panicked going back fall doesnt know want go back back person thats another conversation']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 1, 0, 2, 2, 1, 0, 1, 2, 2, 1, 0, 3, 1, 0, 1, 1, 2, 2, 3]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 3398, 1010, 2714, 3325, 2809, 1011, 2095, 1011, 2214, 1012, 3653, 1011, 6090, 3207, 7712, 2145, 2117, 3694, 2224, 3274, 3132, 1012, 2672, 2733, 2052, 2828, 1010, 2773, 2733, 2028, 5724, 3856, 2079, 1012, 8290, 3274, 1012, 2607, 5080, 2377, 2006, 1010, 2052, 2175, 2269, 1005, 1055, 2160, 2296, 5353, 2052, 6086, 7588, 1012, 2145, 5697, 1012, 5697, 2733, 1010, 2082, 1010, 2893, 2082, 1010, 2893, 2082, 1010, 2008, 1005, 1055, 4440, 2993, 1012, 102, 0, 0, 0, 0], [101, 2066, 5968, 2253, 2125, 1010, 6618, 2041, 1010, 6618, 2147, 24490, 2015, 2066, 19066, 1005, 1055, 3482, 1012, 2667, 3275, 3796, 1012, 1012, 1012, 3042, 18629, 1010, 3042, 6687, 10669, 1010, 4214, 4054, 1010, 2066, 1010, 1000, 2129, 3796, 7858, 2302, 1012, 1012, 1012, 1000, 5128, 25309, 2477, 1010, 4439, 4689, 1010, 19448, 2045, 1012, 2633, 6618, 2365, 1010, 22255, 2172, 2002, 1005, 1055, 2941, 3714, 3898, 3649, 2491, 2404, 1999, 1010, 2071, 9748, 2052, 3649, 2359, 1012, 102], [101, 2036, 6134, 1012, 2569, 2342, 2155, 1010, 6134, 2428, 4389, 4845, 2210, 3124, 2175, 2793, 1022, 1024, 2382, 1023, 1024, 4002, 3080, 2028, 2793, 2184, 1024, 4002, 2843, 3252, 1010, 6719, 2066, 5968, 2718, 2009, 1012, 2111, 2183, 2793, 1015, 1024, 4002, 2851, 1010, 6135, 1012, 1012, 1012, 2371, 19633, 2491, 1010, 2371, 2066, 2974, 11867, 23128, 2210, 2978, 14181, 5080, 6618, 2131, 7955, 2304, 4920, 8631, 1999, 1012, 7955, 3178, 3178, 3178, 3178, 2847, 1010, 2371, 2491, 102], [101, 3398, 1010, 2514, 2066, 2045, 1005, 1055, 2130, 2172, 2920, 1012, 2172, 1010, 2926, 2690, 2082, 2121, 1010, 2498, 2130, 2172, 3198, 2032, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2130, 2365, 2318, 2183, 2067, 2082, 2274, 2154, 2733, 2145, 2150, 3291, 1010, 2974, 1010, 2157, 19453, 2175, 7858, 2175, 6487, 4135, 2595, 2242, 1012, 2113, 2027, 1005, 2128, 2067, 1012, 2425, 2175, 2067, 2272, 2188, 2082, 2145, 2147, 2145, 2150, 3291, 2130, 2295, 2027, 1005, 2128, 2067, 2082, 2274, 2154, 2733, 1012, 3398, 1010, 2145, 8775, 3274, 1010, 2066, 19453, 8775, 3274, 1012, 3398, 1010, 2130, 2175, 2067, 2082, 2514, 2066, 2145, 2052, 3291, 3277, 19512, 1012, 102], [101, 3984, 2028, 3893, 9560, 2755, 20888, 2814, 1010, 5542, 2166, 2521, 2185, 1010, 3649, 1012, 2488, 2591, 2166, 1012, 2831, 10334, 10639, 1010, 2562, 3543, 1012, 28657, 2491, 2067, 2893, 3305, 2045, 1005, 1055, 2051, 2173, 2673, 3499, 2518, 16678, 2051, 1012, 25198, 2096, 1012, 1045, 1005, 1049, 2066, 1010, 1000, 3524, 1010, 3124, 3274, 2154, 1012, 2053, 1012, 4364, 1010, 2053, 1010, 2051, 2507, 3338, 1012, 1000, 2027, 1005, 2222, 2131, 6314, 6685, 1012, 3398, 1010, 2008, 102], [101, 2057, 1005, 2128, 2391, 2365, 2360, 2767, 2333, 2185, 1010, 2057, 1005, 2128, 5116, 7128, 2767, 2187, 2365, 2758, 1010, 1000, 1045, 2814, 1012, 1000, 2655, 2767, 2002, 1005, 2222, 2093, 2847, 1010, 2176, 1010, 2274, 1010, 2416, 1010, 2698, 1012, 2684, 2081, 4522, 4070, 1010, 2179, 2041, 1010, 18792, 2075, 6487, 4135, 2595, 1012, 2340, 1010, 1031, 27118, 21041, 3468, 5890, 1024, 5890, 1024, 5139, 1033, 1010, 2111, 5306, 1010, 18792, 2068, 1012, 2008, 1005, 1055, 2242, 102], [101, 2028, 2518, 2082, 2027, 1005, 2128, 2522, 17258, 5604, 1010, 1031, 27118, 21041, 3468, 4002, 1024, 2656, 1024, 2538, 1033, 1010, 3100, 2033, 1012, 4929, 15806, 1012, 3984, 2204, 2217, 1012, 2521, 2522, 17258, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2172, 4390, 1010, 2130, 14841, 25509, 6559, 1012, 3499, 2068, 1012, 1012, 1012, 2025, 1012, 1012, 1012, 3747, 2994, 2041, 1012, 2438, 1012, 2131, 2054, 3736, 28281, 1012, 2131, 3898, 3405, 2009, 1012, 2146, 2560, 2027, 1005, 2128, 4198, 2178, 1012, 1012, 1012, 2027, 1005, 2128, 12858, 2033, 1010, 1000, 2339, 2025, 1029, 1000, 2056, 1012, 1012, 1012, 6013, 10245, 7507, 2102, 1010, 2767, 2040, 1005, 1055, 2684, 2941, 2359, 6081, 5970, 11307, 2191, 2298, 2172, 2488, 1031, 102], [101, 5770, 2228, 2591, 2865, 1012, 1012, 1012, 2785, 2507, 2376, 2028, 1012, 3288, 2422, 3291, 1012, 2066, 2577, 12305, 1012, 17453, 2387, 3047, 2577, 12305, 1010, 2088, 2031, 1012, 1012, 1012, 3291, 2467, 10318, 1010, 2755, 2583, 2941, 2156, 2009, 1012, 1012, 1012, 3663, 1010, 2591, 5396, 14044, 2045, 1005, 1055, 2204, 2919, 2673, 1012, 1045, 1005, 1049, 2183, 7680, 2066, 2008, 1012, 2591, 5396, 2204, 2518, 1012, 2109, 2204, 1012, 2109, 3893, 2422, 1012, 2153, 1010, 2036, 102], [101, 2242, 2071, 2131, 3780, 1012, 3745, 3209, 1010, 2130, 7046, 4119, 3066, 2477, 1012, 2664, 6570, 6176, 7149, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 3098, 3855, 13798, 1012, 1045, 1005, 1049, 3038, 2027, 1005, 2128, 2428, 3098, 2000, 1012, 1012, 1012, 2204, 2767, 3067, 1010, 2403, 2095, 2214, 2684, 1012, 1012, 1012, 2196, 2657, 2744, 2077, 1012, 1012, 1012, 4937, 26281, 1012, 2054, 1005, 1055, 4937, 5645, 1010, 2801, 1012, 8307, 2667, 17256, 2014, 1012, 2424, 3114, 2172, 22910, 3042, 2138, 1012, 1012, 1012, 2066, 6886, 2330, 5196, 2505, 3632, 1012, 2385, 2095, 2214, 1012, 1012, 1012, 2228, 5382, 2001, 1012, 2179, 102], [101, 2775, 1015, 1005, 1055, 2082, 1010, 2296, 3076, 25249, 2202, 2067, 5743, 2082, 1012, 2224, 3492, 2172, 7580, 1012, 2428, 2808, 1012, 2179, 2428, 2307, 1010, 3274, 1012, 1012, 1012, 1045, 1005, 1049, 3394, 3836, 1010, 4547, 4281, 1010, 2082, 6198, 3274, 3898, 10699, 8536, 1010, 15189, 1012, 4553, 2734, 2022, 1012, 3305, 2036, 25652, 2057, 1005, 2128, 1999, 1012, 2903, 2775, 1015, 2763, 2116, 4845, 2439, 2095, 4547, 3930, 1012, 2113, 3160, 1010, 2245, 1012, 102, 0, 0], [101, 2812, 1010, 1045, 1005, 2222, 2360, 1010, 2228, 3337, 1010, 2028, 2365, 2002, 1005, 1055, 8740, 16774, 2594, 1012, 16792, 8761, 1012, 2812, 1010, 2002, 1005, 1055, 2152, 12285, 1012, 10089, 1010, 28042, 6364, 1010, 2878, 9129, 2367, 4933, 1012, 9020, 1012, 6090, 3207, 7712, 1010, 9410, 2204, 1012, 2467, 6134, 2113, 2002, 1005, 1055, 2725, 1012, 2002, 1005, 2222, 2505, 2146, 2113, 2242, 2746, 2039, 1010, 2025, 1010, 2131, 6908, 2008, 1012, 2878, 2522, 17258, 1010, 16840, 102], [101, 2228, 3437, 3160, 2210, 3495, 1010, 3331, 3747, 2869, 3666, 7858, 2678, 14841, 25509, 6559, 2477, 1012, 2775, 2210, 15896, 2391, 2667, 2518, 2066, 2111, 2079, 1012, 2008, 1005, 1055, 14841, 25509, 6559, 1005, 1055, 2428, 3422, 2619, 3153, 2242, 1010, 3153, 2695, 3524, 2111, 2066, 2009, 1012, 4066, 10089, 2131, 3403, 2005, 1010, 1000, 2821, 2175, 4095, 1010, 2116, 2066, 2131, 1029, 2116, 18959, 2131, 1029, 1000, 4845, 2130, 16644, 2105, 16644, 2105, 1010, 2785, 6637, 1010, 102], [101, 2812, 1010, 2228, 3025, 2023, 1010, 4016, 2875, 2428, 23760, 1011, 4208, 2678, 2399, 1012, 2210, 6133, 3085, 2071, 4009, 2240, 1012, 2342, 3274, 25249, 2242, 1010, 2071, 2202, 2185, 1012, 2066, 3038, 1010, 2082, 1010, 2591, 3550, 1010, 2082, 6198, 1012, 2210, 2524, 4066, 1997, 1012, 1012, 1012, 2112, 10919, 6180, 1012, 2061, 1010, 2236, 1010, 2242, 2147, 4312, 2045, 1005, 1055, 2467, 2183, 17232, 4816, 1012, 2812, 1010, 6001, 1010, 2145, 4553, 2735, 14836, 2147, 1010, 102], [101, 2428, 4621, 2188, 8080, 2009, 1012, 4748, 14945, 1010, 2228, 2428, 2524, 2051, 15899, 2265, 2556, 2465, 1012, 2066, 2871, 6438, 1045, 1005, 1049, 2066, 1010, 1000, 2053, 1010, 2871, 6438, 2015, 1012, 3784, 1012, 2147, 1012, 2387, 3784, 1012, 4638, 1999, 1012, 1000, 2061, 1010, 3092, 2028, 4997, 2518, 2060, 1012, 2002, 1005, 1055, 2893, 4390, 3836, 4771, 14799, 1012, 10103, 1012, 18546, 17470, 2428, 3277, 1012, 2027, 1005, 2128, 5214, 2022, 1012, 2027, 1011, 1031, 2892, 102], [101, 17736, 1012, 17736, 1010, 2469, 1012, 2036, 7496, 2224, 1012, 2583, 8833, 18546, 8654, 8833, 2465, 8797, 999, 8073, 8897, 2556, 1012, 1031, 2892, 28014, 4002, 1024, 2570, 1024, 5641, 1033, 2085, 1010, 2175, 2698, 3357, 2131, 2045, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 3398, 1012, 2365, 2205, 1012, 2812, 1010, 2365, 2001, 1010, 3984, 1010, 2113, 2172, 1010, 3649, 1005, 1055, 2006, 1010, 18777, 1012, 2154, 2522, 17258, 2718, 1010, 6731, 9415, 6905, 2082, 1012, 8636, 1012, 2066, 1010, 1000, 2053, 1010, 2272, 1999, 1012, 1000, 1045, 1005, 1049, 2740, 2729, 7309, 1010, 2027, 1005, 2128, 2066, 1010, 1000, 2053, 1010, 2053, 1010, 6086, 2522, 17258, 1012, 1000, 2522, 17258, 6429, 3039, 11835, 10334, 2034, 2416, 2706, 1012, 4066, 3140, 3066, 102], [101, 2053, 1010, 2123, 1005, 1056, 1012, 4066, 2467, 19209, 15032, 1012, 2061, 1010, 3398, 1010, 2002, 1005, 1055, 3291, 2096, 1012, 2228, 2028, 2518, 2941, 2359, 2360, 3288, 2591, 2518, 1010, 2365, 2941, 6513, 2522, 17258, 1012, 2061, 1010, 2027, 1005, 2310, 2362, 2627, 2459, 3204, 2242, 1012, 2096, 1010, 2066, 17491, 1010, 2253, 2697, 2160, 2052, 5247, 2051, 2149, 1012, 2518, 3098, 2039, 1010, 2215, 5247, 2051, 2767, 2215, 5247, 2051, 2814, 1012, 2061, 1010, 2027, 1005, 102], [101, 2821, 1010, 2092, 1010, 10751, 1010, 2053, 1010, 2113, 2172, 14044, 4748, 14945, 1012, 2514, 2066, 2008, 1005, 1055, 2941, 4788, 1010, 2002, 1005, 1055, 2583, 3579, 2172, 1010, 2002, 1005, 1055, 3718, 9823, 1012, 2002, 1005, 1055, 2747, 2145, 2531, 1003, 6556, 1012, 2061, 1010, 2228, 6211, 14396, 8152, 5089, 1010, 2027, 1005, 2128, 2227, 3898, 1012, 4748, 14945, 1010, 2514, 2066, 5407, 4788, 1010, 1040, 7274, 2571, 14787, 5407, 2488, 1012, 5407, 2488, 1010, 2002, 1005, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Dataset(X_test_tokenized, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([ 101, 3398, 1010, 2714, 3325, 2809, 1011, 2095, 1011, 2214, 1012, 3653,\n",
       "         1011, 6090, 3207, 7712, 2145, 2117, 3694, 2224, 3274, 3132, 1012, 2672,\n",
       "         2733, 2052, 2828, 1010, 2773, 2733, 2028, 5724, 3856, 2079, 1012, 8290,\n",
       "         3274, 1012, 2607, 5080, 2377, 2006, 1010, 2052, 2175, 2269, 1005, 1055,\n",
       "         2160, 2296, 5353, 2052, 6086, 7588, 1012, 2145, 5697, 1012, 5697, 2733,\n",
       "         1010, 2082, 1010, 2893, 2082, 1010, 2893, 2082, 1010, 2008, 1005, 1055,\n",
       "         4440, 2993, 1012,  102,    0,    0,    0,    0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0]),\n",
       " 'labels': tensor(0)}"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_trainer = Trainer(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_trainer.predict??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_pred, label_ids, metrics = test_trainer.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06766115, -0.3253832 , -0.06493668,  0.35471714, -0.15618443,\n",
       "        -0.11512066, -0.16246825, -0.06664449],\n",
       "       [ 0.0916235 , -0.35716227, -0.06221877,  0.41920525, -0.1804009 ,\n",
       "        -0.17619711, -0.06709153,  0.04494047],\n",
       "       [ 0.04280779, -0.37059927, -0.04707661,  0.34497947, -0.17658278,\n",
       "        -0.14099593, -0.0756123 ,  0.05449395],\n",
       "       [-0.1484484 , -0.6329201 , -0.34090024,  0.02485409,  0.0903853 ,\n",
       "        -0.28749904, -0.05994736,  0.20962328],\n",
       "       [ 0.03568046, -0.19395709,  0.02585419,  0.43622717, -0.05606452,\n",
       "        -0.24524978, -0.17073739,  0.12783825],\n",
       "       [ 0.00079341, -0.51856947, -0.12783314,  0.2570194 , -0.02892824,\n",
       "        -0.38981146,  0.03351265,  0.04641962],\n",
       "       [-0.07587789, -0.75548315, -0.28536007,  0.22248712,  0.08766535,\n",
       "        -0.57228875,  0.26338446,  0.34008944],\n",
       "       [-0.01004304, -0.46937793, -0.19732882,  0.2263671 , -0.07728738,\n",
       "        -0.17946137, -0.05250129,  0.01034988],\n",
       "       [-0.053477  , -0.5714641 , -0.28549987,  0.18478891, -0.00757851,\n",
       "        -0.35259894,  0.05648272,  0.14711703],\n",
       "       [ 0.05747456, -0.31170356, -0.03524332,  0.38108492, -0.12696671,\n",
       "        -0.19078948, -0.20011833, -0.04060209],\n",
       "       [-0.1308865 , -0.6566893 , -0.32285345,  0.10054647,  0.04361255,\n",
       "        -0.30973384, -0.03345454,  0.14629164],\n",
       "       [ 0.07171412, -0.10974133,  0.05847579,  0.44376856, -0.12297358,\n",
       "        -0.1427171 , -0.33965313,  0.04299736],\n",
       "       [ 0.07367576, -0.13963875,  0.11183208,  0.43517458, -0.1364998 ,\n",
       "        -0.2929448 , -0.27587953,  0.02942635],\n",
       "       [ 0.0298197 , -0.0647255 ,  0.02872999,  0.43095556, -0.18624072,\n",
       "        -0.11902125, -0.43327087,  0.08291354]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(raw_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 3, 3, 3, 0, 3, 3, 0]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = list(y_pred)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 0, 2, 1, 3, 0, 0, 3, 3]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_ids = list(label_ids)\n",
    "label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anxious', 'anxious', 'anxious', 'anxious', 'anxious', 'serenity', 'anxious', 'anxious', 'serenity']\n",
      "['anxious', 'serenity', 'disgust', 'joy', 'anxious', 'serenity', 'serenity', 'anxious', 'anxious']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     anxious       0.43      0.75      0.55         4\n",
      "     disgust       0.00      0.00      0.00         1\n",
      "         joy       0.00      0.00      0.00         1\n",
      "    serenity       0.50      0.33      0.40         3\n",
      "\n",
      "    accuracy                           0.44         9\n",
      "   macro avg       0.23      0.27      0.24         9\n",
      "weighted avg       0.36      0.44      0.38         9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred = [ index_to_label[p] for p in y_pred]\n",
    "print(pred)\n",
    "true = [ index_to_label[label_id] for label_id in label_ids]\n",
    "print(true)\n",
    "report = classification_report(y_true=true, y_pred=pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving model once satisfactory scores are obtained\n",
    "model.save_pretrained('emotion_analysis_output')\n",
    "### NOTE: unable to upload to GIT the pytorch_model.bin serialized file as it is 427 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading model fine-tuned for emotion analysis\n",
    "finetuned_model = BertForSequenceClassification.from_pretrained(r'C:\\Users\\vhpld\\Desktop\\emotion_analysis_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing the results match with the above model\n",
    "test_trainer1 = Trainer(finetuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_pred, label_ids, metrics = test_trainer1.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = list(np.argmax(raw_pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ids1 = list(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anxious', 'anxious', 'anxious', 'anxious', 'anxious', 'serenity', 'anxious', 'anxious', 'serenity']\n",
      "['anxious', 'serenity', 'disgust', 'joy', 'anxious', 'serenity', 'serenity', 'anxious', 'anxious']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     anxious       0.43      0.75      0.55         4\n",
      "     disgust       0.00      0.00      0.00         1\n",
      "         joy       0.00      0.00      0.00         1\n",
      "    serenity       0.50      0.33      0.40         3\n",
      "\n",
      "    accuracy                           0.44         9\n",
      "   macro avg       0.23      0.27      0.24         9\n",
      "weighted avg       0.36      0.44      0.38         9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\vhpld\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1308: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "pred=['A', 'B']\n",
    "true=['A', 'B']\n",
    "pred = [ index_to_label[p] for p in y_pred1]\n",
    "print(pred)\n",
    "true = [ index_to_label[label_id] for label_id in label_ids1]\n",
    "print(true)\n",
    "report = classification_report(y_true=true, y_pred=pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion -\n",
    "##### With limited data, hyerparameter tuning with 8 classes was not leading to good scores and training loss was fluctuating\n",
    "##### Labels were reduced to 4 from 8\n",
    "##### 4 labels were chosed in a such a way so as to capture varying degrees of positive (serenity, joy) and negative emotions (anxious, disgust)\n",
    "##### Reduction in labels finally led to the training loss fluctuation pattern to disappear and more labels were detected in the output as opposed to only single label being detected when training loss was fluctuating and not declining even after running too many epochs (assuming the batch size was causing the training loss to appear to not converge)\n",
    "\n",
    "#### Next step-\n",
    "##### Annotating the lowPIU and media groups' parent level responses with emotions\n",
    "##### Improvement observed on very small test dataset size and this needs to be increased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
